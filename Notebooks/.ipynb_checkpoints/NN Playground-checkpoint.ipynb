{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621cc43b",
   "metadata": {},
   "source": [
    "# Welcome to the Neural Network Playground\n",
    "\n",
    "Welcome to this Jupyter notebook, the interactive companion to the \"Neural Network Playground\" repository. Here, we embark on a fascinating journey through the diverse world of neural networks, exploring various architectures and their unique capabilities. This notebook is crafted to be both an educational resource and a practical guide, providing you with the opportunity to dive deep into the functionalities, designs, and applications of neural networks across different tasks and data types.\n",
    "\n",
    "Through descriptive explanations, code implementations, and hands-on examples, we aim to foster a deeper understanding of neural networks and inspire you to experiment, innovate, and contribute to the field of artificial intelligence and machine learning. Let's begin our exploration and unlock the potential of neural networks together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159cdffd",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## [Setup](#Setup)\n",
    "- [Imports](#Imports)\n",
    "\n",
    "## [Foundational Concepts](#Foundational-Concepts)\n",
    "- [Base Neural Network Class (BaseNN)](#Base-Neural-Network-Class-(BaseNN))\n",
    "\n",
    "#### Foundational Models:\n",
    "- **Basic Neural Networks**: \n",
    "  - Perceptron\n",
    "  - Feed Forward\n",
    "  - Radial Basis Function Network\n",
    "- **Deep Learning Essentials**: \n",
    "  - Deep Feed Forward (DFF)\n",
    "\n",
    "#### Deep Learning Architectures:\n",
    "- **Core Architectures**: \n",
    "  - **Autoencoders**\n",
    "      - **AE**\n",
    "      - **VAE**\n",
    "      - **DAE**\n",
    "      - **SAE**\n",
    "  - **Convolutional Network Architectures**: \n",
    "      - **Deep Convolutional Network (DCN)**\n",
    "      - **Deconvolutional Network (DN)**     \n",
    "      - **Deep Convolutional Inverse Graphics Network (DCIGN)**\n",
    "      \n",
    "- **Recurrent and Memory Models**: \n",
    "  - RNN\n",
    "  - LSTM\n",
    "  - GRU\n",
    "  - NTM\n",
    "\n",
    "#### Advanced Concepts:\n",
    "- **Probabilistic and Generative Models**: \n",
    "  - Markov Chain\n",
    "  - Hopfield Network\n",
    "  - Boltzmann Machine\n",
    "  - Restricted Boltzmann Machine\n",
    "  - Deep Belief Network\n",
    "  - GAN\n",
    "- **Hybrid and Specialized Models**: \n",
    "  - SNN\n",
    "  - LSM\n",
    "  - ELM\n",
    "  - ESN\n",
    "  - ResNet\n",
    "  - Kohonen Network (SOFM)\n",
    "  - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29765f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f39bb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0495dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d657427",
   "metadata": {},
   "source": [
    "# Foundational Concepts\n",
    "## Base Neural Network Class (BaseNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7db57",
   "metadata": {},
   "source": [
    "Creating a BaseNN class intended to use inheritance in later implementations of different NN's when abstracting the base class to make specialized classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18f98a",
   "metadata": {},
   "source": [
    "In the development of this neural network project, we introduce the `BaseNN` class as a foundational component, leveraging the principles of object-oriented programming to foster a modular and scalable approach to neural network design. \n",
    "\n",
    "The `BaseNN` class serves as a blueprint for all subsequent neural network models, encapsulating common attributes such as `input_size`, `hidden_size`, and `output_size`. \n",
    "\n",
    "These attributes are essential across a wide range of neural network architectures, ensuring a consistent structure across our implementations. \n",
    "\n",
    "Furthermore, the class defines an abstract `forward` method, which obliges any derived class to specify its own data processing mechanism, detailing how inputs are transformed into outputs through the network. This approach not only enforces a uniform interface but also promotes code reusability and simplifies the process of experimenting with and extending different neural network models. \n",
    "\n",
    "By abstracting common functionalities into the `BaseNN` class, we significantly reduce redundancy and streamline the development of specialized neural network architectures, allowing for a clear and efficient exploration of the vast landscape of neural network designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bdcf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for neural networks\n",
    "class BaseNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BaseNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"forward method must be implemented in derived classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab62054",
   "metadata": {},
   "source": [
    "# Foundational Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953cb1e",
   "metadata": {},
   "source": [
    "## Basic Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877d9a",
   "metadata": {},
   "source": [
    "### Perceptron (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48ab51",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "The Perceptron represents the simplest form of a feedforward neural network, consisting of a single neuron with adjustable weights and a bias. Developed in 1957 by Frank Rosenblatt, it laid the groundwork for understanding neural networks. The Perceptron algorithm is a binary classifier that linearly separates data into two parts, making it a cornerstone in the study of machine learning for simple predictive modeling tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Perceptrons can process:\n",
    "- Numerical data\n",
    "- Binary features\n",
    "\n",
    "Given its simplicity, it's primarily suited for linearly separable datasets where inputs can be categorized into two distinct groups.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Perceptrons are utilized for:\n",
    "- Binary classification tasks\n",
    "- Basic pattern recognition\n",
    "\n",
    "Their straightforward approach allows them to make decisions by weighing input features, showcasing early neural network capabilities in distinguishing between two classes.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Due to its simplicity, the scalability of a single-layer Perceptron is limited to problems that are linearly separable. For more complex datasets or non-linear problems, multi-layer networks or different algorithms are recommended.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Perceptrons can be sensitive to noise in the data, especially since they do not incorporate error minimization in the same way as more advanced models. They perform best with clean, well-defined datasets.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "While the basic Perceptron is foundational, several key developments have been made to extend its utility, including:\n",
    "- **Multi-layer Perceptrons (MLPs):** Comprising multiple layers of neurons to tackle non-linearly separable data.\n",
    "- **Stochastic Gradient Descent:** An optimization method allowing Perceptrons and their multi-layer successors to learn from training data iteratively.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Perceptrons:**\n",
    "- For simple linear classification problems.\n",
    "- As a learning tool to understand the basics of neural network architecture and linear decision boundaries.\n",
    "\n",
    "**Considerations:**\n",
    "- The Perceptron's inability to solve non-linear problems limits its application in complex real-world scenarios.\n",
    "- It serves as a building block for more sophisticated networks that can handle a broader range of tasks.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "The Perceptron model, with its simplicity, offers a fundamental understanding of neural network principles. Although its direct applications are limited to linearly separable tasks, the Perceptron remains an essential concept in machine learning, providing a stepping stone to more advanced neural network architectures and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e83688fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize weights and bias randomly\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "\n",
    "    def activate(self, x):\n",
    "        # Simple step function as activation\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum of inputs\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "        # Apply the activation function\n",
    "        output = self.activate(weighted_sum)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f716aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0.5 0.8]\n",
      "Output: 1\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a perceptron with 2 input cells\n",
    "    perceptron = Perceptron(input_size=2)\n",
    "\n",
    "    # Example input\n",
    "    input_data = np.array([0.5, 0.8])\n",
    "\n",
    "    # Get the output from the perceptron\n",
    "    output = perceptron.forward(input_data)\n",
    "\n",
    "    print(f\"Input: {input_data}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e298f2a",
   "metadata": {},
   "source": [
    "### Feed Forward (FF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d803f",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Feed Forward Neural Networks (FFNNs) are the simplest type of artificial neural network architecture, where connections between the nodes do not form a cycle. This model is structured in layers, consisting of an input layer, one or more hidden layers, and an output layer. The data moves in only one direction - forward - from the input nodes, through the hidden nodes (if any), and finally to the output nodes. There are no cycles or loops in the network, hence the name \"feedforward.\"\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Feed Forward Neural Networks are capable of handling a variety of data types, making them versatile for numerous applications:\n",
    "\n",
    "- Numerical data\n",
    "- Categorical data\n",
    "- Images (when flattened to a vector)\n",
    "- Text (via bag-of-words or TF-IDF vectors)\n",
    "\n",
    "Their adaptability makes FFNNs suitable for a broad range of tasks across different fields.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "FFNNs are widely used for:\n",
    "\n",
    "- Classification tasks, both binary and multi-class.\n",
    "- Regression tasks for predicting continuous outcomes.\n",
    "- Pattern recognition, serving as the foundational architecture for more complex tasks.\n",
    "\n",
    "They serve as the backbone for understanding more complex neural network architectures.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of Feed Forward Neural Networks depends on the size of the input data and the complexity of the task. While adding more hidden layers can increase the network's capacity to learn complex patterns, it also raises the computational cost and the risk of overfitting. Techniques like dropout and regularization are often employed to manage these challenges.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "FFNNs exhibit a degree of robustness to noise in the input data, thanks to their capacity to learn generalized representations. However, their performance can be significantly affected by the presence of irrelevant features or highly noisy datasets, necessitating careful data preprocessing and feature selection.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Feed Forward Neural Networks can be implemented with various activation functions (ReLU, Sigmoid, Tanh) and architectures (deep networks with many layers, wide networks with more neurons per layer) to suit specific problems:\n",
    "\n",
    "- **Deep Feed Forward Networks**: Incorporate multiple hidden layers to capture complex patterns.\n",
    "- **Wide Networks**: Increase the number of neurons in hidden layers to enhance model capacity without deepening the architecture.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Feed Forward Neural Networks:**\n",
    "\n",
    "- For straightforward prediction problems where the complexity of recurrent or convolutional networks is unnecessary.\n",
    "- In cases where the data can be represented in a fixed-size vector and does not possess inherent sequential or spatial patterns.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- While FFNNs are powerful for many tasks, they may not be ideal for data with temporal sequences (e.g., time-series) or spatial hierarchies (e.g., images), where recurrent or convolutional architectures might be more appropriate.\n",
    "- Careful design and regularization are essential to prevent overfitting, especially as the network size increases.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Feed Forward Neural Networks form the cornerstone of neural network models, offering a straightforward yet powerful framework for numerous predictive modeling tasks. Their simplicity, coupled with the potential for customization and scalability, makes them an indispensable tool. Understanding and mastering FFNNs provide a solid foundation for delving into more specialized neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc3c747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)  # Single output neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        x = torch.relu(self.hidden_layer(x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the neural network\n",
    "input_size = 2  # Number of input features \n",
    "hidden_size = 3  # Number of neurons in the hidden layers\n",
    "model = FeedforwardNN(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d09fa362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNN(\n",
      "  (input_layer): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (hidden_layer): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Input: tensor([[0.5000, 0.3000]])\n",
      "Output: 0.5437535643577576\n"
     ]
    }
   ],
   "source": [
    "# Define a sample input\n",
    "sample_input = torch.tensor([[0.5, 0.3]])  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output = model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(model)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output:\", output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03243f0",
   "metadata": {},
   "source": [
    "### Radial Basis Network (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb35815",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Radial Basis Function Networks (RBFNs) are a type of artificial neural network that uses radial basis functions as activation functions. They are typically used for interpolation in multidimensional space, pattern recognition, function approximation, and time-series prediction. The core idea behind RBFNs is to transform the input space into a new space where the problem becomes linearly separable. This transformation is achieved using a set of radial basis functions, each associated with a center and affecting only the region close to that center.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "RBF Networks are particularly effective with:\n",
    "\n",
    "- Numerical data\n",
    "- Multidimensional data for function approximation\n",
    "- Patterns that require a localized response\n",
    "\n",
    "Their ability to handle non-linear problems makes them suitable for various tasks in regression, classification, and clustering.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "RBF Networks excel in:\n",
    "\n",
    "- Function approximation and regression tasks\n",
    "- Classification problems\n",
    "- Time-series prediction\n",
    "- Clustering and unsupervised learning\n",
    "\n",
    "The localized nature of radial basis functions allows RBFNs to model complex and non-linear relationships within the data efficiently.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of RBF Networks can be challenging due to the need to select an appropriate number of centers and their locations. While having more centers can improve the network's ability to approximate complex functions, it also increases the computational cost and the risk of overfitting.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "RBF Networks demonstrate robustness to noise in the input data due to the smoothness of the radial basis functions. However, the choice of the width parameter of the basis functions is crucial, as it influences the network's sensitivity to the input data's scale and noise level.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variations of RBF Networks exist, primarily differing in how the centers and the width of the basis functions are determined:\n",
    "\n",
    "- **Fixed Centers Selected Randomly**: Centers are chosen randomly from the input data.\n",
    "- **Clustering-based Centers**: Centers are determined using clustering algorithms like k-means to capture the data's underlying structure.\n",
    "- **Orthogonal Least Squares (OLS)**: A more sophisticated method for selecting centers that aims to minimize redundancy among the basis functions.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Radial Basis Function Networks:**\n",
    "\n",
    "- In situations where the data exhibits non-linear relationships that need to be captured with high precision.\n",
    "- For problems where local interactions dominate the system's behavior, and a global approximation model might not be effective.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- The selection of the number of centers and their locations is critical for the performance of the RBF Network. Incorrect choices can lead to poor generalization or overfitting.\n",
    "- The determination of the width parameter requires careful tuning, often based on cross-validation, to balance the trade-off between bias and variance.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Radial Basis Function Networks offer a powerful and flexible framework for addressing non-linear problems across various domains. By leveraging localized responses to input stimuli, RBFNs can model complex relationships within data, making them a valuable tool for tasks requiring high precision in function approximation, classification, and beyond. Understanding and effectively implementing RBF Networks can provide practitioners with a robust method for tackling challenging problems that traditional linear models cannot solve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "32822c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFunction:\n",
    "    def __init__(self, input_size, num_centers):\n",
    "        # Initialize centers and width parameters randomly\n",
    "        self.centers = np.random.rand(num_centers, input_size)\n",
    "        self.width = np.random.rand()\n",
    "        self.weights = np.random.rand(num_centers)\n",
    "    \n",
    "    def gaussian(self, x, center, width):\n",
    "        # Gaussian activation function\n",
    "        return np.exp(-np.sum((x - center)**2) / (2 * width**2))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Calculate the activation for each center\n",
    "        activations = np.array([self.gaussian(inputs, center, self.width) for center in self.centers])\n",
    "        \n",
    "        # Calculate the weighted sum of activations\n",
    "        weighted_sum = np.dot(activations, self.weights)\n",
    "        \n",
    "        # Apply a threshold for binary output\n",
    "        output = 1 if weighted_sum > 0.5 else 0\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04ad0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0.5 0.8]\n",
      "Output: 0\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an RBF network with 2 input cells and 3 centers\n",
    "    rbf_network = RadialBasisFunction(input_size=2, num_centers=3)\n",
    "    \n",
    "    # Example input\n",
    "    input_data = np.array([0.5, 0.8])\n",
    "    \n",
    "    # Get the output from the RBF network\n",
    "    output = rbf_network.forward(input_data)\n",
    "    \n",
    "    print(f\"Input: {input_data}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc75ae",
   "metadata": {},
   "source": [
    "## Deep Learning Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b152802",
   "metadata": {},
   "source": [
    "### Deep Feed Forward (DFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd969e3e",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Deep Feedforward Neural Networks, often simply referred to as Deep Neural Networks (DNNs), are the quintessential deep learning models. These networks extend the concept of the basic feedforward neural network by introducing multiple hidden layers between the input and output layers. This architecture enables the learning of complex patterns and hierarchies in data, making DNNs incredibly effective for a wide range of predictive modeling tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Deep Feedforward Neural Networks are designed to handle:\n",
    "- Numerical data\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "\n",
    "Their flexibility and capacity for high-dimensional data processing make them applicable across nearly all domains of machine learning and artificial intelligence.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "DNNs are particularly proficient in:\n",
    "- Classification\n",
    "- Regression\n",
    "- Pattern recognition\n",
    "- Feature extraction\n",
    "\n",
    "The depth of these networks allows for the modeling of complex relationships in the data, contributing to advancements in areas like computer vision, natural language processing, and more.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of Deep Feedforward Neural Networks is a hallmark of their design. With the ability to adjust the number of layers and nodes within those layers, DNNs can be tailored to the complexity of the task at hand. However, this scalability comes with increased computational demands, necessitating efficient training techniques and hardware acceleration in many cases.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "DNNs exhibit a notable degree of robustness to noise and variability in the input data, thanks to their layered structure and the non-linear transformations applied at each layer. This makes them well-suited for real-world applications where data imperfections are common.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Deep Feedforward Neural Networks can be customized with various activation functions, optimization algorithms, and regularization techniques to improve their performance and generalization ability. Common variants include:\n",
    "- **ReLU-activated DNNs:** Use the Rectified Linear Unit function to introduce non-linearity without the vanishing gradient problem.\n",
    "- **Dropout-regularized DNNs:** Implement dropout layers to reduce overfitting by randomly omitting subsets of features during training.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Deep Feedforward Neural Networks:**\n",
    "- In tasks requiring the modeling of complex relationships or patterns in the data.\n",
    "- When the dataset is large and high-dimensional, providing enough information to train deep models effectively.\n",
    "\n",
    "**Considerations:**\n",
    "- The depth and complexity of DNNs necessitate careful design and training to avoid issues like overfitting and ensure sufficient generalization to new data.\n",
    "- Training deep models can be computationally intensive and time-consuming, requiring appropriate hardware and optimization strategies.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Deep Feedforward Neural Networks are a foundational pillar of modern deep learning, offering unparalleled flexibility and learning capacity. Their ability to learn from and make predictions on complex data has revolutionized many fields of study and industry. By leveraging advanced training techniques and computational resources, practitioners can unlock the full potential of DNNs to solve a vast array of challenging problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "90960c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Feed Forward Neural Network\n",
    "class DeepFeedforwardNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DeepFeedforwardNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(2)  # Two hidden layers with 4 nodes each\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7d138c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFeedforwardNN(\n",
      "  (input_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=4, out_features=4, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "Output: tensor([[0.4140, 0.3992]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the deep feedforward neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 4  # Number of nodes in each hidden layer\n",
    "output_size = 2  # Number of output nodes\n",
    "deep_feedforward_model = DeepFeedforwardNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.tensor([[0.5, 0.3, 0.8]])  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_deep_feedforward = deep_feedforward_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(deep_feedforward_model)\n",
    "print(\"Output:\", output_deep_feedforward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a754a03",
   "metadata": {},
   "source": [
    "# Deep Learning Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0641e",
   "metadata": {},
   "source": [
    "## Core Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f10a3d",
   "metadata": {},
   "source": [
    "### Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918e8c4",
   "metadata": {},
   "source": [
    "#### Autoencoder (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efdf62",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Autoencoders (AEs) are a type of neural network used for unsupervised learning of efficient data codings. The primary goal of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise.” This is achieved by designing the autoencoder to encode inputs into a low-dimensional space and then decode these encodings back into the original input data.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Autoencoders can handle a variety of data types, including:\n",
    "- Numerical data\n",
    "- Images\n",
    "- Audio signals\n",
    "- Text data\n",
    "\n",
    "Their versatility makes them suitable for applications ranging from compression to noise reduction or feature extraction.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "The main applications of AEs include:\n",
    "- Dimensionality reduction\n",
    "- Feature learning\n",
    "- Data compression\n",
    "- Denoising\n",
    "\n",
    "AEs are particularly useful in scenarios where the intrinsic structure of the data needs to be learned without labeled data.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Autoencoders scale well with the complexity of the data and the desired level of compression or feature extraction. The network architecture can be adjusted according to the specific requirements of the task, allowing for flexible implementations that cater to large and high-dimensional datasets.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Autoencoders, especially denoising autoencoders (DAEs), are designed to be robust to noise in the input data. By learning to reconstruct inputs from corrupted versions, they can effectively identify and ignore irrelevant or noisy features in the data.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of autoencoders have been developed to address different challenges, including:\n",
    "- **Variational Autoencoders (VAEs):** Focus on generating new data that is similar to the training data.\n",
    "- **Denoising Autoencoders (DAEs):** Aim to remove noise from corrupted input data.\n",
    "- **Sparse Autoencoders (SAEs):** Introduce sparsity in the encoded representations to improve feature selection.\n",
    "\n",
    "Note: These three variants each have their own sections directly following this.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Autoencoders:**\n",
    "- For tasks requiring data compression without significant loss of information.\n",
    "- When looking to learn efficient representations of data without supervision.\n",
    "- In applications where the removal of noise or the extraction of relevant features from the data is essential.\n",
    "\n",
    "**Considerations:**\n",
    "- The choice of autoencoder variant and network architecture should be aligned with the specific objectives of the task.\n",
    "- Careful tuning of the network parameters is crucial to achieve the desired balance between compression, reconstruction accuracy, and feature learning.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Autoencoders offer a powerful framework for learning efficient representations of data in an unsupervised manner. By leveraging their ability to compress and denoise data, as well as to learn salient features, autoencoders are instrumental in various applications across machine learning and signal processing domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da5f457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61ad5262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (decoder): Linear(in_features=5, out_features=10, bias=True)\n",
      ")\n",
      "Input: tensor([[0.3715, 0.3640, 0.6137, 0.7465, 0.9896, 0.9221, 0.6359, 0.2760, 0.3283,\n",
      "         0.3633]])\n",
      "Output: tensor([[0.6423, 0.6060, 0.5202, 0.5631, 0.5194, 0.5211, 0.4972, 0.5675, 0.5079,\n",
      "         0.6098]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the autoencoder\n",
    "input_size = 10  # Number of input features\n",
    "hidden_size = 5  # Number of hidden nodes (compressed representation)\n",
    "autoencoder = Autoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))\n",
    "\n",
    "# Forward pass to get the reconstructed output\n",
    "output_autoencoder = autoencoder(sample_input)\n",
    "\n",
    "# Print the model architecture, input, and output\n",
    "print(autoencoder)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output:\", output_autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa7a13",
   "metadata": {},
   "source": [
    "#### VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d1ef9",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Variational Autoencoders (VAEs) are a cornerstone in the field of generative AI, representing a powerful class of deep learning models for generative modeling. They are designed to learn the underlying probability distribution of training data, enabling the generation of new data points with similar properties. VAEs combine traditional autoencoder architecture with variational inference principles, allowing them to compress data into a latent space and then generate data by sampling from this space, thereby facilitating a deep exploration of the continuous latent space representing the data.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "VAEs demonstrate remarkable adaptability across a range of data types, including:\n",
    "- Images\n",
    "- Text\n",
    "- Audio\n",
    "- Continuous numerical data\n",
    "\n",
    "This versatility underscores their prominence in generative AI, making them a popular choice for a wide array of generative tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Emphasizing their role in generative AI, VAEs excel in:\n",
    "- Data generation\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Anomaly detection\n",
    "\n",
    "Their deep learning capabilities enable them not only to model complex distributions but also to generate new, coherent samples, showcasing the transformative potential of generative AI.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "With their deep neural network architecture, VAEs scale effectively to accommodate the complexity and volume of vast datasets, further solidifying their status in generative AI for handling high-dimensional data efficiently.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "VAEs' proficiency in denoising and reconstructing inputs highlights their robustness, making them invaluable for applications in generative AI where data cleanliness cannot be assured.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Reflecting the innovation in generative AI, various VAE models have been developed to address specific challenges or improve upon the original framework, including Conditional VAEs, Beta-VAEs, and Disentangled VAEs, each offering unique advantages for controlled data generation and enhanced interpretability of latent representations.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "In the realm of generative AI, VAEs are particularly suited for:\n",
    "- Generating new data that mimics the properties of specific datasets.\n",
    "- Unsupervised learning of complex data distributions.\n",
    "- Applications requiring a nuanced understanding of data's underlying structure.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "Training VAEs can present challenges, such as mode collapse, underscoring the need for expertise in generative AI to navigate these complexities successfully.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Variational Autoencoders (VAEs) have cemented their place as a fundamental technology in generative AI, offering a sophisticated mechanism for understanding and generating data. Their broad applicability and the depth of insight they provide into data's inherent structure make them a pivotal tool in the advancement of generative modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47d4a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.encoder_fc2_mean = nn.Linear(hidden_size, hidden_size)\n",
    "        self.encoder_fc2_logvar = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "        self.decoder_fc2 = nn.Linear(input_size, input_size)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = torch.relu(self.encoder_fc1(x))\n",
    "        mean = self.encoder_fc2_mean(x)\n",
    "        logvar = self.encoder_fc2_logvar(x)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "\n",
    "        # Decoder\n",
    "        x_hat = torch.relu(self.decoder_fc1(z))\n",
    "        x_hat = torch.sigmoid(self.decoder_fc2(x_hat))\n",
    "\n",
    "        return x_hat, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9fb16f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (encoder_fc2_mean): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (encoder_fc2_logvar): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc2): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Input: tensor([[0.1337, 0.7385, 0.8755, 0.9708]])\n",
      "Output: tensor([[0.5971, 0.4805, 0.5539, 0.5382]], grad_fn=<SigmoidBackward0>)\n",
      "Mean: tensor([[-0.1706,  0.1547,  0.3118,  0.1452]], grad_fn=<AddmmBackward0>)\n",
      "Log Variance: tensor([[ 0.2295, -0.0220,  0.3528,  0.0078]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the variational autoencoder\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 4  # Number of hidden nodes in probabilistic layer\n",
    "vae = VariationalAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))\n",
    "\n",
    "# Forward pass to get the reconstructed output and latent variables\n",
    "output_vae, mean, logvar = vae(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(vae)\n",
    "\n",
    "print(\"\\nInput:\", sample_input)\n",
    "print(\"Output:\", output_vae)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Log Variance:\", logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be943473",
   "metadata": {},
   "source": [
    "#### DAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcfc27f",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Denoising Autoencoders (DAEs) are an advanced type of autoencoder designed to *remove noise from input data*. By intentionally corrupting the input data and then learning to reconstruct the original, uncorrupted data, DAEs are trained to capture the most relevant features. This process enhances the model's ability to generalize from the data, making it highly effective for tasks that require robust feature extraction and data denoising capabilities.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Denoising Autoencoders are capable of processing various data types, including:\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "- Continuous numerical data\n",
    "\n",
    "Their adaptability makes them particularly useful for applications involving noisy or incomplete data.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Denoising Autoencoders are primarily used for:\n",
    "- Data denoising\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Data generation and enhancement\n",
    "\n",
    "By learning to ignore the \"noise\" in data, DAEs excel in recovering clean representations from corrupted inputs.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Similar to other autoencoders, the scalability of DAEs depends on the network architecture. Modern techniques and computational resources allow DAEs to handle large datasets and complex noise patterns effectively, showcasing their scalability in practical applications.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "The core strength of DAEs lies in their robustness to noise. They are specifically trained to identify and ignore irrelevant features (noise), focusing on reconstructing the essential aspects of the data, which makes them exceptionally reliable for denoising tasks.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of DAEs have been developed to address different types of noise or to enhance specific aspects of denoising, including:\n",
    "- **Gaussian Noise DAEs:** Target Gaussian noise in the data.\n",
    "- **Salt-and-Pepper Noise DAEs:** Designed to remove binary noise from images.\n",
    "- **Variational DAEs:** Combine denoising capabilities with variational autoencoder frameworks for improved generative properties.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Denoising Autoencoders:**\n",
    "- For cleaning noisy data before further processing or analysis.\n",
    "- In feature extraction tasks where maintaining data integrity is crucial.\n",
    "- As a preprocessing step to improve the performance of subsequent machine learning models.\n",
    "\n",
    "**Considerations:**\n",
    "- The effectiveness of a DAE can vary based on the noise type and level; selecting the appropriate model variant is key.\n",
    "- Training DAEs requires a balance between denoising capability and preserving relevant features, necessitating careful tuning of model parameters.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Denoising Autoencoders offer a powerful solution for improving data quality, with their unique training strategy enabling them to extract clean, relevant features from noisy inputs. Their versatility across different data types and robustness to various noise patterns make them an invaluable tool in the data preprocessing pipeline, enhancing the performance of machine learning and deep learning models across a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d42b753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = torch.relu(self.encoder_fc1(x))\n",
    "\n",
    "        # Decoder\n",
    "        x_hat = torch.sigmoid(self.decoder_fc1(x))\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "68ddb586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenoisingAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Noisy Input: tensor([[0.8203, 0.0225, 0.3758, 0.4193]])\n",
      "Reconstructed Output: tensor([[0.6731, 0.5674, 0.3965, 0.5406]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the denoising autoencoder\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 4  # Number of hidden nodes\n",
    "dae = DenoisingAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input (noisy data)\n",
    "noisy_input = torch.rand((1, input_size))  # Example Noisy Data\n",
    "\n",
    "# Forward pass to get the reconstructed output\n",
    "output_dae = dae(noisy_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(dae)\n",
    "print(\"\\nNoisy Input:\", noisy_input)\n",
    "print(\"Reconstructed Output:\", output_dae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c614633",
   "metadata": {},
   "source": [
    "#### SAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdf86b",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Sparse Autoencoders represent a specialized variant of autoencoders, aimed at *unsupervised learning of compressed representations*. By introducing sparsity constraints, they enforce most neurons to be inactive, enhancing feature detection and data representation efficiency. This approach improves generalization, making them suitable for tasks requiring robust feature extraction.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Sparse Autoencoders efficiently process:\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "- Continuous numerical data\n",
    "\n",
    "Their versatility across different data types highlights their utility in feature extraction and data compression tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Key applications include:\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Data denoising\n",
    "- Pretraining for deeper neural networks\n",
    "\n",
    "Sparsity constraints enable these models to learn higher-level features, distinguishing them from traditional autoencoders.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Despite sparsity aiding in learning efficient representations, the network's size and depth impact its ability to model complex distributions and computational requirements.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "They demonstrate significant robustness to noise, attributed to their focus on essential features, making them ideal for denoising and robust representation learning.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Variants are based on the sparsity enforcement method:\n",
    "- **KL Divergence Sparse Autoencoder:** Penalizes deviations from a target sparsity level using Kullback-Leibler divergence.\n",
    "- **L1 Regularization Sparse Autoencoder:** Applies L1 penalty on hidden units' activations to encourage sparsity.\n",
    "- **Winner-Take-All (WTA) Sparse Autoencoder:** Only a fraction of the most active hidden units are allowed to update their weights, enhancing sparsity.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Sparse Autoencoders:**\n",
    "- In extracting meaningful features from high-dimensional data.\n",
    "- For dimensionality reduction with interpretability.\n",
    "- During pretraining phases for deep learning models, providing a good initial weight set that captures useful data patterns.\n",
    "\n",
    "**Considerations:**\n",
    "- Selecting the appropriate sparsity constraint and regularization technique is critical for balancing feature selectivity and model complexity.\n",
    "- Hyperparameters require careful tuning to achieve desired sparsity levels and optimal performance.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Sparse Autoencoders stand out for learning efficient and interpretable data representations, with enforced sparsity offering clear advantages in feature selection and model robustness. They are invaluable in preprocessing, feature extraction, and as a pretraining step, enhancing subsequent models' performance across various data types and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8eb9d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, sparsity_target=0.1, sparsity_weight=0.2):\n",
    "        super(SparseAutoencoder, self).__init__(input_size, hidden_size, output_size=input_size)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        # Sparsity parameters\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoded = self.encoder_fc1(x)\n",
    "        encoded = self.relu(encoded)\n",
    "\n",
    "        # Decoder\n",
    "        decoded = torch.sigmoid(self.decoder_fc1(encoded))\n",
    "\n",
    "        return decoded, encoded\n",
    "\n",
    "    def loss_function(self, x, x_hat, encoded):\n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='mean')\n",
    "\n",
    "        # Sparsity loss\n",
    "        sparsity_loss = torch.sum(self.kl_divergence(self.sparsity_target, encoded))\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + self.sparsity_weight * sparsity_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def kl_divergence(self, target, activations):\n",
    "        # KL Divergence to enforce sparsity\n",
    "        p = torch.mean(activations, dim=0)  # Average activation over the dataset\n",
    "        return target * torch.log(target / p) + (1 - target) * torch.log((1 - target) / (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b99e817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Input: tensor([[0.9143, 0.2609, 0.5831, 0.4264, 0.9313]])\n",
      "Reconstructed Output: tensor([[0.5156, 0.4548, 0.5337, 0.5076, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "Encoded Representation: tensor([[0.1736, 0.2612, 0.0377]], grad_fn=<ReluBackward0>)\n",
      "Loss: 0.7090073823928833\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the sparse autoencoder\n",
    "input_size = 5  # Number of input features\n",
    "hidden_size = 3  # Number of hidden nodes\n",
    "sae = SparseAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the reconstructed output and encoded representation\n",
    "output_sae, encoded_sae = sae(sample_input)\n",
    "\n",
    "# Calculate the loss\n",
    "loss_sae = sae.loss_function(sample_input, output_sae, encoded_sae)\n",
    "\n",
    "# Print the model architecture, output, and loss\n",
    "print(sae)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Reconstructed Output:\", output_sae)\n",
    "print(\"Encoded Representation:\", encoded_sae)\n",
    "print(\"Loss:\", loss_sae.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3a3b8",
   "metadata": {},
   "source": [
    "### Convolutional Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc1a1e",
   "metadata": {},
   "source": [
    "#### Deep Convolutional Network (DCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32ef74",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Deep Convolutional Networks (DCNs), also known as Convolutional Neural Networks (CNNs), are a specialized kind of neural network for processing data that has a known grid-like topology. \n",
    "\n",
    "Examples include time-series data, which can be thought of as a 1D grid taking samples at regular time intervals, and image data, which can be seen as a 2D grid of pixels.\n",
    "\n",
    "DCNs have been instrumental in many advances in computer vision, achieving remarkable success in tasks such as image recognition, object detection, and more.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Deep Convolutional Networks are particularly adept at handling:\n",
    "- Image data\n",
    "- Video sequences\n",
    "- Time-series data\n",
    "- Any data that can be represented in a grid-like structure (e.g., sound waves visualized in spectrograms)\n",
    "\n",
    "Their ability to automatically and adaptively learn spatial hierarchies of features makes them highly effective for tasks involving visual inputs.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "DCNs excel in a variety of tasks, including but not limited to:\n",
    "- Image and video recognition\n",
    "- Image classification\n",
    "- Object detection\n",
    "- Semantic segmentation\n",
    "- Natural language processing (when applied to text data in a convolutional manner)\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "One of the key strengths of DCNs is their scalability, not only in terms of handling large volumes of data but also in terms of their capacity to learn from complex and high-dimensional datasets. Their architecture, characterized by layers with convolutions, pooling, and often followed by fully connected layers, allows for efficient training and inference.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "DCNs are known for their robustness to variations and noise in the input data, making them particularly suitable for real-world applications where data imperfection is common. This robustness stems from their ability to learn invariant features that are critical for recognition tasks.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "There are several popular variants and architectures of DCNs, including:\n",
    "- **LeNet:** One of the first convolutional networks that demonstrated the effectiveness of convolutional layers.\n",
    "- **AlexNet:** The network that revitalized interest in convolutional neural networks with its success in the ImageNet challenge.\n",
    "- **VGGNet:** Known for its simplicity and deep architecture.\n",
    "- **ResNet:** Introduced residual connections to enable the training of very deep networks.\n",
    "- **Inception (GoogleNet):** Known for its efficiency and depth with a lower number of parameters.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Deep Convolutional Networks:**\n",
    "- In applications involving image or video processing where capturing spatial hierarchies is crucial.\n",
    "- For tasks requiring the extraction of complex features from large and high-dimensional datasets.\n",
    "\n",
    "**Considerations:**\n",
    "- The design of the network architecture and the choice of hyperparameters are critical for achieving optimal performance.\n",
    "- Training deep convolutional networks requires substantial computational resources, particularly for large datasets and complex models.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Deep Convolutional Networks have revolutionized the field of computer vision and beyond, demonstrating unparalleled success in a wide range of applications involving visual data processing. Their ability to learn powerful representations of data makes them a cornerstone of modern deep learning, with ongoing research pushing the boundaries of what is possible in artificial intelligence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f281322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(BaseNN):\n",
    "    def __init__(self, input_channels, num_classes, image_size):\n",
    "        hidden_size = 64\n",
    "\n",
    "        super(DeepCNN, self).__init__(input_size=image_size, hidden_size=hidden_size, output_size=num_classes)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * (image_size // 4) * (image_size // 4), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7bd5c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Input: tensor([[[[0.8416, 0.7502, 0.6943, 0.9990, 0.1352, 0.9379, 0.5224, 0.0016,\n",
      "           0.4856, 0.2969, 0.5485, 0.5109, 0.1969, 0.6347, 0.5837, 0.2544,\n",
      "           0.9944, 0.6543, 0.0190, 0.2203, 0.7790, 0.7113, 0.4394, 0.2329,\n",
      "           0.7536, 0.3462, 0.4982, 0.6461],\n",
      "          [0.5926, 0.3453, 0.2040, 0.7335, 0.4989, 0.1730, 0.3417, 0.2966,\n",
      "           0.7529, 0.1638, 0.9416, 0.3735, 0.0394, 0.3155, 0.2703, 0.8233,\n",
      "           0.3482, 0.2404, 0.7496, 0.3054, 0.6513, 0.2599, 0.6073, 0.3550,\n",
      "           0.0385, 0.3623, 0.4530, 0.1841],\n",
      "          [0.2913, 0.3471, 0.9294, 0.3384, 0.0483, 0.4575, 0.4068, 0.4923,\n",
      "           0.7009, 0.6776, 0.1894, 0.5771, 0.5684, 0.5581, 0.9404, 0.4589,\n",
      "           0.0707, 0.0925, 0.2207, 0.6587, 0.6358, 0.9302, 0.4232, 0.3691,\n",
      "           0.6835, 0.0965, 0.6459, 0.4436],\n",
      "          [0.4070, 0.9368, 0.3156, 0.7596, 0.2886, 0.3066, 0.5184, 0.4725,\n",
      "           0.0213, 0.4496, 0.4341, 0.7180, 0.7686, 0.6849, 0.5156, 0.8390,\n",
      "           0.1676, 0.5970, 0.6066, 0.9950, 0.9801, 0.8012, 0.4557, 0.4834,\n",
      "           0.5267, 0.1229, 0.3415, 0.4955],\n",
      "          [0.6570, 0.1333, 0.4218, 0.6022, 0.9158, 0.5591, 0.5143, 0.7292,\n",
      "           0.8901, 0.8358, 0.1350, 0.2790, 0.8240, 0.4391, 0.6300, 0.8983,\n",
      "           0.2904, 0.6859, 0.1936, 0.4081, 0.5594, 0.6237, 0.3811, 0.1804,\n",
      "           0.9318, 0.7457, 0.7483, 0.7708],\n",
      "          [0.5658, 0.1252, 0.3710, 0.2168, 0.1354, 0.9895, 0.6651, 0.4854,\n",
      "           0.3141, 0.5700, 0.0055, 0.2448, 0.0686, 0.7780, 0.6278, 0.0592,\n",
      "           0.0150, 0.0525, 0.0866, 0.0936, 0.0653, 0.4216, 0.1205, 0.7400,\n",
      "           0.8600, 0.7120, 0.2445, 0.5148],\n",
      "          [0.5381, 0.3987, 0.4705, 0.6764, 0.7106, 0.5447, 0.3698, 0.9965,\n",
      "           0.6417, 0.1845, 0.4670, 0.0862, 0.5503, 0.5653, 0.5581, 0.6733,\n",
      "           0.7498, 0.9874, 0.8429, 0.4261, 0.9807, 0.5277, 0.6155, 0.1803,\n",
      "           0.6816, 0.5809, 0.2713, 0.8532],\n",
      "          [0.6091, 0.6286, 0.8578, 0.3325, 0.7785, 0.0637, 0.8308, 0.2106,\n",
      "           0.4175, 0.4320, 0.1668, 0.9548, 0.3572, 0.6553, 0.4444, 0.7637,\n",
      "           0.2268, 0.0802, 0.4567, 0.3886, 0.2579, 0.5084, 0.5214, 0.5218,\n",
      "           0.3594, 0.9874, 0.8985, 0.2920],\n",
      "          [0.7988, 0.1092, 0.2722, 0.8905, 0.1588, 0.2165, 0.6245, 0.8128,\n",
      "           0.1305, 0.8879, 0.3957, 0.5638, 0.1872, 0.6641, 0.1207, 0.7493,\n",
      "           0.2266, 0.8582, 0.7396, 0.5715, 0.6918, 0.4061, 0.3173, 0.4066,\n",
      "           0.3342, 0.7291, 0.5846, 0.2585],\n",
      "          [0.3183, 0.6172, 0.7637, 0.5187, 0.6812, 0.6607, 0.9046, 0.9796,\n",
      "           0.3833, 0.5717, 0.8070, 0.1863, 0.4633, 0.5917, 0.5899, 0.1251,\n",
      "           0.8972, 0.8416, 0.1250, 0.9371, 0.8810, 0.7214, 0.1408, 0.4738,\n",
      "           0.0966, 0.6562, 0.8841, 0.0252],\n",
      "          [0.5703, 0.1100, 0.0096, 0.7067, 0.2312, 0.4618, 0.8225, 0.2609,\n",
      "           0.4470, 0.0478, 0.8180, 0.2662, 0.4064, 0.7971, 0.3230, 0.6549,\n",
      "           0.5139, 0.2292, 0.8878, 0.7623, 0.7119, 0.3801, 0.4666, 0.2746,\n",
      "           0.9054, 0.5411, 0.2401, 0.2702],\n",
      "          [0.5271, 0.4513, 0.2630, 0.9955, 0.5424, 0.6414, 0.9787, 0.2198,\n",
      "           0.7390, 0.9203, 0.9526, 0.2521, 0.1699, 0.7213, 0.3826, 0.9203,\n",
      "           0.1774, 0.2361, 0.9956, 0.5058, 0.3289, 0.2344, 0.5601, 0.7075,\n",
      "           0.0877, 0.0558, 0.7953, 0.3158],\n",
      "          [0.2998, 0.7603, 0.5297, 0.3792, 0.8325, 0.3622, 0.2817, 0.4290,\n",
      "           0.9445, 0.6769, 0.1676, 0.0492, 0.8691, 0.8211, 0.4815, 0.1814,\n",
      "           0.7912, 0.8868, 0.0949, 0.1503, 0.9570, 0.1816, 0.2795, 0.1350,\n",
      "           0.4360, 0.7438, 0.1164, 0.2654],\n",
      "          [0.6751, 0.1343, 0.6919, 0.3123, 0.6564, 0.1619, 0.7228, 0.4081,\n",
      "           0.9075, 0.4547, 0.4383, 0.3512, 0.8705, 0.6377, 0.0429, 0.2460,\n",
      "           0.4362, 0.7092, 0.4798, 0.1328, 0.3322, 0.8965, 0.1430, 0.7267,\n",
      "           0.4701, 0.3669, 0.6021, 0.4577],\n",
      "          [0.4165, 0.2244, 0.3704, 0.2285, 0.7839, 0.7773, 0.5840, 0.0355,\n",
      "           0.9028, 0.7229, 0.3849, 0.6205, 0.5678, 0.3210, 0.9566, 0.8960,\n",
      "           0.5650, 0.3271, 0.5935, 0.4548, 0.8319, 0.7744, 0.4613, 0.1200,\n",
      "           0.0265, 0.5457, 0.3282, 0.9983],\n",
      "          [0.4436, 0.4795, 0.1795, 0.2335, 0.0682, 0.2645, 0.9969, 0.5190,\n",
      "           0.1064, 0.2322, 0.0021, 0.0168, 0.4070, 0.7575, 0.7711, 0.0974,\n",
      "           0.3536, 0.9053, 0.9240, 0.9821, 0.4492, 0.5543, 0.4798, 0.3606,\n",
      "           0.8059, 0.3637, 0.7386, 0.4053],\n",
      "          [0.6294, 0.5792, 0.0948, 0.5129, 0.1014, 0.9251, 0.5525, 0.5548,\n",
      "           0.6616, 0.5889, 0.7795, 0.9108, 0.1208, 0.7003, 0.0474, 0.1541,\n",
      "           0.5375, 0.2582, 0.7026, 0.4884, 0.0952, 0.8791, 0.2646, 0.4334,\n",
      "           0.4056, 0.0633, 0.7993, 0.7085],\n",
      "          [0.6559, 0.7679, 0.4948, 0.4288, 0.7852, 0.0016, 0.3190, 0.0383,\n",
      "           0.3658, 0.1517, 0.7414, 0.7769, 0.1659, 0.7156, 0.6424, 0.8502,\n",
      "           0.6627, 0.8331, 0.6857, 0.7459, 0.0817, 0.1824, 0.4505, 0.1886,\n",
      "           0.6519, 0.7536, 0.8986, 0.0757],\n",
      "          [0.9317, 0.8827, 0.6698, 0.2760, 0.6700, 0.6646, 0.0999, 0.4184,\n",
      "           0.8013, 0.0247, 0.3404, 0.4319, 0.3892, 0.8015, 0.4310, 0.6322,\n",
      "           0.4850, 0.8265, 0.6935, 0.5566, 0.2469, 0.8890, 0.5816, 0.0586,\n",
      "           0.4579, 0.5636, 0.6667, 0.1236],\n",
      "          [0.6687, 0.0240, 0.5314, 0.8643, 0.6576, 0.9878, 0.6824, 0.6208,\n",
      "           0.6330, 0.5491, 0.6792, 0.1537, 0.0799, 0.4963, 0.0617, 0.9591,\n",
      "           0.3754, 0.1960, 0.8202, 0.7197, 0.8737, 0.2412, 0.5602, 0.4578,\n",
      "           0.3402, 0.9828, 0.4472, 0.7535],\n",
      "          [0.3623, 0.9181, 0.4652, 0.2223, 0.3627, 0.4007, 0.5702, 0.8368,\n",
      "           0.1507, 0.9298, 0.5610, 0.5867, 0.4616, 0.4400, 0.0462, 0.6890,\n",
      "           0.7158, 0.4127, 0.0152, 0.9404, 0.8848, 0.2008, 0.3202, 0.5168,\n",
      "           0.4856, 0.5385, 0.6037, 0.0696],\n",
      "          [0.9791, 0.7035, 0.6164, 0.4198, 0.1942, 0.1743, 0.1047, 0.7804,\n",
      "           0.5493, 0.2173, 0.2055, 0.5244, 0.3351, 0.2842, 0.9262, 0.5787,\n",
      "           0.5587, 0.7755, 0.2461, 0.5011, 0.9635, 0.8425, 0.5987, 0.4135,\n",
      "           0.7146, 0.0737, 0.6906, 0.1933],\n",
      "          [0.8943, 0.0348, 0.0408, 0.0501, 0.8890, 0.3707, 0.5159, 0.0922,\n",
      "           0.8439, 0.1610, 0.4323, 0.4762, 0.2455, 0.2928, 0.7514, 0.6683,\n",
      "           0.6403, 0.7967, 0.3001, 0.9332, 0.6741, 0.0756, 0.9144, 0.0078,\n",
      "           0.4853, 0.2274, 0.1044, 0.5146],\n",
      "          [0.8367, 0.0694, 0.6086, 0.7970, 0.2208, 0.3871, 0.3397, 0.2040,\n",
      "           0.1223, 0.1461, 0.3676, 0.4004, 0.8814, 0.6601, 0.5076, 0.0940,\n",
      "           0.2433, 0.0530, 0.1600, 0.5388, 0.4397, 0.0326, 0.5876, 0.3615,\n",
      "           0.3715, 0.8437, 0.2684, 0.3578],\n",
      "          [0.2255, 0.4089, 0.8085, 0.0718, 0.7603, 0.9349, 0.8498, 0.6927,\n",
      "           0.5739, 0.3994, 0.1321, 0.8557, 0.7877, 0.9112, 0.8814, 0.0753,\n",
      "           0.4909, 0.6125, 0.4659, 0.1295, 0.1378, 0.1649, 0.3201, 0.2632,\n",
      "           0.4651, 0.2938, 0.6093, 0.8722],\n",
      "          [0.3187, 0.4533, 0.1730, 0.9946, 0.4698, 0.7332, 0.8458, 0.7650,\n",
      "           0.1439, 0.6307, 0.0089, 0.0504, 0.1790, 0.0086, 0.0127, 0.6214,\n",
      "           0.9140, 0.6212, 0.3908, 0.5264, 0.9631, 0.5305, 0.7615, 0.6833,\n",
      "           0.8383, 0.0818, 0.9267, 0.9149],\n",
      "          [0.2606, 0.3893, 0.0818, 0.1263, 0.2127, 0.4098, 0.3625, 0.6794,\n",
      "           0.7117, 0.4749, 0.1346, 0.9043, 0.8045, 0.0137, 0.0679, 0.9400,\n",
      "           0.0321, 0.1405, 0.0635, 0.1485, 0.4557, 0.0796, 0.6698, 0.1212,\n",
      "           0.6503, 0.5455, 0.6831, 0.2782],\n",
      "          [0.9477, 0.0065, 0.2441, 0.0848, 0.2276, 0.8872, 0.6882, 0.7836,\n",
      "           0.0074, 0.7577, 0.2640, 0.2421, 0.7162, 0.5993, 0.1743, 0.9630,\n",
      "           0.7913, 0.2703, 0.3372, 0.5729, 0.2333, 0.6812, 0.9730, 0.9695,\n",
      "           0.4245, 0.5867, 0.6817, 0.1858]]]])\n",
      "Output Scores: [[ 0.00709009  0.08823158  0.05249883  0.04421477 -0.08970241 -0.08476198\n",
      "   0.02083667 -0.12077667  0.06137786  0.06034416]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_channels = 1  # Grayscale images\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "\n",
    "deep_cnn_model = DeepCNN(input_channels, num_classes, image_size)\n",
    "\n",
    "sample_image = torch.rand((1, input_channels, image_size, image_size))\n",
    "\n",
    "output_scores = deep_cnn_model(sample_image)\n",
    "\n",
    "print(deep_cnn_model)\n",
    "print(\"Input:\", sample_image)\n",
    "print(\"Output Scores:\", output_scores.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3267e2d",
   "metadata": {},
   "source": [
    "#### Deconvolutional Network (DN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4318f26",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Deconvolutional Networks (DNs) are a pivotal architecture in the domain of deep learning, focusing on the task of reconstructing or generating data from compressed or encoded representations. Unlike their counterparts that primarily deal with the analysis and feature extraction from data, DNs excel in the reverse process — they are adept at constructing detailed and high-resolution outputs from lower-dimensional data. This capability renders them particularly valuable in applications requiring precise reconstruction of visual details, such as image super-resolution, segmentation, and various generative tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Deconvolutional Networks are versatile in handling data types that benefit from detailed reconstruction, including:\n",
    "- Image data, for tasks like super-resolution and texture synthesis\n",
    "- Video data, for frame interpolation and enhancement\n",
    "- Any spatial or structured data where the reconstruction or generation of detailed features is essential\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "The primary objectives of DNs revolve around:\n",
    "- Image and video reconstruction and generation\n",
    "- Semantic segmentation for detailed image analysis\n",
    "- Inverse problems in imaging, such as denoising, deblurring, and more\n",
    "- Feature learning with a focus on reconstructing hierarchical spatial patterns\n",
    "\n",
    "Their architecture is tailored to incrementally upscale and refine the spatial resolution of the input, making them indispensable for high-fidelity visual content generation.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Deconvolutional Networks demonstrate impressive scalability, particularly in generating high-resolution outputs from compact representations. This scalability is facilitated by their ability to efficiently learn and apply spatial hierarchies in data, making them suitable for tasks demanding high levels of detail and precision in the reconstruction process.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "DNs exhibit a commendable robustness to noise, leveraging their hierarchical learning structure to filter and refine inputs through the reconstruction process. This attribute is particularly beneficial in applications such as image restoration, where the goal is to recover high-quality visuals from degraded inputs.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several notable variants and implementations of Deconvolutional Networks include:\n",
    "- **Convolutional Autoencoders:** Incorporate deconvolutional layers in the decoder phase to reconstruct data from encoded representations.\n",
    "- **Generative Adversarial Networks (GANs):** Utilize deconvolutional layers within the generator component to create realistic images from noise.\n",
    "- **U-Net:** A specific architecture designed for biomedical image segmentation, employing a combination of convolutional and deconvolutional layers for precise feature reconstruction.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Deconvolutional Networks:**\n",
    "- In projects aiming for the generation or detailed reconstruction of images, videos, or any structured spatial data.\n",
    "- Where the application demands the restoration of high-fidelity visual information, such as in medical imaging enhancement or photographic image improvement.\n",
    "\n",
    "**Considerations:**\n",
    "- Designing and training DNs requires a balance between reconstruction detail and computational efficiency.\n",
    "- Despite their effectiveness, DNs may produce artifacts in the generated outputs; incorporating regularization techniques or additional constraints during training can enhance the quality of the results.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Deconvolutional Networks stand out in the landscape of deep learning architectures for their unique ability to generate and reconstruct detailed spatial data. Their role is instrumental in pushing the boundaries of what's achievable in visual data processing, making them a cornerstone technology in the fields of computer vision and generative modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "581681e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDeconvNet(BaseNN):\n",
    "    def __init__(self, input_channels, output_channels, output_size):\n",
    "        hidden_size = 64\n",
    "\n",
    "        super(DeepDeconvNet, self).__init__(input_size=None, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, 64 * (output_size // 4) * (output_size // 4))\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=output_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 64, (self.output_size // 4), (self.output_size // 4))\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.sigmoid(self.deconv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0aeb5a",
   "metadata": {},
   "source": [
    "notes on parameters above: \n",
    "\n",
    "    channels: in context of images, 1=grayscale, 3=RGB\n",
    "    kernel_size: size of convolutional kernel-filter, size of local region considered for each convolutional operation\n",
    "    stride: step-size\n",
    "    padding: zero-padding addied to input of each side, helps maintain/adjust spatial dimensions\n",
    "    output_size: shape of output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "797a279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepDeconvNet(\n",
      "  (fc1): Linear(in_features=64, out_features=3136, bias=True)\n",
      "  (deconv1): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv2): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      ")\n",
      "\n",
      "Sample input: tensor([[0.4380, 0.4187, 0.6530, 0.1150, 0.5291, 0.1683, 0.8574, 0.2010, 0.6640,\n",
      "         0.2806, 0.0871, 0.0788, 0.3152, 0.6341, 0.7846, 0.6743, 0.9100, 0.4062,\n",
      "         0.7622, 0.9301, 0.9135, 0.5297, 0.0706, 0.8812, 0.0376, 0.9353, 0.8289,\n",
      "         0.6439, 0.1777, 0.1155, 0.0237, 0.0496, 0.9864, 0.4551, 0.0065, 0.4626,\n",
      "         0.5333, 0.3711, 0.6614, 0.0536, 0.6321, 0.4212, 0.5704, 0.7844, 0.8846,\n",
      "         0.8827, 0.8305, 0.1142, 0.7229, 0.2315, 0.1688, 0.8104, 0.7483, 0.5463,\n",
      "         0.9333, 0.4758, 0.0061, 0.2297, 0.0740, 0.2641, 0.3769, 0.9194, 0.6599,\n",
      "         0.8781]])\n",
      "\n",
      "Output Image Shape: torch.Size([1, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_channels = 1  # Grayscale images\n",
    "output_channels = 3  # Number of channels in the output image (e.g., RGB)\n",
    "output_size = 28\n",
    "\n",
    "deep_deconv_model = DeepDeconvNet(input_channels, output_channels, output_size)\n",
    "\n",
    "sample_latent_vector = torch.rand((1, deep_deconv_model.hidden_size))\n",
    "\n",
    "output_image = deep_deconv_model(sample_latent_vector)\n",
    "\n",
    "print(deep_deconv_model)\n",
    "print(\"\\nSample input:\", sample_latent_vector)\n",
    "print(\"\\nOutput Image Shape:\", output_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2137100",
   "metadata": {},
   "source": [
    "#### Deep Convolutional Inverse Graphics Network (DCIGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483df6e",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Deep Convolutional Inverse Graphics Networks (DCIGNs) represent an advanced intersection between computer graphics and convolutional neural networks (CNNs), aimed at understanding and manipulating high-dimensional visual data through an inverse graphics framework. DCIGNs are designed to infer the latent graphical representations (such as viewpoint, lighting, and shape) of objects from images, essentially learning to reverse-engineer the process by which images are generated. This approach enables the network to perform tasks such as 3D reconstruction, pose estimation, and lighting adjustment from 2D images, embodying a significant step towards endowing machines with a deeper understanding of the visual world.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "DCIGNs are particularly adept at processing:\n",
    "- Image data\n",
    "- Visual patterns that require interpretation of 3D properties from 2D representations\n",
    "\n",
    "Their design caters to applications involving complex visual understanding, where the extraction and manipulation of underlying graphical components are necessary.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "DCIGNs are utilized in a variety of applications, including but not limited to:\n",
    "- 3D object reconstruction from single or multiple 2D images\n",
    "- Pose estimation and alignment of objects in images\n",
    "- Lighting and texture editing for realistic image manipulation\n",
    "- Scene understanding and segmentation based on geometric cues\n",
    "\n",
    "These tasks demonstrate DCIGNs' capacity to bridge the gap between conventional image analysis and the computational understanding of the physical properties of the scene.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of DCIGNs hinges on their ability to process and interpret high-dimensional data efficiently. While their sophisticated architecture allows for the extraction of intricate graphical details, the computational complexity can escalate with the increase in resolution and the depth of the graphical modeling required. Optimizations and advancements in network design continue to enhance their scalability and efficiency.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "DCIGNs exhibit a commendable level of robustness to noise, attributed to their convolutional nature and the ability to infer latent graphical models from visual data. This robustness enables them to discern and reconstruct the underlying graphical properties of objects even in the presence of visual distortions or incomplete information.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "In the evolving landscape of DCIGNs, several variants have been proposed to address specific challenges and improve performance:\n",
    "- Variations tailored for specific types of graphical properties (e.g., focused on lighting or texture)\n",
    "- Hybrid models that integrate DCIGNs with other neural network architectures to enhance understanding of complex scenes\n",
    "- Incremental learning approaches that allow DCIGNs to refine their graphical inference capabilities over time\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Consider DCIGNs:**\n",
    "- In projects requiring the detailed understanding and manipulation of images' graphical properties.\n",
    "- When aiming to bridge the understanding between 2D visual data and its 3D graphical representation.\n",
    "\n",
    "**Considerations:**\n",
    "- The complexity of DCIGNs necessitates a solid foundation in both deep learning and computer graphics principles.\n",
    "- Practical applications should balance the computational demands with the expected gains in graphical interpretation and manipulation capabilities.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Deep Convolutional Inverse Graphics Networks stand at the forefront of merging deep learning with computer graphics, offering innovative solutions for 3D interpretation and manipulation of 2D images. As research and development in this field continue, DCIGNs hold the promise of revolutionizing how machines understand and interact with the visual world, paving the way for advanced applications in virtual reality, augmented reality, and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9cb536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCIGN(BaseNN):\n",
    "    def __init__(self, input_channels, input_size, output_size):\n",
    "        hidden_size = 64 \n",
    "\n",
    "        super(DCIGN, self).__init__(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the size of the flattened output after convolutional and pooling layers\n",
    "        self.flattened_size = 64 * (input_size // 4) * (input_size // 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8546e78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor: tensor([[-0.1344, -0.1561, -0.1141,  0.0499, -0.0735,  0.1353,  0.0277, -0.1027,\n",
      "          0.1841, -0.1267]], grad_fn=<AddmmBackward0>)\n",
      "Output Shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "input_channels = 3  # for RGB images\n",
    "input_size = 32  # Example size, adjust as needed\n",
    "output_size = 10  # Example output size\n",
    "\n",
    "dcign = DCIGN(input_channels, input_size, output_size)\n",
    "\n",
    "sample_input = torch.randn(1, input_channels, input_size, input_size)\n",
    "\n",
    "output = dcign(sample_input)\n",
    "\n",
    "print(\"Output Tensor:\", output)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59105c",
   "metadata": {},
   "source": [
    "## Recurrent and Memory Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f57745",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e62267",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks that excel at processing sequential data, making them particularly well-suited for tasks involving time-series data, natural language processing, and any scenario where the context or the order of elements is crucial.\n",
    "\n",
    "RNNs are characterized by their ability to maintain a 'memory' of previous inputs by incorporating loops within the network. This memory allows them to exhibit dynamic temporal behavior and understand sequences, distinguishing them from other neural network architectures that treat inputs independently.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "RNNs are adept at handling:\n",
    "\n",
    "- Sequential data\n",
    "- Time-series data\n",
    "- Text and spoken language\n",
    "- Any form of data where the sequence order is significant\n",
    "\n",
    "Their design enables them to process inputs of varying lengths, from short sentences to lengthy documents or extensive time-series datasets.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "RNNs are particularly useful for:\n",
    "\n",
    "- Language modeling and text generation\n",
    "- Speech recognition\n",
    "- Time-series forecasting\n",
    "- Sentiment analysis and other forms of text classification\n",
    "\n",
    "The recurrent nature of RNNs allows them to capture temporal patterns and dependencies, making them a powerful tool for tasks that require an understanding of context within sequences.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "While RNNs theoretically can handle long sequences, they often face challenges with long-term dependencies due to issues like vanishing or exploding gradients. Techniques such as Long Short-Term Memory (LSTM) cells and Gated Recurrent Units (GRUs) have been developed to mitigate these issues and enhance RNNs' ability to scale to longer sequences. These two (LSTM & GRU) follow this section.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "RNNs can be sensitive to noise in the sequence data, especially if the noise affects the temporal dependencies that the network aims to learn. Regularization techniques and careful design of the network architecture are crucial to improve their robustness.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "- **Long Short-Term Memory (LSTM):** Designed to overcome the vanishing gradient problem, allowing RNNs to learn long-term dependencies.\n",
    "- **Gated Recurrent Units (GRUs):** A simpler alternative to LSTMs that achieves similar performance with fewer parameters.\n",
    "- **Bidirectional RNNs:** Process the data in both forward and backward directions, providing additional context and improving performance on tasks like speech recognition.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Recurrent Neural Networks:**\n",
    "\n",
    "- When dealing with sequential data where the order and context significantly impact the output.\n",
    "- For tasks requiring the model to remember and utilize past information over short or long sequences.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- Training RNNs can be computationally intensive and may require sophisticated techniques to deal with challenges like vanishing gradients.\n",
    "- The choice between plain RNNs, LSTMs, and GRUs depends on the specific requirements of the task, including the need for modeling long-term dependencies.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Recurrent Neural Networks represent a cornerstone in the field of sequential data analysis, offering the ability to process and generate predictions based on the context of input sequences.\n",
    "\n",
    "Their unique structure, capable of maintaining a form of internal memory, makes them indispensable for tasks that require understanding the temporal dynamics of data. Despite challenges in training and scalability, advancements like LSTMs and GRUs (seen following this) continue to push the boundaries of what RNNs can achieve, making them a valuable tool in the ever-evolving landscape of neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "354889a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNN, self).__init__(input_size, hidden_size, output_size=1)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.recurrent_layer = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        h_t, _ = self.recurrent_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[:, -1, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "88bc4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple RNN Model:\n",
      "SimpleRNN(\n",
      "  (input_layer): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (recurrent_layer): RNN(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Output: 0.5428599119186401\n"
     ]
    }
   ],
   "source": [
    "# Instantiate RNN Model\n",
    "simple_rnn_model = SimpleRNN(input_size,hidden_size)\n",
    "\n",
    "# Forward pass for the simple RNN model\n",
    "sample_input_rnn = torch.rand((1, 4, input_size))\n",
    "output_rnn = simple_rnn_model(sample_input_rnn)\n",
    "\n",
    "print(\"Simple RNN Model:\")\n",
    "print(simple_rnn_model)\n",
    "print(\"Output:\", output_rnn.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e99e4",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f536108",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Long Short Term Memory networks (LSTMs) are a specialized form of Recurrent Neural Networks (RNNs) designed to address the limitations of traditional RNNs, particularly in learning long-term dependencies. \n",
    "\n",
    "LSTMs are equipped with a unique architecture that includes memory cells and multiple gates (input, output, and forget gates), enabling them to maintain information over extended sequences and effectively manage the flow of information.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "LSTMs are versatile and can process a wide range of sequential data, including:\n",
    "\n",
    "- Textual data for natural language processing tasks.\n",
    "- Time-series data for forecasting in finance, weather, and more.\n",
    "- Sequential inputs from sensors for activity recognition or medical diagnosis.\n",
    "- Audio signals for speech recognition and music generation.\n",
    "\n",
    "Their ability to capture temporal dependencies makes them ideal for applications where the sequence and context of the data matter.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "LSTMs excel in tasks that require understanding and remembering context over long sequences, such as:\n",
    "\n",
    "- Language modeling and text generation.\n",
    "- Sequence to sequence translation, e.g., language translation.\n",
    "- Speech recognition and synthesis.\n",
    "- Complex time-series prediction and anomaly detection.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The sophisticated gating mechanisms of LSTMs allow them to scale well to longer sequences, addressing the vanishing gradient problem common in simpler RNNs. However, this complexity can lead to higher computational costs during training and inference, making efficiency and optimization key considerations for large-scale applications.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Thanks to their ability to selectively remember and forget information, LSTMs demonstrate robustness to noisy and irrelevant inputs, making them suitable for real-world applications where data quality can vary.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants and improvements on the original LSTM architecture have been proposed to enhance performance and efficiency, including:\n",
    "\n",
    "- **Bidirectional LSTMs (BiLSTMs):** Process data in both forward and backward directions, improving context understanding.\n",
    "- **Gated Recurrent Units (GRUs):** A simplified version of LSTMs that combines the input and forget gates into a single update gate.\n",
    "- **Peephole LSTMs:** Allow the gates to access the cell state directly, enhancing the control over the memory cell.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use LSTMs:**\n",
    "\n",
    "- For tasks involving sequences where the context and the temporal order of data points are crucial for making accurate predictions or decisions.\n",
    "- In scenarios where learning long-term dependencies is essential for performance.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- While powerful, LSTMs can be more challenging to train and fine-tune due to their complexity and the larger number of parameters compared to simpler models.\n",
    "- Careful design of the network architecture and selection of hyperparameters are essential to balance performance with computational efficiency.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Long Short Term Memory networks have revolutionized the handling of sequential data by enabling models to learn and remember over long sequences. Their design mitigates the challenges associated with traditional RNNs, making them a cornerstone for a wide array of applications in natural language processing, time-series analysis, and beyond. As research continues, LSTMs remain a critical tool in the deep learning toolkit, driving advancements in understanding sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "16386575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Neural Network\n",
    "class LSTMNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_t, c_t) = self.lstm_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[-1, :, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f0792094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNN(\n",
      "  (lstm_layer): LSTM(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "Input: tensor([[[0.9924, 0.6684, 0.1993],\n",
      "         [0.3595, 0.2677, 0.8802],\n",
      "         [0.4155, 0.1640, 0.4112],\n",
      "         [0.9775, 0.2113, 0.1807]]])\n",
      "Output: tensor([[0.4030, 0.3684, 0.4962, 0.6528]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LSTM neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 3  # Number of memory cells\n",
    "output_size = 4  # Number of output nodes\n",
    "lstm_model = LSTMNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, 4, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_lstm = lstm_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(lstm_model)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output:\", output_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0a067",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f61f92",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Gated Recurrent Units (GRUs) are a type of recurrent neural network (RNN) architecture, introduced as an alternative to the traditional Long Short-Term Memory (LSTM) units. GRUs aim to solve the vanishing gradient problem that plagues standard RNNs, allowing for more effective learning of dependencies across longer sequences. \n",
    "\n",
    "This is achieved through the use of gating mechanisms that regulate the flow of information inside the unit without relying on a memory cell, making them simpler and often faster to train compared to LSTMs, while achieving comparable performance.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "GRUs are especially well-suited for processing sequential data, including but not limited to:\n",
    "- Time-series data\n",
    "- Text data\n",
    "- Audio signals\n",
    "- Any form of data where the sequence and the temporal dynamics are important\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "GRUs excel in tasks requiring the modeling of sequence dependencies, such as:\n",
    "- Sequence prediction\n",
    "- Language modeling and text generation\n",
    "- Speech recognition\n",
    "- Time-series forecasting\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The simplified architecture of GRUs, relative to LSTMs, allows for more efficient training and inference, making them scalable to longer sequences and larger datasets. They are capable of capturing long-range dependencies across sequences, albeit with fewer parameters than LSTMs, which can lead to faster convergence in training.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Similar to LSTMs, GRUs are designed to be robust to noise in the sequence data. Their gating mechanisms effectively allow the model to focus on the most relevant parts of the input data, improving the model's ability to learn from noisy or incomplete sequences.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "While the standard GRU architecture is powerful, various modifications and extensions have been proposed to enhance its performance, including:\n",
    "- Bidirectional GRUs (BiGRUs), which process data in both forward and backward directions to capture context more effectively.\n",
    "- Deep GRUs, which stack multiple GRU layers to increase the model's ability to represent complex patterns.\n",
    "- Attention-augmented GRUs, which incorporate attention mechanisms to dynamically weigh the importance of different parts of the input sequence.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use GRUs:**\n",
    "- For applications involving sequential data where training efficiency and model simplicity are prioritized.\n",
    "- In cases where the sequence length is considerable, and the model needs to capture long-term dependencies without the computational overhead of LSTMs.\n",
    "\n",
    "**Considerations:**\n",
    "- While GRUs simplify the architecture and reduce the computational burden compared to LSTMs, the choice between GRUs and LSTMs should be based on the specific requirements of the task and dataset.\n",
    "- Experimentation with both GRUs and LSTMs is often necessary to determine the most effective architecture for a given application.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Gated Recurrent Units provide a powerful and efficient tool for modeling sequential data, balancing the complexity and computational requirements. Their ability to learn long-term dependencies with fewer parameters than LSTMs makes them an attractive option for many applications in sequence modeling, offering a blend of performance and efficiency that can be crucial for real-world tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7a021e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super(GRUNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.gru_layer = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_t, _ = self.gru_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[:, -1, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d9345c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNN(\n",
      "  (gru_layer): GRU(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "Input: tensor([[[1.8415e-01, 6.1823e-01, 4.4725e-01],\n",
      "         [7.0103e-02, 5.4926e-04, 4.1641e-01],\n",
      "         [3.0791e-02, 7.8418e-01, 5.0378e-01],\n",
      "         [3.2799e-01, 5.9636e-01, 2.6900e-01]]])\n",
      "Output: tensor([[0.4379, 0.3787, 0.4923, 0.4360]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the GRU neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 3  # Number of memory cells\n",
    "output_size = 4  # Number of output nodes\n",
    "gru_model = GRUNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, 4, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_gru = gru_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(gru_model)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output:\", output_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac764c25",
   "metadata": {},
   "source": [
    "### Neural Turing Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949e106c",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Neural Turing Machines (NTMs) introduced an innovative blend of neural networks with an external memory component, reminiscent of traditional Turing machines. This combination allows NTMs to perform data processing while also having the capability to store and retrieve information dynamically. Initially proposed to enhance problem-solving and data manipulation, NTMs marked a significant evolution in the quest to imbue neural networks with memory and learning flexibility similar to human cognition.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Neural Turing Machines are adept at handling diverse data types, including:\n",
    "- Sequential data\n",
    "- Numerical data\n",
    "- Symbolic data\n",
    "\n",
    "This versatility stems from their unique memory system, enabling them to manage tasks that necessitate a nuanced understanding of sequences and historical context.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "NTMs have shown promise in various applications, such as:\n",
    "- Sequence prediction and generation\n",
    "- Executing simple algorithms (e.g., sorting, copying)\n",
    "- Complex problem-solving requiring memory and deduction\n",
    "\n",
    "Despite their potential, the practical adoption of NTMs in solving broad AI challenges remains exploratory, with newer models and architectures often preferred for specific applications.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of Neural Turing Machines is influenced by the neural network's architecture and the external memory's design. While theoretically capable of handling tasks with extensive memory requirements, practical implementations face challenges related to training complexity and computational demands.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "NTMs offer a degree of robustness to noise, leveraging the neural network's capacity for pattern recognition alongside structured memory access to mitigate the effects of noisy inputs. However, the intricacies of memory management can introduce complexities in achieving consistent performance across varied tasks.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Advancements in the concept of NTMs have led to the development of variants such as:\n",
    "- **Differentiable Neural Computers (DNCs):** DNCs build on NTMs by refining memory access mechanisms, aiming for greater efficiency and applicability.\n",
    "- **Memory Networks:** While not direct descendants, Memory Networks share the ethos of integrating memory with neural processing, tailored for tasks requiring reasoning and long-term context.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Explore Neural Turing Machines:**\n",
    "- For academic and research endeavors focused on enhancing neural networks with memory capabilities.\n",
    "- In experimental projects aiming to understand and innovate on the integration of external memory systems with machine learning models.\n",
    "\n",
    "**Considerations:**\n",
    "- The complexity of NTMs and their variants necessitates a deep understanding of both neural networks and memory systems, making them more suited for research than immediate practical application.\n",
    "- Given the rapid advancement in AI, practitioners often turn to more recent developments tailored to specific use cases, such as Transformer models for sequence understanding and GANs for generative tasks.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "While Neural Turing Machines represent a pivotal step towards more intelligent and flexible AI systems, their role in current neural network research has evolved. Today, NTMs are appreciated more for their conceptual contributions to integrating memory with neural processing rather than as a go-to solution for practical applications. As AI continues to advance, the principles underlying NTMs inspire ongoing innovations in creating models that more closely mimic human cognitive abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c22bd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, controller_size, memory_units, memory_unit_size):\n",
    "        super(NTM, self).__init__()\n",
    "        self.controller = nn.LSTM(input_size, controller_size, batch_first=True)  # Note: batch_first=True\n",
    "        self.memory = torch.zeros(memory_units, memory_unit_size)  # Initialize memory\n",
    "        \n",
    "        # Parameters for read and write heads (simplified)\n",
    "        self.read_weights = nn.Parameter(torch.randn(memory_units))\n",
    "        self.write_weights = nn.Parameter(torch.randn(memory_units))\n",
    "        self.read_vector = nn.Parameter(torch.randn(memory_unit_size))\n",
    "        self.output_layer = nn.Linear(controller_size + memory_unit_size, output_size)\n",
    "        \n",
    "    def read_from_memory(self):\n",
    "        # Simplified read mechanism: weighted sum of memory content\n",
    "        return torch.matmul(self.read_weights, self.memory)\n",
    "    \n",
    "    def write_to_memory(self, write_vector):\n",
    "        # Simplified write mechanism: update memory with new content\n",
    "        self.memory += torch.outer(self.write_weights, write_vector)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Controller processing. Note: Assuming x is of shape [batch, seq_len, input_size]\n",
    "        controller_output, _ = self.controller(x)\n",
    "        \n",
    "        # For simplicity, assuming we're dealing with single time-step sequences or interested in the last time step\n",
    "        # Adjusting controller_output to ensure it's [batch, controller_size]\n",
    "        controller_output = controller_output[:, -1, :]\n",
    "        \n",
    "        # Read from memory\n",
    "        read_vector = self.read_from_memory()  # This will be [memory_units, memory_unit_size]\n",
    "        # Ensuring read_vector is [batch, memory_unit_size] for batch processing\n",
    "        read_vector = read_vector.unsqueeze(0).expand(controller_output.size(0), -1)  # Expand to match batch size\n",
    "        \n",
    "        # Prepare controller output and read vector for final output\n",
    "        combined_output = torch.cat([controller_output, read_vector], dim=1)\n",
    "        final_output = self.output_layer(combined_output)\n",
    "        \n",
    "        # Example write operation (could be based on controller output or other logic)\n",
    "        write_vector = torch.randn_like(read_vector[0])  # Assuming write_vector needs to match a single read_vector\n",
    "        self.write_to_memory(write_vector)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f5b7e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NTM parameters\n",
    "input_dim = 4\n",
    "output_dim = 2\n",
    "controller_size = 10\n",
    "memory_units = 20\n",
    "memory_unit_size = 5\n",
    "\n",
    "# Instantiate the NTM model\n",
    "ntm_model = NTM(input_dim, output_dim, controller_size, memory_units, memory_unit_size)\n",
    "\n",
    "# Generate a sample input sequence\n",
    "sample_input = torch.randn(10, 1, input_dim)  # Example: 10 sequences, each with 1 timestep and input_dim features\n",
    "\n",
    "# Forward pass through the NTM\n",
    "output_sequences = [ntm_model(seq.unsqueeze(0)) for seq in sample_input]  # Note the addition of unsqueeze(0) for batch dimension\n",
    "\n",
    "# Convert output sequences to a tensor\n",
    "output_tensor = torch.cat(output_sequences, dim=0)  # Concatenating along the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7a8f456d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor Shape: torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Tensor Shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ec8bb97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0928,  0.8264, -1.0069, -0.2640]],\n",
      "\n",
      "        [[-1.4829, -0.0949, -0.4584, -2.0857]],\n",
      "\n",
      "        [[ 0.6140,  0.3885,  1.4811, -0.6414]],\n",
      "\n",
      "        [[-0.0198,  1.6060,  0.0311,  0.7117]],\n",
      "\n",
      "        [[ 1.6702,  0.0438, -1.5361, -0.3919]],\n",
      "\n",
      "        [[-0.2231,  1.0052,  0.4035, -2.5949]],\n",
      "\n",
      "        [[-0.0191,  0.8821,  1.2349,  0.7389]],\n",
      "\n",
      "        [[ 0.6528, -0.3223, -2.4276,  1.0514]],\n",
      "\n",
      "        [[ 0.6787, -0.8386, -0.2332, -0.0820]],\n",
      "\n",
      "        [[ 1.0996, -0.7407,  0.0811, -1.2383]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "02cdd394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2207, -0.2602],\n",
      "        [-3.2119, -1.4007],\n",
      "        [-3.3713, -2.9428],\n",
      "        [-2.3315, -2.3056],\n",
      "        [-1.7444, -2.7277],\n",
      "        [-4.4917,  0.1537],\n",
      "        [-3.8595,  0.1150],\n",
      "        [-2.0371,  0.3657],\n",
      "        [-3.1308, -1.3382],\n",
      "        [-6.6954, -1.4974]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a339a745",
   "metadata": {},
   "source": [
    "# Advanced Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d79e1c",
   "metadata": {},
   "source": [
    "## Probabilistic and Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b869e1",
   "metadata": {},
   "source": [
    "### Markov Chain (MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf55135",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Markov Chains represent a stochastic model describing a sequence of possible events where the probability of each event depends only on the state attained in the previous event. This mathematical framework is fundamental in the study of random processes and is widely applicable across various domains, including statistical mechanics, economics, and predictive modeling. Markov Chains are particularly valued for their simplicity and power in modeling the randomness of systems evolving over time.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Markov Chains are applicable to:\n",
    "- Discrete events or states\n",
    "- Temporal or spatial sequences\n",
    "\n",
    "Their adaptability allows them to model a wide array of processes, from simple random walks to complex decision-making scenarios.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Markov Chains excel in:\n",
    "- Predicting state transitions\n",
    "- Modeling random processes\n",
    "- Decision making under uncertainty\n",
    "\n",
    "Their predictive capabilities make them an essential tool for scenarios where future states depend on the current state, without the need for historical data.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Markov Chains scale well with the complexity of the model, primarily influenced by the number of states. While larger state spaces increase computational demands, advancements in algorithms and computing power have made it feasible to tackle complex chains efficiently.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Given their probabilistic nature, Markov Chains naturally incorporate and manage uncertainty and noise within their models. This robustness makes them suitable for applications where data may be incomplete or inherently random.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Markov Chains come in various forms, including:\n",
    "- **Discrete-Time Markov Chains:** Model transitions in discrete time steps.\n",
    "- **Continuous-Time Markov Chains:** Allow for transitions at any point in time.\n",
    "- **Hidden Markov Models (HMMs):** Extend Markov Chains by allowing observations to be a probabilistic function of the state, useful in scenarios where states are not directly observable.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Markov Chains:**\n",
    "- For modeling sequential or temporal data where future states depend on the current state.\n",
    "- In decision-making processes to evaluate different strategies under uncertainty.\n",
    "- When analyzing systems or processes that evolve over time in predictable patterns.\n",
    "\n",
    "**Considerations:**\n",
    "- Markov Chains assume the future is independent of the past given the present state, which may not hold in systems with memory or where historical context is crucial.\n",
    "- They are best applied to processes where this assumption of memorylessness (the Markov property) is reasonable or where state transitions are primarily influenced by the current state.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Markov Chains offer a powerful and flexible framework for modeling random processes and making predictions based on state transitions. By understanding their structure, capabilities, and the variety of their applications, one can effectively leverage Markov Chains to gain insights into complex systems, predict future events, and make informed decisions under uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "89af0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChainNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MarkovChainNN, self).__init__(input_size, hidden_size, output_size=input_size)\n",
    "        self.transition_matrix = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply a simple linear transformation based on the transition matrix\n",
    "        x = torch.matmul(x, self.transition_matrix)\n",
    "        # Apply a linear layer to get the final output\n",
    "        output = self.output_layer(x)\n",
    "        # You might want to apply some non-linearity here based on your specific needs\n",
    "        # For example, you can use torch.relu(output) or torch.sigmoid(output) depending on the task\n",
    "        return output\n",
    "\n",
    "# Instantiate the Markov Chain neural network\n",
    "input_size = 4  # Number of input features \n",
    "hidden_size = 8  # Number of hidden states\n",
    "markov_chain_model = MarkovChainNN(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_markov_chain = markov_chain_model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dbcf5cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarkovChainNN(\n",
      "  (output_layer): Linear(in_features=8, out_features=4, bias=True)\n",
      ")\n",
      "Input: tensor([[0.6269, 0.7124, 0.9113, 0.9550]])\n",
      "\n",
      "Output: tensor([[-1.3894, -1.1051,  2.1734,  1.1821]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the model architecture and output\n",
    "print(markov_chain_model)\n",
    "print(\"Input:\",sample_input)\n",
    "print(\"\\nOutput:\", output_markov_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed412a",
   "metadata": {},
   "source": [
    "### Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ee1d6",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Hopfield Networks are a form of recurrent neural network with a unique structure that allows them to serve as associative memory systems. These networks are characterized by fully connected neurons with symmetric weight matrices, enabling them to converge to stable states or \"memories\". This architecture makes Hopfield Networks particularly adept at solving optimization and memory recall tasks, leveraging their ability to find energy minima to recall stored patterns.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Hopfield Networks primarily deal with:\n",
    "- Binary data\n",
    "- Bipolar data\n",
    "\n",
    "Their structure is optimized for patterns represented in these formats, making them suitable for tasks that can be encoded as binary or bipolar vectors.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Hopfield Networks are well-suited for:\n",
    "- Pattern recognition\n",
    "- Associative memory recall\n",
    "- Optimization problems\n",
    "\n",
    "Their ability to serve as content-addressable (\"associative\") memory systems allows them to recall entire patterns based on partial or noisy inputs, showcasing their strength in tasks requiring robust pattern completion and error correction.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "While Hopfield Networks provide powerful capabilities for pattern recognition and memory recall, their scalability is limited by the network size due to the fully connected nature of the architecture. The capacity of a Hopfield Network to store memories without error is approximately 15% of the number of neurons, limiting the size of problems they can effectively solve without modifications or extensions.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "A key feature of Hopfield Networks is their robustness to noise in input patterns. They can recover original stored patterns from inputs that are partially incorrect or incomplete, making them highly effective for tasks requiring error tolerance and noise reduction in pattern recall.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "To address scalability and efficiency, several variants of Hopfield Networks have been developed, including:\n",
    "- **Continuous Hopfield Networks:** Extend the binary model to continuous values, allowing for application to a wider range of problems.\n",
    "- **Stochastic Hopfield Networks:** Introduce randomness in the update rules, enhancing the network's ability to escape local minima and find better solutions for optimization problems.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Hopfield Networks:**\n",
    "- When the task involves recovering or completing partial patterns.\n",
    "- For optimization problems where potential solutions can be encoded as binary or bipolar vectors.\n",
    "- In applications where associative memory models offer a natural solution.\n",
    "\n",
    "**Considerations:**\n",
    "- Hopfield Networks are not well-suited for large-scale problems due to their limited storage capacity and the computational cost of fully connected networks.\n",
    "- They may not be the best choice for new tasks with high-dimensional data or where deep learning approaches have demonstrated superior performance.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Hopfield Networks offer a approach to associative memory and optimization problems, with their unique ability to recall stored patterns from noisy or incomplete inputs. Understanding their structure, capabilities, and limitations is crucial for leveraging their strengths in relevant applications, while recognizing when alternative neural network models might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b7c8d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HopfieldNetwork(BaseNN):\n",
    "    def __init__(self, input_size):\n",
    "        super(HopfieldNetwork, self).__init__(input_size, hidden_size=None, output_size=input_size)\n",
    "        \n",
    "        # Weight matrix for the Hopfield Network\n",
    "        self.weights = nn.Parameter(torch.zeros((input_size, input_size), dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the Hopfield Network dynamics\n",
    "        y = torch.sign(x @ self.weights).long()  # Convert to torch.long after applying sign\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "221dcea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HopfieldNetwork()\n",
      "Input tensor([[ 1., -1.,  1., -1.,  1.]])\n",
      "Output: [[0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 5  # Change this based on your needs\n",
    "hopfield_model = HopfieldNetwork(input_size)\n",
    "\n",
    "# Define a sample input pattern (1 or -1)\n",
    "sample_input = torch.tensor([[1, -1, 1, -1, 1]], dtype=torch.float)  # Change the data type to torch.float\n",
    "\n",
    "# Forward pass to retrieve the output\n",
    "output_hopfield = hopfield_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(hopfield_model)\n",
    "print(\"Input\", sample_input)\n",
    "print(\"Output:\", output_hopfield.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f9cec",
   "metadata": {},
   "source": [
    "### Boltzmann Machine (BM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d81af",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "A Boltzmann Machine (BM) is a type of stochastic Recurrent Neural Network (RNN) that's fundamental in the field of deep learning for unsupervised learning tasks. \n",
    "\n",
    "Characterized by its symmetrical weight structure between units and operates on binary states. Its core objective is to learn the joint probability distribution of its input data, making it powerful for understanding complex data structures, feature detection, and generative models.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Boltzmann Machines can handle a variety of data types, notably:\n",
    "- Binary data\n",
    "- Categorical data (via binary encoding)\n",
    "- Any data type that can be effectively represented in a binary format\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "The main objectives of Boltzmann Machines include:\n",
    "- Learning the underlying probability distribution of the data\n",
    "- Feature learning and representation\n",
    "- Dimensionality reduction\n",
    "- Density estimation\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of Boltzmann Machines is challenged by the computation required for their training process, which involves sampling from a complex energy-based model. Techniques like Restricted Boltzmann Machines (RBMs) and training enhancements have been developed to address these scalability issues.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Boltzmann Machines exhibit a degree of robustness to noise due to their probabilistic nature, allowing them to model complex, noisy data distributions effectively. Their stochastic processing can infer the underlying structure from noisy inputs, making them resilient in various applications.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of the original Boltzmann Machine exist, each designed to optimize certain aspects of its architecture and training:\n",
    "- **Restricted Boltzmann Machine (RBM):** Simplifies the architecture by removing connections between nodes of the same layer, enhancing training efficiency.\n",
    "- **Deep Boltzmann Machine (DBM):** Extends RBMs by adding multiple hidden layers, increasing the model's capacity to represent more complex data.\n",
    "- **Continuous Boltzmann Machine:** Adapts the binary state model to handle continuous data, broadening its applicability.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Boltzmann Machines:**\n",
    "- For unsupervised learning tasks where understanding the data's probability distribution is crucial.\n",
    "- In complex systems modeling, where the interactions between components can be represented probabilistically.\n",
    "- For pre-training deep neural networks as a way to initialize weights that can capture useful features without labeled data.\n",
    "\n",
    "**Considerations:**\n",
    "- Training Boltzmann Machines, especially in their unrestricted form, can be computationally intensive due to the need for Monte Carlo simulations to approximate the likelihood gradient.\n",
    "- The choice between Boltzmann Machine variants largely depends on the specific requirements of the task, such as the data type and the desired balance between model complexity and computational feasibility.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Boltzmann Machines excel in modeling complex data distributions and feature extraction without supervision. They are a key asset in deep learning for tasks requiring insight into data structures without labeled data. They continue to be pivotal in generative and unsupervised learning research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7668407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzmannMachine(nn.Module):\n",
    "    def __init__(self, num_visible, num_hidden):\n",
    "        super(BoltzmannMachine, self).__init__()\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        # Define the parameters (weights and biases)\n",
    "        self.weights = nn.Parameter(torch.randn(num_visible, num_hidden))\n",
    "        self.visible_bias = nn.Parameter(torch.randn(num_visible))\n",
    "        self.hidden_bias = nn.Parameter(torch.randn(num_hidden))\n",
    "\n",
    "    def forward(self, visible_states):\n",
    "        # Ensure visible_states has the correct dimensions (batch_size x num_visible)\n",
    "        if visible_states.dim() == 1:\n",
    "            visible_states = visible_states.view(1, -1)\n",
    "\n",
    "        # Compute the hidden probabilities given visible states\n",
    "        hidden_probabilities = F.sigmoid(F.linear(visible_states, self.weights.t(), self.hidden_bias))\n",
    "\n",
    "        # Sample hidden states from the computed probabilities\n",
    "        hidden_states = torch.bernoulli(hidden_probabilities)\n",
    "\n",
    "        # Compute the visible probabilities given the sampled hidden states\n",
    "        visible_probabilities = F.sigmoid(F.linear(hidden_states, self.weights, self.visible_bias))\n",
    "\n",
    "        # Sample visible states from the computed probabilities\n",
    "        visible_states = torch.bernoulli(visible_probabilities)\n",
    "\n",
    "        return visible_states, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9ebd1d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoltzmannMachine()\n",
      "Sampled Visible State: [[1. 0. 1. 1. 0.]]\n",
      "Sampled Hidden State: [[0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_visible = 5\n",
    "num_hidden = 3\n",
    "\n",
    "boltzmann_machine = BoltzmannMachine(num_visible, num_hidden)\n",
    "\n",
    "# Define a sample visible state (binary values)\n",
    "sample_visible_state = torch.tensor([1, 0, 1, 0, 1.], dtype=torch.float)\n",
    "\n",
    "# Perform a Gibbs sampling step\n",
    "sampled_visible, sampled_hidden = boltzmann_machine(sample_visible_state)\n",
    "\n",
    "# Print the model architecture and sampled states\n",
    "print(boltzmann_machine)\n",
    "print(\"Sampled Visible State:\", sampled_visible.detach().numpy())\n",
    "print(\"Sampled Hidden State:\", sampled_hidden.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba54b75",
   "metadata": {},
   "source": [
    "### Restricted Boltzmann Machine (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9d8db1",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) refine the concept of Boltzmann Machines by imposing a specific structure: they restrict connections between nodes, allowing only connections between visible and hidden layers without interconnections within a layer. This simplification leads to more efficient training algorithms and practical applicability in deep learning tasks such as dimensionality reduction, feature learning, and collaborative filtering.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "RBMs efficiently process a variety of data types, including:\n",
    "- Binary\n",
    "- Continuous (using specific variants like Gaussian RBMs)\n",
    "- Categorical data (through adaptations)\n",
    "\n",
    "Their flexible structure makes them adept at handling complex data representations.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "RBMs excel at:\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Pretraining for deep neural networks\n",
    "- Collaborative filtering and recommendation systems\n",
    "\n",
    "The bipartite architecture enables RBMs to uncover hidden patterns within data, making them powerful tools for unsupervised learning tasks.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The restricted architecture of RBMs simplifies learning, allowing them to scale to large datasets more efficiently than traditional Boltzmann Machines. Their training is facilitated by algorithms like Contrastive Divergence, enhancing scalability and practicality.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "RBMs demonstrate considerable robustness to noise, attributed to their ability to learn probabilistic representations of data. This makes them suitable for applications where data quality is varied or uncertain.\n",
    "\n",
    "**Differences from Boltzmann Machines**\n",
    "\n",
    "Unlike general Boltzmann Machines, RBMs have a restricted architecture that eliminates intra-layer connections, significantly improving training efficiency. This restriction not only simplifies learning but also enhances their applicability in real-world deep learning tasks.\n",
    "\n",
    "**Preferred Usage over Boltzmann Machines**\n",
    "\n",
    "RBMs are preferred in scenarios requiring efficient feature learning, dimensionality reduction, or as a pretraining step for deeper neural networks due to their structured architecture and effective learning algorithms. They are particularly favored in collaborative filtering and recommendation systems, where their probabilistic nature allows for nuanced preference modeling.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Restricted Boltzmann Machines offer a targeted and efficient approach to learning data representations, standing out for their structure that enables practical and scalable unsupervised learning. By focusing on the relationship between visible and hidden layers without internal connections, RBMs provide a powerful mechanism for feature extraction and serve as a foundational building block in the architecture of deep belief networks and other deep learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "27c65354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(BaseNN):\n",
    "    def __init__(self, visible_size, hidden_size):\n",
    "        super(RBM, self).__init__(visible_size, hidden_size, None)\n",
    "        self.weights = nn.Parameter(torch.randn(visible_size, hidden_size))\n",
    "        self.visible_bias = nn.Parameter(torch.zeros(visible_size))\n",
    "        self.hidden_bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_prob = F.sigmoid(F.linear(x, self.weights.t(), self.hidden_bias))\n",
    "        hidden_state = torch.bernoulli(hidden_prob)\n",
    "        reconstructed_prob = F.sigmoid(F.linear(hidden_state, self.weights, self.visible_bias))\n",
    "        return hidden_state, reconstructed_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0741e106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM()\n",
      "Sampled Visible State: [[1. 0. 1. 0. 1.]]\n",
      "Hidden States: [[1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "visible_size = 5\n",
    "hidden_size = 3\n",
    "\n",
    "# Create an RBM model\n",
    "rbm_model = RBM(visible_size, hidden_size)\n",
    "\n",
    "# Define a sample visible state (binary values)\n",
    "sample_visible_state = torch.tensor([[1, 0, 1, 0, 1.]], dtype=torch.float)\n",
    "\n",
    "# Forward pass to get the hidden states\n",
    "hidden_states, _ = rbm_model(sample_visible_state)\n",
    "\n",
    "# Print the model architecture and hidden states\n",
    "print(rbm_model)\n",
    "print(\"Sampled Visible State:\", sample_visible_state.detach().numpy())\n",
    "print(\"Hidden States:\", hidden_states.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee6f13",
   "metadata": {},
   "source": [
    "### Deep Belief Network (DBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930ddac",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Deep Belief Networks (DBNs) are advanced generative models that stack multiple layers of Restricted Boltzmann Machines (RBMs) or similar unsupervised networks. By layering RBMs, DBNs can learn a hierarchical representation of data, capturing complex, high-level features in data. \n",
    "\n",
    "Initially introduced to improve training efficiency and model depth, DBNs have been pivotal in the development of deep learning, particularly in unsupervised and semi-supervised learning tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "DBNs are versatile and can process various data types, including:\n",
    "- Binary and continuous data\n",
    "- Images and text\n",
    "- Complex, high-dimensional datasets\n",
    "\n",
    "This flexibility makes them suitable for a wide range of applications in machine learning and artificial intelligence.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "DBNs are particularly effective for:\n",
    "- Feature learning and extraction\n",
    "- Classification with minimal labeled data\n",
    "- Generative tasks, including data generation and reconstruction\n",
    "- Dimensionality reduction\n",
    "\n",
    "Their hierarchical structure allows for learning abstract representations of data at higher layers, making them valuable for tasks requiring deep feature extraction.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "One of DBNs' key advantages is their scalability. Through pretraining each layer as an RBM before fine-tuning the entire network, DBNs efficiently handle large, complex datasets. This staged training approach significantly reduces the training difficulties associated with deep architectures.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Due to their generative nature and the probabilistic learning of RBMs, DBNs exhibit robustness against noisy and incomplete data. They can infer missing information and maintain performance even in less-than-ideal data conditions.\n",
    "\n",
    "**Evolution from Boltzmann Machines and RBMs**\n",
    "\n",
    "DBNs build on the foundations of Boltzmann Machines and RBMs by structuring multiple RBMs in a deep architecture. This evolution addresses the limitations of single-layer models, offering a more potent and efficient way to capture complex data structures through deep, hierarchical learning.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Deep Belief Networks:**\n",
    "- In unsupervised learning scenarios to discover intricate structures in unlabeled data.\n",
    "- For semi-supervised learning tasks where labeled data is scarce but unlabeled data is abundant.\n",
    "- As a pretraining step for deep neural networks, to initialize weights that capture meaningful features without requiring labeled data.\n",
    "\n",
    "**Considerations:**\n",
    "- While DBNs offer powerful modeling capabilities, their training process can be complex and computationally intensive.\n",
    "- The choice of hyperparameters and the structure of the network can significantly impact performance, requiring careful tuning and experimentation.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Deep Belief Networks represent a significant milestone in the evolution of neural networks, bridging the gap between traditional models and the deep learning architectures that dominate the field today. By leveraging the strengths of RBMs in a multi-layered setup, DBNs provide a robust framework for extracting deep, hierarchical features from data. Their development has not only advanced the capabilities of generative models but also set the stage for the widespread adoption of deep learning in solving complex, real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7ff63408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN(nn.Module):\n",
    "    def __init__(self, visible_size, hidden_sizes):\n",
    "        super(DBN, self).__init__()\n",
    "        self.rbm_layers = nn.ModuleList()\n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            # The visible size for the first RBM is the input size,\n",
    "            # and for subsequent RBMs, it's the size of the previous hidden layer.\n",
    "            rbm_visible_size = visible_size if i == 0 else hidden_sizes[i-1]\n",
    "            self.rbm_layers.append(RBM(rbm_visible_size, hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through each RBM layer\n",
    "        # Note: This simply propagates the input through each RBM to transform it.\n",
    "        # Actual training of RBMs would typically occur in a layer-wise manner before this step.\n",
    "        for rbm_layer in self.rbm_layers:\n",
    "            x, _ = rbm_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "616caf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN(\n",
      "  (rbm_layers): ModuleList(\n",
      "    (0-1): 2 x RBM()\n",
      "  )\n",
      ")\n",
      "Sample Input: tensor([[0.6356, 0.3107, 0.0539, 0.6232, 0.9010]])\n",
      "Output: [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "visible_size = 5\n",
    "hidden_sizes = [5, 1]\n",
    "\n",
    "# Create a Deep Belief Network\n",
    "dbn_model = DBN(visible_size, hidden_sizes)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, visible_size))\n",
    "\n",
    "# Forward pass through the DBN\n",
    "output_dbn = dbn_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(dbn_model)\n",
    "print(\"Sample Input:\", sample_input)\n",
    "\n",
    "print(\"Output:\", output_dbn.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd278c",
   "metadata": {},
   "source": [
    "### General Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbe30c",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Generative Adversarial Networks (GANs) represent a revolutionary approach in the field of artificial intelligence, particularly in generating synthetic data that closely mimics real data. Comprising two key components, a generator and a discriminator, GANs engage in a dynamic competition. The generator creates data aiming to pass as real, while the discriminator evaluates it against actual data, trying to differentiate the fake from the real. This iterative adversarial process enhances the generator's output over time, leading to highly realistic synthetic data.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "GANs are remarkably flexible, capable of generating various data types, such as:\n",
    "- Images\n",
    "- Videos\n",
    "- Text\n",
    "- Audio\n",
    "- Complex data structures\n",
    "\n",
    "Their adaptability has fueled advancements in numerous domains, including digital art, content creation, and even scientific research.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Key applications of GANs include:\n",
    "- Photorealistic image and video generation\n",
    "- Image super-resolution and enhancement\n",
    "- Realistic scenario generation for simulation and training\n",
    "- Style transfer across images or texts\n",
    "- Data augmentation to improve machine learning models\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of GANs varies with the complexity of the data and the design of the generator and discriminator networks. High-resolution and intricate data generation demand significant computational resources and sophisticated neural network architectures, yet ongoing research is progressively overcoming these challenges.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "GANs inherently learn to filter out noise and generate clean data representations by mimicking the true data distribution. This makes them inherently robust to input noise, although the quality of the generated output heavily relies on the diversity and cleanliness of the training data.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Numerous GAN variants have been developed to address specific challenges and enhance functionality:\n",
    "- **Conditional GANs (cGANs):** Enable controlled data generation based on conditional inputs.\n",
    "- **CycleGANs:** Facilitate unpaired image-to-image translation tasks.\n",
    "- **StyleGANs:** Produce highly realistic and customizable imagery, notably in facial generation.\n",
    "- **Wasserstein GANs (WGANs):** Improve training stability and convergence through an alternative loss function that measures the distance between generated and real data distributions more effectively.\n",
    "\n",
    "**Unique Features**\n",
    "\n",
    "- **Adversarial Training:** The core mechanism of GANs, where two networks are trained in opposition to each other, is unique to this architecture. This approach simulates a competitive game that continuously improves the quality of generated data.\n",
    "- **Implicit Density Estimation:** Unlike models that explicitly model the probability distribution of data, GANs learn to generate data by implicitly estimating the data distribution, allowing for the generation of highly realistic samples.\n",
    "- **Creative and Generative Capability:** GANs have the unique ability to create new, unseen data instances that can be indistinguishable from real instances, pushing the boundaries of AI's creative capabilities.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use GANs:**\n",
    "- For generating new, realistic samples from learned data distributions in various applications, from artistic creation to data augmentation.\n",
    "- In scenarios where high-quality, novel content creation is essential, leveraging the unique capabilities of GANs can provide significant benefits.\n",
    "\n",
    "**Considerations:**\n",
    "- The complexity of training GANs, requiring careful balancing between generator and discriminator, presents a notable challenge.\n",
    "- Ethical considerations must be taken into account, particularly with the potential for creating misleading or harmful synthetic content.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Generative Adversarial Networks have significantly impacted the generation of synthetic data, offering unparalleled realism across various applications. Despite the challenges in training and ethical concerns, GANs continue to be a focal point of AI research, driving innovation and expanding the possibilities of creative and scientific exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa33e6",
   "metadata": {},
   "source": [
    "**Generator Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d4611dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c8e3c",
   "metadata": {},
   "source": [
    "**Discriminator Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a1f612a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e8216",
   "metadata": {},
   "source": [
    "**GAN class**\n",
    "Utilizes the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "54a18ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(BaseNN):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__(input_size=None, hidden_size=None, output_size=None)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, x):\n",
    "        generated_data = self.generator(x)\n",
    "        discriminator_output = self.discriminator(generated_data)\n",
    "        return generated_data, discriminator_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "15f75f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN(\n",
      "  (generator): Generator(\n",
      "    (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      "  (discriminator): Discriminator(\n",
      "    (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Generated Data: tensor([[0.5443, 0.4855, 0.4727, 0.4808, 0.4928, 0.4908, 0.3693, 0.4935, 0.5071,\n",
      "         0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator Output: tensor([[0.5372]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "\n",
    "generator = Generator(input_size, hidden_size, output_size)\n",
    "discriminator = Discriminator(output_size, hidden_size, 1)\n",
    "\n",
    "gan_model = GAN(generator, discriminator)\n",
    "\n",
    "sample_noise = torch.randn((1, input_size))\n",
    "\n",
    "generated_data, discriminator_output = gan_model(sample_noise)\n",
    "\n",
    "print(gan_model)\n",
    "print(\"Generated Data:\", generated_data)\n",
    "print(\"Discriminator Output:\", discriminator_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7945808",
   "metadata": {},
   "source": [
    "## Hybrid and Specialized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2bc21a",
   "metadata": {},
   "source": [
    "### Spiking Neural Network (SNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea3562",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Spiking Neural Networks (SNNs) stand at the forefront of simulating the way biological brains operate, marking a significant leap in the field of neuromorphic computing. Unlike traditional artificial neural networks that process information in a continuous manner, SNNs incorporate the concept of time into their operational framework. They communicate through discrete events or \"spikes\", mimicking the neural activity observed in biological neurons. This approach allows SNNs to process information more efficiently and with a higher degree of biological realism, potentially leading to more power-efficient AI systems that can operate closer to the way human cognition works.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "SNNs are versatile in processing various types of data, especially those that benefit from temporal or spatiotemporal dynamics, including:\n",
    "- Time-series data\n",
    "- Audio signals\n",
    "- Visual sequences\n",
    "- Spatiotemporal patterns\n",
    "\n",
    "Their unique ability to handle time-dependent information makes them particularly suitable for applications in dynamic environments and real-time processing.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "SNNs excel in tasks that involve complex temporal dynamics and require high efficiency, such as:\n",
    "- Real-time signal processing\n",
    "- Neuromorphic computing applications\n",
    "- Brain-computer interfaces\n",
    "- Robotics and control systems\n",
    "- Event-based vision processing\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of SNNs is influenced by their ability to efficiently process information using fewer computational resources compared to traditional neural networks. This efficiency, however, comes with the challenge of developing suitable hardware and algorithms that can fully exploit the temporal dynamics of spiking neurons.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "SNNs demonstrate inherent robustness to noise due to their event-driven nature, which allows them to focus on significant temporal changes in the input data, ignoring irrelevant fluctuations. This characteristic makes them particularly adept at working in noisy or chaotic environments.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "There are several key variants and developments in the field of SNNs, aimed at enhancing their performance and applicability:\n",
    "- **Leaky Integrate-and-Fire (LIF) Models:** Simulate the real behavior of neurons with a simple model that integrates incoming signals until a threshold is reached, and then fires.\n",
    "- **Spike Response Model (SRM):** Includes the dynamics of after-potentials, offering a more detailed emulation of biological neuron behavior.\n",
    "- **Spiking Deep Neural Networks:** Integrate deep learning principles with SNNs, enabling complex hierarchical representations and learning capabilities.\n",
    "\n",
    "**Unique Features**\n",
    "\n",
    "- **Temporal Dynamics:** The intrinsic use of time and event-driven processing allows SNNs to efficiently handle tasks with a temporal component, offering a unique advantage in processing speed and computational efficiency.\n",
    "- **Biological Plausibility:** SNNs closely resemble the functioning of biological neural networks, providing insights into brain-like computing mechanisms and facilitating advancements in brain-computer interfaces.\n",
    "- **Energy Efficiency:** Due to their event-driven nature, SNNs have the potential to significantly reduce power consumption, making them ideal for deployment in low-power devices and edge computing scenarios.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use SNNs:**\n",
    "- In applications where temporal dynamics are crucial, and efficiency is of paramount importance.\n",
    "- For developing systems that require low power consumption and are inspired by biological processing mechanisms.\n",
    "\n",
    "**Common Uses:**\n",
    "\n",
    "- **Neurobiological Research:** Offers a platform for exploring theories of brain function and the principles underlying neural computation.\n",
    "- **Sensory Processing:** Applied in processing and interpreting data from sensory inputs, such as visual and auditory systems, in a manner similar to biological systems.\n",
    "- **Edge Computing:** Ideal for deployment in edge devices due to their low power consumption, where they can perform real-time data analysis.\n",
    "- **Pattern Recognition:** Utilized in tasks requiring the detection of patterns over time, such as speech recognition or gesture analysis.\n",
    "- **Robotic Control:** Empowers robots with the ability to process sensory inputs in real-time, leading to more adaptive and responsive behaviors.\n",
    "\n",
    "**Considerations:**\n",
    "- The complexity of accurately modeling and simulating SNNs poses challenges, particularly in terms of hardware and algorithmic development.\n",
    "- The field is still evolving, with ongoing research needed to fully harness the potential of SNNs in practical applications.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Spiking Neural Networks represent a paradigm shift towards more biologically realistic and efficient forms of computing. By leveraging the temporal dynamics of information processing, SNNs open new avenues in the development of AI systems that are both power-efficient and capable of complex temporal pattern recognition.\n",
    "\n",
    "Despite the challenges in their implementation and the need for specialized hardware, the potential of SNNs to revolutionize various fields of technology and neuroscience is immense, marking them as a critical area of research in the pursuit of brain-inspired computing solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c6d2802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        # Define a simple linear layer to simulate neuron connections\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        # Spike function could be a Heaviside step function or similar\n",
    "        self.spike_fn = lambda x: torch.heaviside(x - 0.5, torch.tensor([0.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.spike_fn(x)  # Simulate spiking behavior\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aa46ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([[-0.4870,  1.1858,  1.5178, -0.3734, -1.9753, -1.6772, -0.6153, -0.3206,\n",
      "         -0.8044, -0.2002]])\n",
      "Spiked Output: tensor([[0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0.]], grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 10  # Output size is not used in this simplified example but included for consistency with the class definition\n",
    "\n",
    "# Instantiate the SNN model\n",
    "snn_model = SNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate a sample input (batch size, input size)\n",
    "sample_input = torch.randn((1, input_size))\n",
    "\n",
    "# Forward pass through the SNN\n",
    "spiked_output = snn_model(sample_input)\n",
    "\n",
    "print(\"Sample Input:\", sample_input)\n",
    "print(\"Spiked Output:\", spiked_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ada33",
   "metadata": {},
   "source": [
    "Output explanation: The output tensor is of size *hidden size* with values either 0 or 1, representing whether each neuron in the hidden layer fired (1) or did not fire(0) based on the simplified spike function.\n",
    "\n",
    "This example demonstrates the instantation and baic usage, however it is a highly abstracted version for practical implementation and scope limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b21f0",
   "metadata": {},
   "source": [
    "### Liquid State Machine (LSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860233f1",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Liquid State Machines (LSMs) are an innovative class of recurrent neural network architectures that belong to the broader category of reservoir computing. \n",
    "\n",
    "Distinguished by their dynamic \"liquid\" reservoir, LSMs excel in processing time-varying inputs, making them particularly adept at tasks requiring the handling of complex temporal patterns. The reservoir, a randomly connected network of spiking neurons, acts as a dynamic memory that transforms incoming signals into a high-dimensional space, allowing a simple readout layer to learn from this rich representation.\n",
    "\n",
    "This approach enables LSMs to tackle problems in real-time signal processing and pattern recognition with remarkable efficiency and minimal training requirements for the readout layer.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "LSMs are inherently designed to process time-sensitive data, making them well-suited for various types of temporal and spatiotemporal inputs, such as:\n",
    "- Time-series data\n",
    "- Speech and audio signals\n",
    "- Real-time sensory data\n",
    "- Dynamic patterns in video sequences\n",
    "\n",
    "Their capacity to handle rapidly changing inputs makes them ideal for environments where data evolves over time.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "LSMs are primarily used in tasks that demand the processing of time-dependent information, including:\n",
    "- Speech and language processing\n",
    "- Real-time sensory data interpretation\n",
    "- Dynamic pattern recognition and prediction\n",
    "- Brain-computer interfaces\n",
    "- Robotics and autonomous system controls\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of LSMs is largely dependent on the size and complexity of the liquid reservoir. While increasing the reservoir size can enhance the system's ability to model complex dynamics, it also raises the computational overhead. Nonetheless, the modular nature of the readout layer facilitates scalability by allowing it to be trained separately from the reservoir, easing the computational burden.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "LSMs exhibit inherent robustness to noise, courtesy of their high-dimensional reservoir space, which can filter and dilute irrelevant variations in the input signals. This attribute makes LSMs particularly resilient in noisy or unpredictable environments, maintaining their ability to extract relevant patterns and dynamics from the input data.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "In the realm of LSMs, variations often focus on optimizing the reservoir's structure or enhancing the efficiency of the readout mechanism:\n",
    "- **Echo State Networks (ESNs):** Although not spiking models, ESNs share similar principles with LSMs in terms of reservoir computing, focusing on continuous values.\n",
    "- **Spiking LSMs:** Adapt the LSM framework to include spiking neurons, further increasing biological plausibility and computational efficiency.\n",
    "- **Modular LSMs:** Implement reservoirs with modular architectures to improve processing of multi-dimensional or complex data types.\n",
    "\n",
    "**Unique Features**\n",
    "\n",
    "- **Dynamic Memory:** The reservoir provides a dynamic memory mechanism that captures and retains information about previous inputs over time, enabling effective processing of temporal patterns.\n",
    "- **Minimal Training:** Only the readout layer of an LSM requires training, significantly reducing the computational resources and time needed compared to fully trainable networks.\n",
    "- **High-Dimensional Processing:** By projecting inputs into a high-dimensional space, LSMs can disentangle complex patterns, enhancing the separability of different signal types or states.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use LSMs:**\n",
    "- For tasks involving complex temporal dynamics where traditional approaches struggle to capture the essence of time-varying patterns.\n",
    "- In scenarios requiring real-time processing and decision-making based on evolving data streams.\n",
    "\n",
    "**Common Uses:**\n",
    "\n",
    "- **Speech and Language Processing:** LSMs are adept at recognizing and generating temporal patterns in speech and language, making them valuable for applications in speech recognition, natural language understanding, and speech synthesis.\n",
    "\n",
    "- **Sensory Data Analysis:** Due to their ability to process real-time data streams, LSMs are well-suited for interpreting sensory data, such as visual or auditory signals, enabling applications in surveillance, security, and autonomous navigation.\n",
    "\n",
    "- **Robotic Control Systems:** LSMs can be applied in robotics to process sensory feedback and control signals in real-time, facilitating complex behaviors in autonomous robots and drones.\n",
    "\n",
    "- **Neuroscientific Modeling:** By mimicking aspects of biological neural networks, LSMs contribute to the study of brain functions, offering insights into how neural circuits process temporal information.\n",
    "\n",
    "- **Predictive Analytics:** Their proficiency in handling time-series data makes LSMs useful for forecasting and predictive modeling in various domains, including finance, weather prediction, and energy consumption.\n",
    "\n",
    "**Considerations:**\n",
    "- Designing and optimizing the reservoir's structure and parameters can be challenging, as it requires a balance between flexibility and the ability to capture relevant dynamics.\n",
    "- While LSMs are robust to noise and efficient in processing, the choice of readout and training methods can significantly impact performance and accuracy.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Liquid State Machines offer a compelling approach to handling complex temporal data, leveraging the dynamics of a \"liquid\" reservoir to achieve high levels of computational efficiency and robustness. Their unique structure and operational paradigm enable them to excel in a variety of applications that require real-time processing and analysis of time-varying signals. As research continues to advance in optimizing their architectures and expanding their applicability, LSMs stand as a promising tool in the pursuit of more adaptive and efficient computational models, particularly in the realms of signal processing, robotics, and cognitive computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1c70767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSM(SNN):\n",
    "    def __init__(self, input_size, reservoir_size, output_size):\n",
    "        super(LSM, self).__init__(input_size, reservoir_size, output_size)\n",
    "        # In a true LSM, the reservoir would be more complex and involve dynamic connections.\n",
    "        # Here, we simulate it with a single RNN layer for simplicity.\n",
    "        self.reservoir = nn.RNN(input_size, reservoir_size, batch_first=True)\n",
    "        # The readout layer\n",
    "        self.readout = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input through the simplified 'reservoir'\n",
    "        reservoir_state, _ = self.reservoir(x)\n",
    "        \n",
    "        # Assuming the last state as the representation\n",
    "        reservoir_state = reservoir_state[:, -1, :]\n",
    "        output = self.readout(reservoir_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "68dcf3c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSM Model: LSM(\n",
      "  (linear): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (reservoir): RNN(10, 128, batch_first=True)\n",
      "  (readout): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "reservoir_size = 128\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the LSM model\n",
    "lsm_model = LSM(input_size, reservoir_size, output_size)\n",
    "\n",
    "# Generate a sample input (batch size, sequence length, input size)\n",
    "# Create a batch of 5 sequences, each of length 7 (time steps) with 10 features\n",
    "sample_input = torch.randn((5, 7, input_size))\n",
    "\n",
    "# Forward pass through the LSM\n",
    "output = lsm_model(sample_input)\n",
    "\n",
    "print(\"LSM Model:\", lsm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "af548488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Input: tensor([[[-2.2870, -0.6020,  0.4581,  0.3155,  0.9874, -1.1581, -1.0296,\n",
      "          -0.5327,  1.7288, -0.1072],\n",
      "         [ 0.3248,  0.8543,  1.4226, -0.8097, -0.1075, -1.0899,  2.0022,\n",
      "           0.3711,  0.8556, -0.3455],\n",
      "         [-0.3686,  0.2758, -0.9305, -1.0735, -1.2743, -1.3175,  0.4138,\n",
      "          -0.0509, -0.5690,  0.8209],\n",
      "         [-1.0232, -1.1425, -0.2191,  0.3554, -0.9358,  0.5128, -0.9342,\n",
      "           0.7023, -0.7787,  1.0745],\n",
      "         [ 0.9797, -0.8769, -0.4176, -0.4004, -2.5792, -0.2520,  0.9345,\n",
      "          -1.5107,  0.8268,  1.1648],\n",
      "         [ 0.2904,  0.3135,  0.3127,  0.9124,  0.3826, -1.4527,  0.7033,\n",
      "          -1.1990,  0.0404, -1.9410],\n",
      "         [-0.1467, -0.8170,  0.5425, -2.0117, -1.0829,  0.1070, -1.5289,\n",
      "           1.2122,  0.7413, -0.5711]],\n",
      "\n",
      "        [[ 0.3061,  0.4070, -1.1646, -0.9781,  0.9480,  1.6887, -0.0564,\n",
      "           0.7411, -1.1133,  0.0513],\n",
      "         [-1.6780, -0.1483, -1.3194, -1.1139, -0.2867,  0.7030,  0.6239,\n",
      "          -1.7717, -0.6412,  0.9044],\n",
      "         [-1.7258,  0.7463, -0.8363, -1.1265, -1.2379,  0.6984, -0.4598,\n",
      "           0.7921, -2.2401,  0.2447],\n",
      "         [-0.3520,  0.6758, -0.0540,  1.4684,  0.9373, -0.0730,  0.6122,\n",
      "           0.4738,  0.6291,  1.0355],\n",
      "         [-0.4622, -0.8761, -0.2561, -2.3528, -0.4197,  1.4794, -1.0323,\n",
      "          -0.5490, -0.3609,  0.2979],\n",
      "         [-0.2544,  0.0739, -0.5543,  0.1975, -0.8214,  0.4094,  0.2922,\n",
      "           0.1413, -0.4379, -0.0491],\n",
      "         [-2.0431, -0.8473,  0.7986,  0.3923,  0.5557,  1.0087, -0.1559,\n",
      "          -1.2846,  0.4125, -0.5846]],\n",
      "\n",
      "        [[ 1.0507, -0.4560, -0.5245,  0.6221,  0.5760, -0.4387, -0.7495,\n",
      "           0.5808, -0.3545, -0.3442],\n",
      "         [-0.6188, -1.2067,  0.1126, -0.2016,  0.9083, -1.9682, -0.1365,\n",
      "          -1.3136, -2.0009, -1.0447],\n",
      "         [-0.3645,  0.2166, -1.1241, -0.2961, -0.6045, -0.2095, -0.6362,\n",
      "          -1.2790,  0.6715,  0.2289],\n",
      "         [ 0.7141,  1.8196,  0.9401,  1.1644,  1.5377, -1.4973, -0.2117,\n",
      "           1.3993,  0.2538,  0.0398],\n",
      "         [-0.9608,  0.2830,  0.3364,  0.4651, -0.2264, -0.2540, -0.0767,\n",
      "          -0.7723,  1.5548,  0.0535],\n",
      "         [-0.0153,  2.8105, -0.7657,  0.6707, -1.0392, -1.7544,  0.0366,\n",
      "          -0.4248, -0.2989, -1.2015],\n",
      "         [-0.5309,  1.5240, -0.9513, -1.8653,  0.1059, -1.6579, -0.9842,\n",
      "          -0.0211, -1.0208,  1.8346]],\n",
      "\n",
      "        [[ 0.9118, -1.6057,  0.2925, -1.0499,  1.4815,  0.6072, -0.4828,\n",
      "          -0.0656, -0.9293,  1.2727],\n",
      "         [-2.0355, -2.8474, -0.0926, -0.0787,  1.0841,  1.5255, -1.3043,\n",
      "          -0.6745,  1.4698,  0.1225],\n",
      "         [ 0.7422, -0.8681, -1.0049, -0.4040,  0.4772, -0.6752,  0.5651,\n",
      "          -1.4832, -2.1939,  0.8626],\n",
      "         [ 1.2574, -1.1390, -0.1056,  0.5788,  1.6011, -0.6134, -1.4023,\n",
      "           0.9085, -1.4533, -0.3623],\n",
      "         [ 0.2654, -1.0711,  1.8402,  1.5888,  2.1838, -0.6347, -0.7926,\n",
      "           0.6119,  0.2418, -0.0560],\n",
      "         [ 0.0291,  0.7780,  2.0324, -0.2695,  0.5308,  1.5990,  1.3978,\n",
      "          -1.0864, -0.7916,  0.0502],\n",
      "         [ 2.2167, -1.2562, -0.7064,  0.4896, -0.4926, -1.1468, -1.3719,\n",
      "           0.8584, -0.5673,  0.6885]],\n",
      "\n",
      "        [[ 0.9000, -1.3180, -0.5329, -1.7736, -2.2360, -0.6181, -1.0336,\n",
      "           0.3746, -1.7631, -2.6400],\n",
      "         [-0.0524, -0.5077, -0.4716, -0.4414, -1.2042,  0.3205,  0.3108,\n",
      "           1.0059, -1.1665, -0.7034],\n",
      "         [-0.1262, -0.9832,  0.0807, -0.0176, -1.2149, -0.6668,  2.1605,\n",
      "           0.8271,  0.6716, -0.2656],\n",
      "         [ 1.3978,  0.6908,  2.0845,  0.7155,  1.5583,  0.5783,  0.4191,\n",
      "          -0.3822,  0.3546, -0.8513],\n",
      "         [ 0.4692,  0.6699,  0.8917,  0.2911,  0.5080,  0.0229,  0.1135,\n",
      "          -1.2087, -0.7329,  1.2346],\n",
      "         [-2.2830, -0.1697, -1.4847,  1.4317, -0.6506, -0.0828, -0.1498,\n",
      "           0.8820, -1.3165,  1.5654],\n",
      "         [ 0.4285, -0.8424,  0.1187,  0.4183,  1.0837, -1.7125,  0.8806,\n",
      "          -2.0283,  1.8172,  0.5592]]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSample Input:\", sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fd426b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Shape: torch.Size([5, 1])\n",
      "\n",
      "Output: tensor([[-0.1373],\n",
      "        [ 0.1295],\n",
      "        [-0.0087],\n",
      "        [-0.0679],\n",
      "        [ 0.0045]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOutput Shape:\", output.shape)\n",
    "print(\"\\nOutput:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4109c9e",
   "metadata": {},
   "source": [
    "**Expected Output and Understanding**\n",
    "\n",
    "- **LSM Model:** This print statement will display the structure of the LSM model, including the RNN (reservoir) and the readout linear layer.\n",
    "- **Output Shape:** Since the readout layer's output size is 1, and you're processing a batch of 5 sequences, the output shape should be `[5, 1]`, indicating that for each sequence in the batch, you get a single output value.\n",
    "- **Output:** This will show the actual output values from the LSM. These values are generated by processing the synthetic sequential data through the LSM's reservoir and readout layer.\n",
    "\n",
    "This example is a straightforward demonstration meant to illustrate how you might set up and use an LSM model with PyTorch for sequence processing tasks. The synthetic data doesn't represent a specific real-world problem, but in practice, you could adapt this setup to work on tasks like time-series forecasting, sequence classification, or any problem where understanding temporal dynamics is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce46100",
   "metadata": {},
   "source": [
    "### Extreme Learning Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5025992",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Extreme Learning Machines (ELM) represent a unique class of feedforward neural networks that emphasize speed and efficiency in learning. Unlike traditional neural network methodologies, which adjust weights through iterative optimization (e.g., backpropagation), ELMs randomly assign weights to hidden nodes and only require the determination of the output weights. This simplification leads to significantly faster training times without compromising the network's ability to generalize well to unseen data.\n",
    "\n",
    "ELMs are particularly notable for their ability to handle regression, classification, clustering, and feature learning tasks with a fraction of the computational cost associated with conventional techniques. This approach hinges on the singular value decomposition or other least squares methods to solve the output weights, making the learning process exceptionally efficient.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "ELMs are versatile in their application, capable of processing a wide range of data types, including:\n",
    "- Numerical data\n",
    "- Categorical data\n",
    "- Image data\n",
    "- Time-series data\n",
    "\n",
    "This flexibility allows ELMs to be applied across various domains, from simple regression tasks to complex pattern recognition in images and speech.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "ELMs are designed to perform a variety of machine learning tasks efficiently, such as:\n",
    "- Binary and multi-class classification\n",
    "- Regression\n",
    "- Feature learning\n",
    "- Clustering\n",
    "\n",
    "Their rapid learning capability makes them suitable for scenarios where time and computational resources are limited.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of ELMs is highly advantageous, as the learning speed does not significantly degrade with the increase in data size or dimensionality. This characteristic is primarily due to the fixed, random nature of hidden layer weights and the linear nature of the output layer's learning process.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "ELMs demonstrate a commendable level of robustness to noise and overfitting, partly due to their regularization techniques and the non-iterative learning approach. These features make ELMs reliable for applications in noisy environments or where data may be prone to variations.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of ELMs have been developed to enhance their performance and applicability, including:\n",
    "- **Kernel ELMs (K-ELMs):** Utilize kernel methods to handle nonlinearly separable data more effectively.\n",
    "- **Online Sequential ELMs (OS-ELMs):** Adapt the ELM framework for sequential and time-series data processing, allowing for dynamic updates to the model.\n",
    "- **Convolutional ELMs (C-ELMs):** Combine ELM principles with convolutional structures to better handle spatial data, such as images.\n",
    "\n",
    "**Unique Features**\n",
    "\n",
    "- **Fast Learning Speed:** ELMs significantly reduce training time by eliminating the need for iterative weight adjustments in hidden layers.\n",
    "- **Simplicity and Efficiency:** The straightforward learning algorithm allows for easy implementation and rapid deployment of models.\n",
    "- **Generalization Capability:** Despite the simplified learning process, ELMs maintain a strong ability to generalize from training to unseen data.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use ELMs:**\n",
    "- In applications requiring rapid model training and deployment.\n",
    "- When working with limited computational resources or in need of low-power solutions.\n",
    "- For tasks involving a wide variety of data types and learning objectives.\n",
    "\n",
    "**Common Uses:**\n",
    "\n",
    "- **Image Processing and Recognition:** ELMs can efficiently handle high-dimensional image data for tasks such as object recognition and classification.\n",
    "- **Signal Processing:** Suitable for filtering, feature extraction, and classification of signals in real-time applications.\n",
    "- **Financial Analysis:** Applied in predictive modeling for stock market trends, credit scoring, and fraud detection due to their fast learning capability.\n",
    "- **Biomedical Applications:** Useful in diagnostic systems, patient data analysis, and bioinformatics research for rapid and efficient data processing.\n",
    "\n",
    "**Considerations:**\n",
    "- While ELMs offer many advantages, the randomness in hidden layer weights may sometimes lead to variability in model performance. Proper tuning and regularization can mitigate this issue.\n",
    "- The choice of the number of hidden nodes and activation function can significantly affect the model's accuracy and efficiency.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Extreme Learning Machines offer a compelling alternative to traditional neural network training methods by prioritizing speed and simplicity without sacrificing performance. Their broad applicability across different data types and learning tasks, combined with their computational efficiency, makes ELMs a valuable tool in the machine learning practitioner's toolkit. As the field evolves, further innovations and applications of ELMs are expected to emerge, expanding their utility and efficiency in solving complex real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2eacf9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ELM, self).__init__(input_size, hidden_size, output_size)\n",
    "        \n",
    "        # Initialize random weights for the hidden layer\n",
    "        self.hidden_weights = torch.randn(input_size, hidden_size)\n",
    "        self.hidden_bias = torch.randn(hidden_size)\n",
    "        \n",
    "        # No learning required for hidden layer, so no need for parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Linear output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate the output of the hidden layer\n",
    "        hidden_output = torch.matmul(x, self.hidden_weights) + self.hidden_bias\n",
    "        hidden_output = F.relu(hidden_output)  # Apply ReLU activation function\n",
    "        \n",
    "        # Pass the hidden layer output through the output layer\n",
    "        output = self.output_layer(hidden_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3d65d24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([[0.3551, 0.0431]])\n",
      "Output Prediction: [[-0.10309483]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "elm_model = ELM(input_size, hidden_size, output_size)\n",
    "sample_input = torch.randn((1, input_size))\n",
    "\n",
    "output_prediction = elm_model(sample_input)\n",
    "print(\"Sample Input:\", sample_input)\n",
    "print(\"Output Prediction:\", output_prediction.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d522611",
   "metadata": {},
   "source": [
    "### Echo State Network (ESN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f937df",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Echo State Networks (ESNs) belong to the reservoir computing family, distinguished for their novel approach to training recurrent neural networks (RNNs). ESNs simplify the training process by keeping the internal connections of the network (the \"reservoir\") fixed while only adjusting the weights of the output layer. This architecture enables ESNs to efficiently process temporal or sequential data, making them particularly adept at tasks requiring memory of past inputs, such as time-series forecasting and sequence modeling.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "ESNs are particularly tailored for:\n",
    "- Time-series forecasting\n",
    "- Sequence generation\n",
    "- Speech and audio processing\n",
    "- Any task involving dynamic temporal patterns\n",
    "\n",
    "Their design is inherently suited to deal with data that evolves over time, capturing the underlying temporal dynamics.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "ESNs excel in:\n",
    "- Predictive modeling in time-series analysis\n",
    "- Generating coherent sequences in text or music\n",
    "- Recognizing patterns in audio and speech signals\n",
    "- Simulating dynamical systems\n",
    "\n",
    "They thrive on tasks that require an understanding of time and sequence, making them ideal for applications where historical context significantly influences future predictions.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of ESNs is influenced by the size of the reservoir. A larger reservoir can capture more complex dynamics but at the cost of increased computational requirements. However, due to the fixed nature of the reservoir connections, the main computational effort lies in training the readout layer, which remains efficient even as the system scales.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "One of the strengths of ESNs is their robustness to noise, stemming from the reservoir's ability to project inputs into a high-dimensional space, effectively filtering out noise and enhancing signal features relevant for the task at hand.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Variations of ESNs focus on optimizing reservoir properties and connectivity patterns to improve performance and efficiency. These include:\n",
    "- **Leaky integrator neurons** to better model temporal dependencies\n",
    "- **Modular ESNs** for handling multi-scale temporal patterns\n",
    "- **Deep ESNs** that layer multiple reservoirs for more complex hierarchical processing\n",
    "\n",
    "**Unique Features**\n",
    "\n",
    "- **Dynamic Reservoir:** Provides a flexible, high-dimensional representation of input sequences, enabling the capture of long-term dependencies.\n",
    "- **Efficient Training:** Requires training only the readout layer, significantly reducing the computational cost and complexity compared to traditional recurrent neural networks.\n",
    "- **Adaptability:** Can be easily adapted and optimized for a wide range of time-series tasks without extensive retraining or modification.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use ESNs:**\n",
    "- In scenarios where capturing complex temporal dynamics is essential, but data availability is limited.\n",
    "- For applications requiring quick adaptation to new patterns or rapid deployment of time-series models.\n",
    "\n",
    "**Common Uses:**\n",
    "\n",
    "- **Financial Market Prediction:** ESNs can analyze historical market data to forecast future trends and fluctuations.\n",
    "- **Environmental Modeling:** Useful in predicting natural phenomena, such as weather patterns or ecological changes, by learning from temporal sequences.\n",
    "- **Health Monitoring:** ESNs process real-time health data, predicting potential anomalies or diseases based on historical patterns.\n",
    "- **Energy Demand Forecasting:** They can predict future energy demands by analyzing consumption patterns over time.\n",
    "\n",
    "**Considerations:**\n",
    "- While ESNs offer a powerful tool for time-series analysis, the choice of reservoir size and connectivity patterns can significantly impact their performance.\n",
    "- Ensuring the reservoir's dynamics are rich yet stable requires careful tuning of parameters, which can be both an art and a science.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Echo State Networks offer a versatile and efficient approach to time-series analysis, combining the power of high-dimensional signal processing with the simplicity of linear readout training. Their ability to capture and leverage complex temporal dynamics makes them a valuable tool in various domains, from financial forecasting to environmental modeling. As research continues to refine and expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3407498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESN(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size):\n",
    "        super(ESN, self).__init__()\n",
    "        # Initialize weights\n",
    "        self.W_in = nn.Parameter(torch.randn(input_size, reservoir_size) * 0.1)  # Scaled for stability\n",
    "        self.W_res = nn.Parameter(torch.randn(reservoir_size, reservoir_size) * 0.1)\n",
    "        self.W_out = nn.Parameter(torch.randn(reservoir_size, output_size) * 0.1)\n",
    "        \n",
    "        # Ensure the echo state property through spectral radius adjustment\n",
    "        spectral_radius = torch.max(abs(torch.linalg.eigvals(self.W_res)))\n",
    "        self.W_res.data /= spectral_radius / 0.95  # Adjust to slightly below 1 for stability\n",
    "        \n",
    "        # Initialize the reservoir state\n",
    "        self.reservoir_state = torch.zeros(1, reservoir_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update the reservoir with current input and previous state\n",
    "        self.reservoir_state = torch.tanh(x @ self.W_in + self.reservoir_state @ self.W_res)\n",
    "        \n",
    "        # Compute the output\n",
    "        output = self.reservoir_state @ self.W_out\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "cc9aad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([[ 0.9712, -0.5128]])\n",
      "Output Prediction: [[-0.01740052]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 2\n",
    "reservoir_size = 5\n",
    "output_size = 1\n",
    "\n",
    "esn_model = ESN(input_size, reservoir_size, output_size)\n",
    "sample_input = torch.randn((1, input_size))\n",
    "\n",
    "output_prediction = esn_model(sample_input)\n",
    "print(\"Sample Input:\", sample_input)\n",
    "print(\"Output Prediction:\", output_prediction.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b37044",
   "metadata": {},
   "source": [
    "### Deep Residual Network (DRN) (ResNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cf6d8",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Deep Residual Networks (ResNets) represent a revolutionary breakthrough in deep learning, particularly in the context of convolutional neural networks (CNNs). Introduced to address the vanishing gradient problem and facilitate the training of networks that are substantially deeper than those previously feasible, ResNets introduce the concept of residual learning. \n",
    "\n",
    "At their core, ResNets are designed to learn residual functions with reference to the layer inputs, as opposed to learning unreferenced functions. This is achieved through the use of \"shortcut connections\" (or skip connections) that bypass one or more layers. These connections perform identity mapping, and their outputs are added to the outputs of the stacked layers, effectively allowing the network to learn modifications to the identity mapping rather than the entire transformation, which has proven to be easier and more effective.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "ResNets are capable of processing a wide range of data types, but they are most commonly applied to:\n",
    "- Image data\n",
    "- Video sequences\n",
    "- Any high-dimensional data that can benefit from deep feature extraction\n",
    "\n",
    "This makes ResNets particularly powerful for tasks in computer vision, where deep feature hierarchies are crucial.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "ResNets have been successfully applied to a variety of tasks, including but not limited to:\n",
    "- Image classification\n",
    "- Object detection\n",
    "- Semantic segmentation\n",
    "- Face recognition\n",
    "\n",
    "Their ability to efficiently model complex hierarchies of features makes them suitable for virtually any task that can benefit from deep learning.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "One of the hallmark features of ResNets is their scalability. They have been successfully trained with depths of over a hundred layers, and in some configurations, even beyond a thousand layers. This scalability is largely attributable to their residual learning framework, which mitigates the vanishing gradient problem and allows for effective training of very deep networks.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "ResNets demonstrate a strong robustness to noise and perturbations in the input data, a trait that is particularly valuable in real-world applications where data can be noisy or incomplete. The skip connections help in propagating gradients throughout the network, ensuring that even the deepest layers can adjust and learn from the training data.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of the original ResNet architecture have been proposed, including:\n",
    "- **ResNet-V2:** Improves upon the original by modifying the placement of activation functions and normalization layers.\n",
    "- **ResNeXt:** Introduces a \"cardinality\" dimension, which offers a way to increase model capacity with a more efficient use of parameters.\n",
    "- **Wide ResNets (WRN):** Adjusts the width of the ResNet layers, providing an alternative approach to increasing capacity and performance.\n",
    "\n",
    "**Unique Features**\n",
    "\n",
    "- **Skip Connections:** Allow the network to bypass layers, which helps alleviate the vanishing gradient problem and enables the training of very deep networks.\n",
    "- **Ease of Optimization:** ResNets are easier to optimize compared to traditional deep CNNs, thanks to the residual learning principle.\n",
    "- **Adaptability:** The architecture is highly adaptable and has seen success in a wide range of applications beyond image processing, including audio recognition and natural language processing.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use ResNets:**\n",
    "- For tasks requiring deep feature extraction, such as high-level image recognition and classification.\n",
    "- In scenarios where training deep networks has been challenging due to vanishing gradients.\n",
    "\n",
    "**Common Uses:**\n",
    "\n",
    "- **Image and Video Recognition:** ResNets have set new benchmarks in accuracy for image classification and video analysis tasks.\n",
    "- **Medical Image Analysis:** Their deep feature extraction capabilities make them ideal for diagnosing diseases from medical imagery.\n",
    "- **Autonomous Vehicles:** Used for object detection and scene understanding, crucial for the navigation systems of self-driving cars.\n",
    "\n",
    "**Considerations:**\n",
    "- While ResNets allow for the training of very deep networks, careful consideration must be given to the specific architecture and depth, as overly complex models may overfit on smaller datasets.\n",
    "- The success of ResNet models also underscores the importance of residual learning as a strategy for training deep networks, influencing the development of new architectures in deep learning.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Deep Residual Networks represent a major milestone in the evolution of neural network architectures, offering unparalleled depth and performance in a wide range of computer vision tasks. Their innovative design not only tackles longstanding challenges in training deep networks but also sets a new standard for what's achievable in machine learning and artificial intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1e316558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "66aef728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_blocks):\n",
    "        super(ResNet, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.in_channels = 64\n",
    "        self.conv = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(self.in_channels, hidden_size, num_blocks, stride=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # The number of output channels from the last block will be `hidden_size`\n",
    "        # Adjust this based on architecture\n",
    "        final_out_channels = hidden_size\n",
    "\n",
    "        # Initialize the fully connected layer with the correct number of input features\n",
    "        self.fc = nn.Linear(final_out_channels, output_size)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "618f712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Output shape: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "# Instantiate the ResNet model\n",
    "resnet_model = ResNet(input_size=3, hidden_size=64, output_size=10, num_blocks=2)\n",
    "\n",
    "# Define a sample input tensor\n",
    "sample_input = torch.rand((4, 3, 224, 224))  # Batch size, Channels, Height, Width\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_resnet = resnet_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(resnet_model)\n",
    "print(\"Output shape:\", output_resnet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7afff0",
   "metadata": {},
   "source": [
    "### Kohonen Networks (KN) / Self-Organizing Feature Map (SOFM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbbc8d",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Kohonen Networks, also known as Self-Organizing Maps (SOMs) or Self-Organizing Feature Maps (SOFMs), are a type of artificial neural network that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples. \n",
    "\n",
    "This method makes them particularly useful for visualizing high-dimensional data. Through the self-organizing process, Kohonen Networks are able to capture the topological properties of the input space, making them an excellent tool for dimensionality reduction, clustering, and feature mapping.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Kohonen Networks are versatile in their application and can process a wide range of data types, including:\n",
    "- Multivariate data\n",
    "- Images and visual patterns\n",
    "- Text data\n",
    "- Complex numerical datasets\n",
    "\n",
    "Their ability to map high-dimensional data onto a lower-dimensional grid makes them suitable for tasks where data visualization and clustering are critical.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "The primary objectives of Kohonen Networks include:\n",
    "- Data visualization: Simplifying complex high-dimensional data into a more interpretable form.\n",
    "- Clustering: Grouping similar data points together based on their characteristics.\n",
    "- Feature extraction: Identifying the most relevant features in the dataset.\n",
    "- Pattern recognition: Identifying patterns within datasets that may not be immediately obvious.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of Kohonen Networks depends on the size of the map and the dimensionality of the input data. Larger maps can represent more complex data but require more computational resources. Adjusting the size and topology of the map allows for flexibility in managing the trade-off between detail and computational expense.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Kohonen Networks are relatively robust to noise due to their competitive learning mechanism, which tends to emphasize the most significant patterns in the data. However, the presence of too much noise can still affect the quality of the mapping, potentially leading to less distinct clusters.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variations of Kohonen Networks have been developed to enhance their performance and applicability, including:\n",
    "- **Toroidal SOMs:** Employing a toroidal grid to minimize edge effects.\n",
    "- **Growing SOMs:** Dynamically adjusting the size of the map to better fit the data.\n",
    "- **Kernel SOMs:** Incorporating kernel methods to handle non-linear mappings.\n",
    "\n",
    "**Unique Features**\n",
    "\n",
    "- **Topological Preservation:** Kohonen Networks maintain the topological properties of the input space, ensuring that similar data points in the high-dimensional space remain close on the map.\n",
    "- **Unsupervised Learning:** They do not require labeled data for training, making them suitable for exploratory data analysis.\n",
    "- **Visualization:** The two-dimensional grid representation provides an intuitive way to visualize complex datasets.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Kohonen Networks:**\n",
    "- For exploratory data analysis when you want to uncover underlying patterns or structures in the data without predefined labels.\n",
    "- In situations where visualizing high-dimensional data in a lower-dimensional space can aid in understanding or communication.\n",
    "\n",
    "**Common Uses:**\n",
    "\n",
    "- **Market Segmentation:** Clustering customers based on purchasing behavior or preferences.\n",
    "- **Bioinformatics:** Analyzing genetic or protein expression data for patterns or clusters.\n",
    "- **Image Processing:** Feature extraction and pattern recognition in images.\n",
    "- **Financial Analysis:** Identifying patterns in market data or customer segments.\n",
    "\n",
    "**Considerations:**\n",
    "- Choosing the appropriate map size and learning parameters is crucial for achieving meaningful results.\n",
    "- The training process can be computationally intensive for large datasets or maps.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Kohonen Networks offer a unique approach to the unsupervised learning of high-dimensional data, providing a structured way to visualize and cluster complex datasets. By preserving the topological and metric relationships of the input space, SOMs serve as a valuable tool in the data scientist's toolkit, especially for tasks involving data exploration and understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "782be121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOFM(nn.Module):\n",
    "    def __init__(self, input_dim, map_size, lr=0.1, sigma=None):\n",
    "        super(SOFM, self).__init__()\n",
    "        self.map_size = map_size\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma if sigma is not None else max(map_size) / 2  # Initial radius\n",
    "        self.weight = nn.Parameter(torch.randn(map_size[0], map_size[1], input_dim))\n",
    "        self.locations = torch.tensor(np.array([[i, j] for i in range(map_size[0]) for j in range(map_size[1])])).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x is compatible with weight dimensions for broadcasting\n",
    "        x = x.view(1, 1, -1)  # Reshape x to [1, 1, input_dim]\n",
    "\n",
    "        # Calculate square difference\n",
    "        sq_diff = torch.sum((self.weight - x) ** 2, dim=2)\n",
    "\n",
    "        # Find the best matching unit (BMU)\n",
    "        _, bmu_idx = torch.min(sq_diff.view(-1), dim=0)  # Flatten sq_diff and find index of BMU\n",
    "\n",
    "        # Retrieve the BMU location\n",
    "        bmu_location = self.locations[bmu_idx]  # Correctly index into self.locations\n",
    "\n",
    "        # Calculate distance squared from all neurons to BMU\n",
    "        distance_sq = torch.sum((self.locations - bmu_location) ** 2, dim=1)\n",
    "        lr = self.lr * torch.exp(-distance_sq / (2 * self.sigma ** 2))  # Adjust learning rate based on distance\n",
    "\n",
    "        # Apply learning rate to weight update\n",
    "        # Ensure lr is correctly shaped for broadcasting\n",
    "        lr = lr.view(self.map_size[0], self.map_size[1], 1)\n",
    "        weight_update = lr * (x - self.weight)\n",
    "\n",
    "        # Update the weights\n",
    "        self.weight.data += weight_update\n",
    "\n",
    "        return bmu_idx\n",
    "\n",
    "    def train_sofm(self, data, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for x in data:\n",
    "                self.forward(x)\n",
    "            # Decay learning parameters\n",
    "            self.lr *= 0.995  # Learning rate decay\n",
    "            self.sigma *= 0.995  # Radius decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd1f9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_dim = 3  # Dimensionality of input data\n",
    "map_size = (10, 10)  # Size of the SOFM map\n",
    "\n",
    "sofm = SOFM(input_dim=input_dim, map_size=map_size)\n",
    "data = torch.rand(100, input_dim)  # Example dataset\n",
    "\n",
    "sofm.train_sofm(data, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cbd99",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a90307",
   "metadata": {},
   "source": [
    "### Support Vector Machines: A Comprehensive Analysis\n",
    "\n",
    "**High-Level Overview**\n",
    "\n",
    "Support Vector Machines (SVMs) are a powerful class of supervised learning models used for classification and regression tasks. Developed in the 1960s and refined in the 1990s, SVMs are based on the concept of finding the hyperplane that best separates different classes in the feature space. By maximizing the margin between the nearest points of the classes (support vectors), SVMs achieve high generalization ability, making them highly effective for a wide range of pattern recognition tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "SVMs are versatile and can process:\n",
    "- Numerical data\n",
    "- Categorical data (after encoding)\n",
    "- Text data (using TF-IDF or word embeddings)\n",
    "- Image data (using feature extraction techniques)\n",
    "\n",
    "This flexibility allows SVMs to be applied in various domains, from document classification to image recognition.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "SVMs are primarily used for:\n",
    "- Binary classification\n",
    "- Multiclass classification\n",
    "- Regression tasks\n",
    "- Outlier detection\n",
    "\n",
    "Their effectiveness in high-dimensional spaces and with complex decision boundaries makes them suitable for tasks requiring precise and robust classification and regression models.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "While SVMs perform exceptionally well on small to medium-sized datasets, their computational complexity can become a challenge with very large datasets or extremely high-dimensional spaces. Kernel tricks and dimensionality reduction techniques are often used to mitigate these challenges and enhance scalability.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "SVMs exhibit a significant level of robustness to noise and overfitting, especially in scenarios where the margin is maximized with a correct choice of the regularization parameter. Their reliance on support vectors (the most informative data points) rather than the entire dataset contributes to their resilience.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of SVMs exist to cater to specific needs, including:\n",
    "- **Linear SVMs:** Best suited for linearly separable data.\n",
    "- **Kernel SVMs:** Use kernel functions to operate in a transformed feature space, allowing them to handle non-linear data.\n",
    "- **Nu-SVMs and C-SVMs:** Offer different formulations for the optimization problem, providing flexibility in controlling the trade-off between margin size and misclassification error.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use SVMs:**\n",
    "- In binary or multiclass classification problems with clear margin separation.\n",
    "- For text and image classification tasks where high-dimensional feature spaces are common.\n",
    "- In applications where model interpretability is important, as the support vectors and the decision boundary provide insights into the model's predictions.\n",
    "\n",
    "**Considerations:**\n",
    "- Choosing the right kernel and tuning hyperparameters (like C and gamma) are crucial steps that significantly impact SVM performance.\n",
    "- SVMs may require more preprocessing effort, such as normalization and encoding, to ensure optimal model training.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Support Vector Machines stand out for their robustness, versatility, and efficacy in handling classification and regression tasks across a broad spectrum of domains. By effectively navigating the trade-offs between complexity and performance, practitioners can leverage SVMs to build highly accurate, generalizable models for a diverse array of challenges in machine learning and pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4a8cf267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearSVM, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)  # Output size is 1 for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def hinge_loss(y_pred, y_true):\n",
    "    return torch.mean(torch.clamp(1 - y_pred * y_true, min=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c9a383",
   "metadata": {},
   "source": [
    "Hinge Loss: A custom function that computes the hinge loss, encouraging the model to not only correctly classify training samples but also to maximize the margin between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a454ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.7767\n",
      "Epoch [20/100], Loss: 0.6771\n",
      "Epoch [30/100], Loss: 0.6226\n",
      "Epoch [40/100], Loss: 0.5987\n",
      "Epoch [50/100], Loss: 0.5754\n",
      "Epoch [60/100], Loss: 0.5521\n",
      "Epoch [70/100], Loss: 0.5289\n",
      "Epoch [80/100], Loss: 0.5057\n",
      "Epoch [90/100], Loss: 0.4824\n",
      "Epoch [100/100], Loss: 0.4592\n",
      "Accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 2  # Number of features\n",
    "output_size = 1  # Binary classification\n",
    "\n",
    "# Generate synthetic data for binary classification\n",
    "# Class 0: centered at (0.5, 0.5), Class 1: centered at (1.5, 1.5)\n",
    "n_samples = 100\n",
    "x_class0 = torch.rand((n_samples//2, input_size)) * 0.5\n",
    "y_class0 = torch.zeros(n_samples//2, 1)\n",
    "x_class1 = torch.rand((n_samples//2, input_size)) * 0.5 + 1\n",
    "y_class1 = torch.ones(n_samples//2, 1)\n",
    "\n",
    "x_train = torch.cat([x_class0, x_class1], dim=0)\n",
    "y_train = torch.cat([y_class0, y_class1], dim=0)\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = LinearSVM(input_size, output_size)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(svm_model.parameters(), lr=0.02)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = svm_model(x_train).squeeze()  # Ensure output matches dimension of y_train\n",
    "    labels = 2 * y_train.squeeze() - 1  # Convert labels to {-1, 1}\n",
    "    loss = hinge_loss(outputs, labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    y_pred = svm_model(x_train).squeeze()\n",
    "    y_pred_labels = (y_pred > 0).float()\n",
    "    accuracy = (y_pred_labels == y_train.squeeze()).float().mean()\n",
    "    print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88337c",
   "metadata": {},
   "source": [
    "#### Simplifications and Disclaimers\n",
    "Linear Decision Boundary: This example focuses on a linear SVM, suitable for linearly separable data. Real-world datasets often require more complex models, such as kernel SVMs, to capture non-linear relationships.\n",
    "Gradient Descent Optimization: While traditional SVMs are typically trained using quadratic programming solvers to directly solve the convex optimization problem, this example employs gradient descent for simplicity and educational clarity.\n",
    "Feature Space and Data Type: The demonstration uses synthetic numerical data. In practice, SVMs can be applied to a wide range of data types, including categorical and text data, often requiring preprocessing steps like encoding and feature extraction.\n",
    "Performance Metrics: The example primarily evaluates the model based on accuracy. Comprehensive model evaluation might include additional metrics and validation techniques to assess performance thoroughly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
