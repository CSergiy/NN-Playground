{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621cc43b",
   "metadata": {},
   "source": [
    "# Welcome to the Neural Network Playground\n",
    "\n",
    "Welcome to this Jupyter notebook, the interactive companion to the \"Neural Network Playground\" repository. Here, we embark on a fascinating journey through the diverse world of neural networks, exploring various architectures and their unique capabilities. This notebook is crafted to be both an educational resource and a practical guide, providing you with the opportunity to dive deep into the functionalities, designs, and applications of neural networks across different tasks and data types.\n",
    "\n",
    "Through descriptive explanations, code implementations, and hands-on examples, we aim to foster a deeper understanding of neural networks and inspire you to experiment, innovate, and contribute to the field of artificial intelligence and machine learning. Let's begin our exploration and unlock the potential of neural networks together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159cdffd",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "## [Setup](#Setup)\n",
    "- [Imports](#Imports)\n",
    "\n",
    "## [Foundational Concepts](#Foundational-Concepts)\n",
    "- [Base Neural Network Class (BaseNN)](#Base-Neural-Network-Class-(BaseNN))\n",
    "\n",
    "#### Foundational Models:\n",
    "- **Basic Neural Networks**: \n",
    "  - Perceptron\n",
    "  - Feed Forward\n",
    "  - Radial Basis Function Network\n",
    "- **Deep Learning Essentials**: \n",
    "  - Deep Feed Forward (DFF)\n",
    "\n",
    "#### Deep Learning Architectures:\n",
    "- **Core Architectures**: \n",
    "  - **Autoencoders**\n",
    "      - **AE**\n",
    "      - **VAE**\n",
    "      - **DAE**\n",
    "      - **SAE**\n",
    "  - Deep Convolutional Network (DCN) \n",
    "  \n",
    "- **Recurrent and Memory Models**: \n",
    "  - RNN\n",
    "  - LSTM\n",
    "  - GRU\n",
    "  - NTM\n",
    "\n",
    "#### Advanced Concepts:\n",
    "- **Probabilistic and Generative Models**: \n",
    "  - Markov Chain\n",
    "  - Hopfield Network\n",
    "  - Boltzmann Machine\n",
    "  - Restricted Boltzmann Machine\n",
    "  - Deep Belief Network\n",
    "  - GAN\n",
    "- **Hybrid and Specialized Models**: \n",
    "  - SNN\n",
    "  - LSM\n",
    "  - ELM\n",
    "  - ESN\n",
    "  - ResNet\n",
    "  - Kohonen Network (SOFM)\n",
    "  - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29765f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f39bb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0495dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d657427",
   "metadata": {},
   "source": [
    "# Foundational Concepts\n",
    "## Base Neural Network Class (BaseNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7db57",
   "metadata": {},
   "source": [
    "Creating a BaseNN class intended to use inheritance in later implementations of different NN's when abstracting the base class to make specialized classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18f98a",
   "metadata": {},
   "source": [
    "In the development of this neural network project, we introduce the `BaseNN` class as a foundational component, leveraging the principles of object-oriented programming to foster a modular and scalable approach to neural network design. \n",
    "\n",
    "The `BaseNN` class serves as a blueprint for all subsequent neural network models, encapsulating common attributes such as `input_size`, `hidden_size`, and `output_size`. \n",
    "\n",
    "These attributes are essential across a wide range of neural network architectures, ensuring a consistent structure across our implementations. \n",
    "\n",
    "Furthermore, the class defines an abstract `forward` method, which obliges any derived class to specify its own data processing mechanism, detailing how inputs are transformed into outputs through the network. This approach not only enforces a uniform interface but also promotes code reusability and simplifies the process of experimenting with and extending different neural network models. \n",
    "\n",
    "By abstracting common functionalities into the `BaseNN` class, we significantly reduce redundancy and streamline the development of specialized neural network architectures, allowing for a clear and efficient exploration of the vast landscape of neural network designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdcf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for neural networks\n",
    "class BaseNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BaseNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"forward method must be implemented in derived classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab62054",
   "metadata": {},
   "source": [
    "# Foundational Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953cb1e",
   "metadata": {},
   "source": [
    "## Basic Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877d9a",
   "metadata": {},
   "source": [
    "### Perceptron (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48ab51",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "The Perceptron represents the simplest form of a feedforward neural network, consisting of a single neuron with adjustable weights and a bias. Developed in 1957 by Frank Rosenblatt, it laid the groundwork for understanding neural networks. The Perceptron algorithm is a binary classifier that linearly separates data into two parts, making it a cornerstone in the study of machine learning for simple predictive modeling tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Perceptrons can process:\n",
    "- Numerical data\n",
    "- Binary features\n",
    "\n",
    "Given its simplicity, it's primarily suited for linearly separable datasets where inputs can be categorized into two distinct groups.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Perceptrons are utilized for:\n",
    "- Binary classification tasks\n",
    "- Basic pattern recognition\n",
    "\n",
    "Their straightforward approach allows them to make decisions by weighing input features, showcasing early neural network capabilities in distinguishing between two classes.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Due to its simplicity, the scalability of a single-layer Perceptron is limited to problems that are linearly separable. For more complex datasets or non-linear problems, multi-layer networks or different algorithms are recommended.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Perceptrons can be sensitive to noise in the data, especially since they do not incorporate error minimization in the same way as more advanced models. They perform best with clean, well-defined datasets.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "While the basic Perceptron is foundational, several key developments have been made to extend its utility, including:\n",
    "- **Multi-layer Perceptrons (MLPs):** Comprising multiple layers of neurons to tackle non-linearly separable data.\n",
    "- **Stochastic Gradient Descent:** An optimization method allowing Perceptrons and their multi-layer successors to learn from training data iteratively.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Perceptrons:**\n",
    "- For simple linear classification problems.\n",
    "- As a learning tool to understand the basics of neural network architecture and linear decision boundaries.\n",
    "\n",
    "**Considerations:**\n",
    "- The Perceptron's inability to solve non-linear problems limits its application in complex real-world scenarios.\n",
    "- It serves as a building block for more sophisticated networks that can handle a broader range of tasks.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "The Perceptron model, with its simplicity, offers a fundamental understanding of neural network principles. Although its direct applications are limited to linearly separable tasks, the Perceptron remains an essential concept in machine learning, providing a stepping stone to more advanced neural network architectures and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83688fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize weights and bias randomly\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "\n",
    "    def activate(self, x):\n",
    "        # Simple step function as activation\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum of inputs\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "        # Apply the activation function\n",
    "        output = self.activate(weighted_sum)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f716aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0.5 0.8]\n",
      "Output: 1\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a perceptron with 2 input cells\n",
    "    perceptron = Perceptron(input_size=2)\n",
    "\n",
    "    # Example input\n",
    "    input_data = np.array([0.5, 0.8])\n",
    "\n",
    "    # Get the output from the perceptron\n",
    "    output = perceptron.forward(input_data)\n",
    "\n",
    "    print(f\"Input: {input_data}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e298f2a",
   "metadata": {},
   "source": [
    "### Feed Forward (FF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d803f",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Feed Forward Neural Networks (FFNNs) are the simplest type of artificial neural network architecture, where connections between the nodes do not form a cycle. This model is structured in layers, consisting of an input layer, one or more hidden layers, and an output layer. The data moves in only one direction - forward - from the input nodes, through the hidden nodes (if any), and finally to the output nodes. There are no cycles or loops in the network, hence the name \"feedforward.\"\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Feed Forward Neural Networks are capable of handling a variety of data types, making them versatile for numerous applications:\n",
    "\n",
    "- Numerical data\n",
    "- Categorical data\n",
    "- Images (when flattened to a vector)\n",
    "- Text (via bag-of-words or TF-IDF vectors)\n",
    "\n",
    "Their adaptability makes FFNNs suitable for a broad range of tasks across different fields.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "FFNNs are widely used for:\n",
    "\n",
    "- Classification tasks, both binary and multi-class.\n",
    "- Regression tasks for predicting continuous outcomes.\n",
    "- Pattern recognition, serving as the foundational architecture for more complex tasks.\n",
    "\n",
    "They serve as the backbone for understanding more complex neural network architectures.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of Feed Forward Neural Networks depends on the size of the input data and the complexity of the task. While adding more hidden layers can increase the network's capacity to learn complex patterns, it also raises the computational cost and the risk of overfitting. Techniques like dropout and regularization are often employed to manage these challenges.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "FFNNs exhibit a degree of robustness to noise in the input data, thanks to their capacity to learn generalized representations. However, their performance can be significantly affected by the presence of irrelevant features or highly noisy datasets, necessitating careful data preprocessing and feature selection.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Feed Forward Neural Networks can be implemented with various activation functions (ReLU, Sigmoid, Tanh) and architectures (deep networks with many layers, wide networks with more neurons per layer) to suit specific problems:\n",
    "\n",
    "- **Deep Feed Forward Networks**: Incorporate multiple hidden layers to capture complex patterns.\n",
    "- **Wide Networks**: Increase the number of neurons in hidden layers to enhance model capacity without deepening the architecture.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Feed Forward Neural Networks:**\n",
    "\n",
    "- For straightforward prediction problems where the complexity of recurrent or convolutional networks is unnecessary.\n",
    "- In cases where the data can be represented in a fixed-size vector and does not possess inherent sequential or spatial patterns.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- While FFNNs are powerful for many tasks, they may not be ideal for data with temporal sequences (e.g., time-series) or spatial hierarchies (e.g., images), where recurrent or convolutional architectures might be more appropriate.\n",
    "- Careful design and regularization are essential to prevent overfitting, especially as the network size increases.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Feed Forward Neural Networks form the cornerstone of neural network models, offering a straightforward yet powerful framework for numerous predictive modeling tasks. Their simplicity, coupled with the potential for customization and scalability, makes them an indispensable tool. Understanding and mastering FFNNs provide a solid foundation for delving into more specialized neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3c747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)  # Single output neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        x = torch.relu(self.hidden_layer(x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the neural network\n",
    "input_size = 2  # Number of input features \n",
    "hidden_size = 3  # Number of neurons in the hidden layers\n",
    "model = FeedforwardNN(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09fa362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNN(\n",
      "  (input_layer): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (hidden_layer): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Input: tensor([[0.5000, 0.3000]])\n",
      "Output: 0.47536784410476685\n"
     ]
    }
   ],
   "source": [
    "# Define a sample input\n",
    "sample_input = torch.tensor([[0.5, 0.3]])  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output = model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(model)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output:\", output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03243f0",
   "metadata": {},
   "source": [
    "### Radial Basis Network (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb35815",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Radial Basis Function Networks (RBFNs) are a type of artificial neural network that uses radial basis functions as activation functions. They are typically used for interpolation in multidimensional space, pattern recognition, function approximation, and time-series prediction. The core idea behind RBFNs is to transform the input space into a new space where the problem becomes linearly separable. This transformation is achieved using a set of radial basis functions, each associated with a center and affecting only the region close to that center.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "RBF Networks are particularly effective with:\n",
    "\n",
    "- Numerical data\n",
    "- Multidimensional data for function approximation\n",
    "- Patterns that require a localized response\n",
    "\n",
    "Their ability to handle non-linear problems makes them suitable for various tasks in regression, classification, and clustering.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "RBF Networks excel in:\n",
    "\n",
    "- Function approximation and regression tasks\n",
    "- Classification problems\n",
    "- Time-series prediction\n",
    "- Clustering and unsupervised learning\n",
    "\n",
    "The localized nature of radial basis functions allows RBFNs to model complex and non-linear relationships within the data efficiently.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of RBF Networks can be challenging due to the need to select an appropriate number of centers and their locations. While having more centers can improve the network's ability to approximate complex functions, it also increases the computational cost and the risk of overfitting.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "RBF Networks demonstrate robustness to noise in the input data due to the smoothness of the radial basis functions. However, the choice of the width parameter of the basis functions is crucial, as it influences the network's sensitivity to the input data's scale and noise level.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variations of RBF Networks exist, primarily differing in how the centers and the width of the basis functions are determined:\n",
    "\n",
    "- **Fixed Centers Selected Randomly**: Centers are chosen randomly from the input data.\n",
    "- **Clustering-based Centers**: Centers are determined using clustering algorithms like k-means to capture the data's underlying structure.\n",
    "- **Orthogonal Least Squares (OLS)**: A more sophisticated method for selecting centers that aims to minimize redundancy among the basis functions.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Radial Basis Function Networks:**\n",
    "\n",
    "- In situations where the data exhibits non-linear relationships that need to be captured with high precision.\n",
    "- For problems where local interactions dominate the system's behavior, and a global approximation model might not be effective.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- The selection of the number of centers and their locations is critical for the performance of the RBF Network. Incorrect choices can lead to poor generalization or overfitting.\n",
    "- The determination of the width parameter requires careful tuning, often based on cross-validation, to balance the trade-off between bias and variance.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Radial Basis Function Networks offer a powerful and flexible framework for addressing non-linear problems across various domains. By leveraging localized responses to input stimuli, RBFNs can model complex relationships within data, making them a valuable tool for tasks requiring high precision in function approximation, classification, and beyond. Understanding and effectively implementing RBF Networks can provide practitioners with a robust method for tackling challenging problems that traditional linear models cannot solve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32822c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFunction:\n",
    "    def __init__(self, input_size, num_centers):\n",
    "        # Initialize centers and width parameters randomly\n",
    "        self.centers = np.random.rand(num_centers, input_size)\n",
    "        self.width = np.random.rand()\n",
    "        self.weights = np.random.rand(num_centers)\n",
    "    \n",
    "    def gaussian(self, x, center, width):\n",
    "        # Gaussian activation function\n",
    "        return np.exp(-np.sum((x - center)**2) / (2 * width**2))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Calculate the activation for each center\n",
    "        activations = np.array([self.gaussian(inputs, center, self.width) for center in self.centers])\n",
    "        \n",
    "        # Calculate the weighted sum of activations\n",
    "        weighted_sum = np.dot(activations, self.weights)\n",
    "        \n",
    "        # Apply a threshold for binary output\n",
    "        output = 1 if weighted_sum > 0.5 else 0\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ad0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0.5 0.8]\n",
      "Output: 1\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an RBF network with 2 input cells and 3 centers\n",
    "    rbf_network = RadialBasisFunction(input_size=2, num_centers=3)\n",
    "    \n",
    "    # Example input\n",
    "    input_data = np.array([0.5, 0.8])\n",
    "    \n",
    "    # Get the output from the RBF network\n",
    "    output = rbf_network.forward(input_data)\n",
    "    \n",
    "    print(f\"Input: {input_data}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc75ae",
   "metadata": {},
   "source": [
    "## Deep Learning Essentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b152802",
   "metadata": {},
   "source": [
    "### Deep Feed Forward (DFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd969e3e",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Deep Feedforward Neural Networks, often simply referred to as Deep Neural Networks (DNNs), are the quintessential deep learning models. These networks extend the concept of the basic feedforward neural network by introducing multiple hidden layers between the input and output layers. This architecture enables the learning of complex patterns and hierarchies in data, making DNNs incredibly effective for a wide range of predictive modeling tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Deep Feedforward Neural Networks are designed to handle:\n",
    "- Numerical data\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "\n",
    "Their flexibility and capacity for high-dimensional data processing make them applicable across nearly all domains of machine learning and artificial intelligence.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "DNNs are particularly proficient in:\n",
    "- Classification\n",
    "- Regression\n",
    "- Pattern recognition\n",
    "- Feature extraction\n",
    "\n",
    "The depth of these networks allows for the modeling of complex relationships in the data, contributing to advancements in areas like computer vision, natural language processing, and more.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of Deep Feedforward Neural Networks is a hallmark of their design. With the ability to adjust the number of layers and nodes within those layers, DNNs can be tailored to the complexity of the task at hand. However, this scalability comes with increased computational demands, necessitating efficient training techniques and hardware acceleration in many cases.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "DNNs exhibit a notable degree of robustness to noise and variability in the input data, thanks to their layered structure and the non-linear transformations applied at each layer. This makes them well-suited for real-world applications where data imperfections are common.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Deep Feedforward Neural Networks can be customized with various activation functions, optimization algorithms, and regularization techniques to improve their performance and generalization ability. Common variants include:\n",
    "- **ReLU-activated DNNs:** Use the Rectified Linear Unit function to introduce non-linearity without the vanishing gradient problem.\n",
    "- **Dropout-regularized DNNs:** Implement dropout layers to reduce overfitting by randomly omitting subsets of features during training.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Deep Feedforward Neural Networks:**\n",
    "- In tasks requiring the modeling of complex relationships or patterns in the data.\n",
    "- When the dataset is large and high-dimensional, providing enough information to train deep models effectively.\n",
    "\n",
    "**Considerations:**\n",
    "- The depth and complexity of DNNs necessitate careful design and training to avoid issues like overfitting and ensure sufficient generalization to new data.\n",
    "- Training deep models can be computationally intensive and time-consuming, requiring appropriate hardware and optimization strategies.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Deep Feedforward Neural Networks are a foundational pillar of modern deep learning, offering unparalleled flexibility and learning capacity. Their ability to learn from and make predictions on complex data has revolutionized many fields of study and industry. By leveraging advanced training techniques and computational resources, practitioners can unlock the full potential of DNNs to solve a vast array of challenging problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90960c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Feed Forward Neural Network\n",
    "class DeepFeedforwardNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DeepFeedforwardNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(2)  # Two hidden layers with 4 nodes each\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d138c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFeedforwardNN(\n",
      "  (input_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=4, out_features=4, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "Output: tensor([[0.4298, 0.3762]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the deep feedforward neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 4  # Number of nodes in each hidden layer\n",
    "output_size = 2  # Number of output nodes\n",
    "deep_feedforward_model = DeepFeedforwardNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.tensor([[0.5, 0.3, 0.8]])  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_deep_feedforward = deep_feedforward_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(deep_feedforward_model)\n",
    "print(\"Output:\", output_deep_feedforward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a754a03",
   "metadata": {},
   "source": [
    "# Deep Learning Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0641e",
   "metadata": {},
   "source": [
    "## Core Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918e8c4",
   "metadata": {},
   "source": [
    "### Autoencoder (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59efdf62",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Autoencoders (AEs) are a type of neural network used for unsupervised learning of efficient data codings. The primary goal of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise.” This is achieved by designing the autoencoder to encode inputs into a low-dimensional space and then decode these encodings back into the original input data.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Autoencoders can handle a variety of data types, including:\n",
    "- Numerical data\n",
    "- Images\n",
    "- Audio signals\n",
    "- Text data\n",
    "\n",
    "Their versatility makes them suitable for applications ranging from compression to noise reduction or feature extraction.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "The main applications of AEs include:\n",
    "- Dimensionality reduction\n",
    "- Feature learning\n",
    "- Data compression\n",
    "- Denoising\n",
    "\n",
    "AEs are particularly useful in scenarios where the intrinsic structure of the data needs to be learned without labeled data.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Autoencoders scale well with the complexity of the data and the desired level of compression or feature extraction. The network architecture can be adjusted according to the specific requirements of the task, allowing for flexible implementations that cater to large and high-dimensional datasets.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Autoencoders, especially denoising autoencoders (DAEs), are designed to be robust to noise in the input data. By learning to reconstruct inputs from corrupted versions, they can effectively identify and ignore irrelevant or noisy features in the data.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of autoencoders have been developed to address different challenges, including:\n",
    "- **Variational Autoencoders (VAEs):** Focus on generating new data that is similar to the training data.\n",
    "- **Denoising Autoencoders (DAEs):** Aim to remove noise from corrupted input data.\n",
    "- **Sparse Autoencoders (SAEs):** Introduce sparsity in the encoded representations to improve feature selection.\n",
    "\n",
    "Note: These three variants each have their own sections directly following this.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Autoencoders:**\n",
    "- For tasks requiring data compression without significant loss of information.\n",
    "- When looking to learn efficient representations of data without supervision.\n",
    "- In applications where the removal of noise or the extraction of relevant features from the data is essential.\n",
    "\n",
    "**Considerations:**\n",
    "- The choice of autoencoder variant and network architecture should be aligned with the specific objectives of the task.\n",
    "- Careful tuning of the network parameters is crucial to achieve the desired balance between compression, reconstruction accuracy, and feature learning.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Autoencoders offer a powerful framework for learning efficient representations of data in an unsupervised manner. By leveraging their ability to compress and denoise data, as well as to learn salient features, autoencoders are instrumental in various applications across machine learning and signal processing domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5f457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61ad5262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (decoder): Linear(in_features=5, out_features=10, bias=True)\n",
      ")\n",
      "Input: tensor([[0.7870, 0.7469, 0.8532, 0.2466, 0.9331, 0.9623, 0.0533, 0.2371, 0.3791,\n",
      "         0.9191]])\n",
      "Output: tensor([[0.5045, 0.4231, 0.4493, 0.4943, 0.4588, 0.6211, 0.4717, 0.3584, 0.3688,\n",
      "         0.6155]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the autoencoder\n",
    "input_size = 10  # Number of input features\n",
    "hidden_size = 5  # Number of hidden nodes (compressed representation)\n",
    "autoencoder = Autoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))\n",
    "\n",
    "# Forward pass to get the reconstructed output\n",
    "output_autoencoder = autoencoder(sample_input)\n",
    "\n",
    "# Print the model architecture, input, and output\n",
    "print(autoencoder)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output:\", output_autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa7a13",
   "metadata": {},
   "source": [
    "#### VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d1ef9",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Variational Autoencoders (VAEs) are a cornerstone in the field of generative AI, representing a powerful class of deep learning models for generative modeling. They are designed to learn the underlying probability distribution of training data, enabling the generation of new data points with similar properties. VAEs combine traditional autoencoder architecture with variational inference principles, allowing them to compress data into a latent space and then generate data by sampling from this space, thereby facilitating a deep exploration of the continuous latent space representing the data.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "VAEs demonstrate remarkable adaptability across a range of data types, including:\n",
    "- Images\n",
    "- Text\n",
    "- Audio\n",
    "- Continuous numerical data\n",
    "\n",
    "This versatility underscores their prominence in generative AI, making them a popular choice for a wide array of generative tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Emphasizing their role in generative AI, VAEs excel in:\n",
    "- Data generation\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Anomaly detection\n",
    "\n",
    "Their deep learning capabilities enable them not only to model complex distributions but also to generate new, coherent samples, showcasing the transformative potential of generative AI.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "With their deep neural network architecture, VAEs scale effectively to accommodate the complexity and volume of vast datasets, further solidifying their status in generative AI for handling high-dimensional data efficiently.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "VAEs' proficiency in denoising and reconstructing inputs highlights their robustness, making them invaluable for applications in generative AI where data cleanliness cannot be assured.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Reflecting the innovation in generative AI, various VAE models have been developed to address specific challenges or improve upon the original framework, including Conditional VAEs, Beta-VAEs, and Disentangled VAEs, each offering unique advantages for controlled data generation and enhanced interpretability of latent representations.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "In the realm of generative AI, VAEs are particularly suited for:\n",
    "- Generating new data that mimics the properties of specific datasets.\n",
    "- Unsupervised learning of complex data distributions.\n",
    "- Applications requiring a nuanced understanding of data's underlying structure.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "Training VAEs can present challenges, such as mode collapse, underscoring the need for expertise in generative AI to navigate these complexities successfully.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Variational Autoencoders (VAEs) have cemented their place as a fundamental technology in generative AI, offering a sophisticated mechanism for understanding and generating data. Their broad applicability and the depth of insight they provide into data's inherent structure make them a pivotal tool in the advancement of generative modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d4a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.encoder_fc2_mean = nn.Linear(hidden_size, hidden_size)\n",
    "        self.encoder_fc2_logvar = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "        self.decoder_fc2 = nn.Linear(input_size, input_size)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = torch.relu(self.encoder_fc1(x))\n",
    "        mean = self.encoder_fc2_mean(x)\n",
    "        logvar = self.encoder_fc2_logvar(x)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "\n",
    "        # Decoder\n",
    "        x_hat = torch.relu(self.decoder_fc1(z))\n",
    "        x_hat = torch.sigmoid(self.decoder_fc2(x_hat))\n",
    "\n",
    "        return x_hat, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb16f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (encoder_fc2_mean): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (encoder_fc2_logvar): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc2): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Input: tensor([[0.0343, 0.9712, 0.4583, 0.0153]])\n",
      "Output: tensor([[0.4484, 0.5287, 0.4188, 0.4760]], grad_fn=<SigmoidBackward0>)\n",
      "Mean: tensor([[-0.1030, -0.2888, -0.1711,  0.0543]], grad_fn=<AddmmBackward0>)\n",
      "Log Variance: tensor([[-0.0906, -0.0857,  0.0126,  0.3173]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the variational autoencoder\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 4  # Number of hidden nodes in probabilistic layer\n",
    "vae = VariationalAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))\n",
    "\n",
    "# Forward pass to get the reconstructed output and latent variables\n",
    "output_vae, mean, logvar = vae(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(vae)\n",
    "\n",
    "print(\"\\nInput:\", sample_input)\n",
    "print(\"Output:\", output_vae)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Log Variance:\", logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be943473",
   "metadata": {},
   "source": [
    "#### DAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcfc27f",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Denoising Autoencoders (DAEs) are an advanced type of autoencoder designed to *remove noise from input data*. By intentionally corrupting the input data and then learning to reconstruct the original, uncorrupted data, DAEs are trained to capture the most relevant features. This process enhances the model's ability to generalize from the data, making it highly effective for tasks that require robust feature extraction and data denoising capabilities.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Denoising Autoencoders are capable of processing various data types, including:\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "- Continuous numerical data\n",
    "\n",
    "Their adaptability makes them particularly useful for applications involving noisy or incomplete data.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Denoising Autoencoders are primarily used for:\n",
    "- Data denoising\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Data generation and enhancement\n",
    "\n",
    "By learning to ignore the \"noise\" in data, DAEs excel in recovering clean representations from corrupted inputs.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Similar to other autoencoders, the scalability of DAEs depends on the network architecture. Modern techniques and computational resources allow DAEs to handle large datasets and complex noise patterns effectively, showcasing their scalability in practical applications.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "The core strength of DAEs lies in their robustness to noise. They are specifically trained to identify and ignore irrelevant features (noise), focusing on reconstructing the essential aspects of the data, which makes them exceptionally reliable for denoising tasks.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of DAEs have been developed to address different types of noise or to enhance specific aspects of denoising, including:\n",
    "- **Gaussian Noise DAEs:** Target Gaussian noise in the data.\n",
    "- **Salt-and-Pepper Noise DAEs:** Designed to remove binary noise from images.\n",
    "- **Variational DAEs:** Combine denoising capabilities with variational autoencoder frameworks for improved generative properties.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Denoising Autoencoders:**\n",
    "- For cleaning noisy data before further processing or analysis.\n",
    "- In feature extraction tasks where maintaining data integrity is crucial.\n",
    "- As a preprocessing step to improve the performance of subsequent machine learning models.\n",
    "\n",
    "**Considerations:**\n",
    "- The effectiveness of a DAE can vary based on the noise type and level; selecting the appropriate model variant is key.\n",
    "- Training DAEs requires a balance between denoising capability and preserving relevant features, necessitating careful tuning of model parameters.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Denoising Autoencoders offer a powerful solution for improving data quality, with their unique training strategy enabling them to extract clean, relevant features from noisy inputs. Their versatility across different data types and robustness to various noise patterns make them an invaluable tool in the data preprocessing pipeline, enhancing the performance of machine learning and deep learning models across a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d42b753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = torch.relu(self.encoder_fc1(x))\n",
    "\n",
    "        # Decoder\n",
    "        x_hat = torch.sigmoid(self.decoder_fc1(x))\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68ddb586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenoisingAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Noisy Input: tensor([[0.8623, 0.3365, 0.8590, 0.2574]])\n",
      "Reconstructed Output: tensor([[0.3802, 0.5042, 0.6004, 0.3728]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the denoising autoencoder\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 4  # Number of hidden nodes\n",
    "dae = DenoisingAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input (noisy data)\n",
    "noisy_input = torch.rand((1, input_size))  # Example Noisy Data\n",
    "\n",
    "# Forward pass to get the reconstructed output\n",
    "output_dae = dae(noisy_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(dae)\n",
    "print(\"\\nNoisy Input:\", noisy_input)\n",
    "print(\"Reconstructed Output:\", output_dae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c614633",
   "metadata": {},
   "source": [
    "#### SAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdf86b",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Sparse Autoencoders represent a specialized variant of autoencoders, aimed at *unsupervised learning of compressed representations*. By introducing sparsity constraints, they enforce most neurons to be inactive, enhancing feature detection and data representation efficiency. This approach improves generalization, making them suitable for tasks requiring robust feature extraction.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Sparse Autoencoders efficiently process:\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "- Continuous numerical data\n",
    "\n",
    "Their versatility across different data types highlights their utility in feature extraction and data compression tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Key applications include:\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Data denoising\n",
    "- Pretraining for deeper neural networks\n",
    "\n",
    "Sparsity constraints enable these models to learn higher-level features, distinguishing them from traditional autoencoders.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Despite sparsity aiding in learning efficient representations, the network's size and depth impact its ability to model complex distributions and computational requirements.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "They demonstrate significant robustness to noise, attributed to their focus on essential features, making them ideal for denoising and robust representation learning.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Variants are based on the sparsity enforcement method:\n",
    "- **KL Divergence Sparse Autoencoder:** Penalizes deviations from a target sparsity level using Kullback-Leibler divergence.\n",
    "- **L1 Regularization Sparse Autoencoder:** Applies L1 penalty on hidden units' activations to encourage sparsity.\n",
    "- **Winner-Take-All (WTA) Sparse Autoencoder:** Only a fraction of the most active hidden units are allowed to update their weights, enhancing sparsity.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Sparse Autoencoders:**\n",
    "- In extracting meaningful features from high-dimensional data.\n",
    "- For dimensionality reduction with interpretability.\n",
    "- During pretraining phases for deep learning models, providing a good initial weight set that captures useful data patterns.\n",
    "\n",
    "**Considerations:**\n",
    "- Selecting the appropriate sparsity constraint and regularization technique is critical for balancing feature selectivity and model complexity.\n",
    "- Hyperparameters require careful tuning to achieve desired sparsity levels and optimal performance.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Sparse Autoencoders stand out for learning efficient and interpretable data representations, with enforced sparsity offering clear advantages in feature selection and model robustness. They are invaluable in preprocessing, feature extraction, and as a pretraining step, enhancing subsequent models' performance across various data types and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8eb9d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, sparsity_target=0.1, sparsity_weight=0.2):\n",
    "        super(SparseAutoencoder, self).__init__(input_size, hidden_size, output_size=input_size)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        # Sparsity parameters\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoded = self.encoder_fc1(x)\n",
    "        encoded = self.relu(encoded)\n",
    "\n",
    "        # Decoder\n",
    "        decoded = torch.sigmoid(self.decoder_fc1(encoded))\n",
    "\n",
    "        return decoded, encoded\n",
    "\n",
    "    def loss_function(self, x, x_hat, encoded):\n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='mean')\n",
    "\n",
    "        # Sparsity loss\n",
    "        sparsity_loss = torch.sum(self.kl_divergence(self.sparsity_target, encoded))\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + self.sparsity_weight * sparsity_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def kl_divergence(self, target, activations):\n",
    "        # KL Divergence to enforce sparsity\n",
    "        p = torch.mean(activations, dim=0)  # Average activation over the dataset\n",
    "        return target * torch.log(target / p) + (1 - target) * torch.log((1 - target) / (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b99e817f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Input: tensor([[0.1020, 0.0088, 0.7749, 0.5999, 0.3943]])\n",
      "Reconstructed Output: tensor([[0.5544, 0.6054, 0.4550, 0.4770, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "Encoded Representation: tensor([[0.0000, 0.0000, 0.3422]], grad_fn=<ReluBackward0>)\n",
      "Loss: inf\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the sparse autoencoder\n",
    "input_size = 5  # Number of input features\n",
    "hidden_size = 3  # Number of hidden nodes\n",
    "sae = SparseAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the reconstructed output and encoded representation\n",
    "output_sae, encoded_sae = sae(sample_input)\n",
    "\n",
    "# Calculate the loss\n",
    "loss_sae = sae.loss_function(sample_input, output_sae, encoded_sae)\n",
    "\n",
    "# Print the model architecture, output, and loss\n",
    "print(sae)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Reconstructed Output:\", output_sae)\n",
    "print(\"Encoded Representation:\", encoded_sae)\n",
    "print(\"Loss:\", loss_sae.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc1a1e",
   "metadata": {},
   "source": [
    "### Deep Convolutional Network (DCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32ef74",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Deep Convolutional Networks (DCNs), also known as Convolutional Neural Networks (CNNs), are a specialized kind of neural network for processing data that has a known grid-like topology. \n",
    "\n",
    "Examples include time-series data, which can be thought of as a 1D grid taking samples at regular time intervals, and image data, which can be seen as a 2D grid of pixels.\n",
    "\n",
    "DCNs have been instrumental in many advances in computer vision, achieving remarkable success in tasks such as image recognition, object detection, and more.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Deep Convolutional Networks are particularly adept at handling:\n",
    "- Image data\n",
    "- Video sequences\n",
    "- Time-series data\n",
    "- Any data that can be represented in a grid-like structure (e.g., sound waves visualized in spectrograms)\n",
    "\n",
    "Their ability to automatically and adaptively learn spatial hierarchies of features makes them highly effective for tasks involving visual inputs.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "DCNs excel in a variety of tasks, including but not limited to:\n",
    "- Image and video recognition\n",
    "- Image classification\n",
    "- Object detection\n",
    "- Semantic segmentation\n",
    "- Natural language processing (when applied to text data in a convolutional manner)\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "One of the key strengths of DCNs is their scalability, not only in terms of handling large volumes of data but also in terms of their capacity to learn from complex and high-dimensional datasets. Their architecture, characterized by layers with convolutions, pooling, and often followed by fully connected layers, allows for efficient training and inference.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "DCNs are known for their robustness to variations and noise in the input data, making them particularly suitable for real-world applications where data imperfection is common. This robustness stems from their ability to learn invariant features that are critical for recognition tasks.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "There are several popular variants and architectures of DCNs, including:\n",
    "- **LeNet:** One of the first convolutional networks that demonstrated the effectiveness of convolutional layers.\n",
    "- **AlexNet:** The network that revitalized interest in convolutional neural networks with its success in the ImageNet challenge.\n",
    "- **VGGNet:** Known for its simplicity and deep architecture.\n",
    "- **ResNet:** Introduced residual connections to enable the training of very deep networks.\n",
    "- **Inception (GoogleNet):** Known for its efficiency and depth with a lower number of parameters.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Deep Convolutional Networks:**\n",
    "- In applications involving image or video processing where capturing spatial hierarchies is crucial.\n",
    "- For tasks requiring the extraction of complex features from large and high-dimensional datasets.\n",
    "\n",
    "**Considerations:**\n",
    "- The design of the network architecture and the choice of hyperparameters are critical for achieving optimal performance.\n",
    "- Training deep convolutional networks requires substantial computational resources, particularly for large datasets and complex models.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Deep Convolutional Networks have revolutionized the field of computer vision and beyond, demonstrating unparalleled success in a wide range of applications involving visual data processing. Their ability to learn powerful representations of data makes them a cornerstone of modern deep learning, with ongoing research pushing the boundaries of what is possible in artificial intelligence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f281322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(BaseNN):\n",
    "    def __init__(self, input_channels, num_classes, image_size):\n",
    "        hidden_size = 64\n",
    "\n",
    "        super(DeepCNN, self).__init__(input_size=image_size, hidden_size=hidden_size, output_size=num_classes)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * (image_size // 4) * (image_size // 4), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bd5c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Input: tensor([[[[4.5906e-01, 9.7879e-01, 1.5767e-01, 1.5335e-01, 5.5557e-01,\n",
      "           8.6285e-01, 6.4839e-01, 5.1157e-01, 3.6890e-01, 6.9478e-01,\n",
      "           6.8469e-01, 7.8236e-01, 8.4249e-02, 2.5696e-01, 6.6906e-01,\n",
      "           7.7359e-01, 3.7520e-01, 4.7202e-01, 6.3767e-01, 5.4821e-01,\n",
      "           6.2599e-01, 5.4250e-02, 7.0396e-01, 6.0149e-01, 5.8098e-01,\n",
      "           7.4248e-01, 5.5248e-01, 6.0071e-01],\n",
      "          [2.9531e-01, 5.5693e-01, 4.0573e-01, 3.6962e-01, 5.4100e-02,\n",
      "           4.2071e-01, 2.9789e-01, 4.8125e-01, 8.6545e-01, 5.7631e-01,\n",
      "           6.8455e-01, 1.8524e-01, 7.4843e-01, 4.5054e-01, 7.6650e-01,\n",
      "           8.0952e-01, 7.0001e-01, 5.6114e-02, 9.9282e-01, 7.2805e-01,\n",
      "           2.3671e-01, 4.8110e-01, 6.6983e-01, 8.3002e-01, 7.4044e-01,\n",
      "           7.7290e-02, 3.9747e-01, 1.2017e-01],\n",
      "          [8.7467e-01, 5.8379e-01, 2.4939e-01, 2.0134e-01, 7.5501e-01,\n",
      "           8.5101e-01, 4.8388e-01, 5.7895e-01, 9.6345e-01, 8.5153e-02,\n",
      "           8.3282e-01, 3.2452e-01, 5.2271e-01, 7.6322e-01, 7.0711e-01,\n",
      "           7.7016e-01, 6.9141e-01, 9.1830e-02, 5.8071e-01, 4.2504e-01,\n",
      "           6.3326e-01, 6.4161e-01, 7.4243e-01, 7.4435e-01, 3.2136e-01,\n",
      "           9.0227e-01, 7.0739e-01, 6.0522e-01],\n",
      "          [3.1884e-01, 8.9216e-01, 8.0807e-01, 2.9350e-03, 5.1653e-01,\n",
      "           9.8546e-01, 3.5822e-01, 7.9644e-01, 1.5399e-02, 1.3545e-01,\n",
      "           1.3088e-01, 8.8358e-01, 3.2624e-01, 6.0159e-01, 8.3363e-01,\n",
      "           2.7636e-01, 3.5192e-01, 8.2163e-01, 5.3631e-01, 9.0192e-01,\n",
      "           2.9833e-01, 6.5647e-01, 5.1695e-01, 3.8845e-01, 8.7145e-01,\n",
      "           9.5754e-01, 5.1755e-02, 6.6059e-01],\n",
      "          [9.4189e-01, 4.6391e-01, 2.3890e-01, 6.1557e-01, 9.6924e-01,\n",
      "           1.4621e-01, 9.0078e-01, 8.5228e-01, 3.9545e-01, 6.4340e-01,\n",
      "           3.2954e-01, 5.6799e-01, 3.6632e-01, 9.6114e-01, 5.6839e-01,\n",
      "           3.2048e-01, 8.3476e-01, 6.4078e-01, 2.5140e-01, 1.1503e-01,\n",
      "           6.3509e-01, 9.4857e-01, 4.7954e-01, 6.9801e-01, 9.3752e-01,\n",
      "           3.2977e-01, 7.1464e-01, 4.8030e-01],\n",
      "          [9.7769e-01, 6.5219e-01, 1.0124e-01, 9.8108e-01, 6.0117e-01,\n",
      "           9.1618e-01, 4.2572e-01, 3.3811e-01, 3.4846e-01, 4.7007e-01,\n",
      "           7.8545e-01, 1.8511e-01, 1.8718e-01, 7.8536e-01, 2.9525e-01,\n",
      "           5.2448e-01, 6.7441e-01, 7.3522e-01, 7.8491e-01, 1.9677e-02,\n",
      "           5.9927e-01, 2.3790e-01, 7.9675e-01, 2.0576e-01, 1.4942e-01,\n",
      "           8.8884e-01, 4.5726e-01, 7.7887e-01],\n",
      "          [5.5540e-01, 7.5237e-01, 9.0187e-01, 8.9003e-01, 9.1873e-01,\n",
      "           6.4527e-01, 8.6886e-01, 3.7132e-01, 8.4422e-01, 6.6149e-01,\n",
      "           4.9614e-01, 5.7405e-01, 6.5206e-01, 2.0766e-01, 8.6446e-01,\n",
      "           6.1741e-01, 3.5260e-01, 1.6115e-01, 9.5998e-01, 1.0598e-01,\n",
      "           2.9205e-01, 8.1288e-01, 1.7443e-01, 5.8392e-01, 6.4841e-01,\n",
      "           2.7332e-01, 1.3713e-01, 9.8325e-01],\n",
      "          [6.6296e-01, 8.0154e-01, 8.0351e-01, 1.6876e-01, 9.1599e-01,\n",
      "           8.1113e-02, 9.1949e-01, 8.3071e-01, 8.5079e-01, 7.7500e-01,\n",
      "           2.1325e-01, 2.3798e-01, 5.1660e-01, 8.8025e-01, 2.8207e-02,\n",
      "           3.6061e-01, 1.2748e-01, 8.8137e-01, 4.0251e-01, 9.3202e-02,\n",
      "           9.1158e-01, 6.4100e-01, 3.1717e-01, 6.1095e-01, 5.7929e-01,\n",
      "           1.9829e-01, 2.7635e-02, 4.1177e-01],\n",
      "          [8.9083e-02, 4.0045e-02, 2.6691e-01, 1.9030e-01, 7.8508e-01,\n",
      "           1.3293e-01, 2.5324e-01, 5.9898e-01, 7.3630e-01, 5.0586e-01,\n",
      "           7.8900e-01, 4.2484e-01, 4.6700e-01, 8.5324e-01, 6.9024e-01,\n",
      "           4.0748e-01, 6.9003e-01, 3.9090e-01, 8.4180e-01, 7.9911e-01,\n",
      "           4.7117e-01, 5.3657e-01, 8.1996e-01, 9.1126e-02, 7.9930e-01,\n",
      "           7.3005e-01, 7.8478e-01, 1.8258e-01],\n",
      "          [3.2294e-01, 6.8406e-01, 7.3794e-01, 2.2648e-01, 7.4880e-01,\n",
      "           3.8823e-01, 8.8051e-01, 1.3089e-01, 3.6101e-01, 4.4684e-01,\n",
      "           8.1875e-03, 1.4502e-01, 2.1431e-02, 5.9833e-01, 7.3124e-01,\n",
      "           9.7444e-01, 7.5478e-02, 5.4561e-01, 4.7714e-01, 5.9634e-01,\n",
      "           8.7033e-01, 8.2452e-01, 4.7887e-01, 1.8602e-02, 3.7340e-01,\n",
      "           1.4741e-01, 3.3589e-01, 3.4906e-01],\n",
      "          [6.9256e-01, 5.6371e-01, 4.8301e-01, 1.2542e-01, 5.5749e-01,\n",
      "           6.3873e-01, 4.0999e-01, 4.2231e-01, 2.6336e-01, 2.0499e-01,\n",
      "           4.9445e-01, 4.5981e-01, 1.5241e-02, 8.3015e-01, 9.4995e-01,\n",
      "           5.3706e-01, 7.2579e-01, 5.3145e-01, 6.6630e-01, 9.8847e-01,\n",
      "           8.4998e-01, 1.0486e-01, 4.1989e-01, 1.4968e-01, 5.2105e-01,\n",
      "           6.8471e-01, 8.2060e-01, 6.5785e-02],\n",
      "          [4.2106e-01, 1.4081e-02, 7.6696e-01, 1.3538e-01, 3.3319e-01,\n",
      "           7.3197e-02, 7.5796e-01, 7.3828e-01, 5.0540e-01, 5.9706e-01,\n",
      "           9.3328e-01, 8.3383e-01, 3.8012e-01, 7.5728e-01, 7.7730e-01,\n",
      "           8.0598e-01, 8.8933e-01, 9.9816e-01, 1.1764e-01, 1.6008e-01,\n",
      "           8.7608e-01, 9.2529e-01, 8.7812e-01, 1.7060e-01, 6.6651e-01,\n",
      "           1.6112e-01, 8.6722e-01, 3.5886e-01],\n",
      "          [4.4737e-01, 5.3469e-01, 8.4063e-01, 9.0228e-01, 8.9556e-01,\n",
      "           8.3846e-01, 2.1807e-01, 2.3119e-01, 8.7832e-01, 7.0198e-02,\n",
      "           4.0152e-02, 1.6597e-01, 1.6077e-01, 8.4440e-01, 6.5203e-01,\n",
      "           9.3929e-01, 6.0825e-01, 2.6247e-01, 4.7025e-02, 6.7185e-01,\n",
      "           9.9138e-01, 4.2210e-02, 5.2676e-01, 9.7960e-01, 3.0670e-01,\n",
      "           5.8158e-01, 1.6832e-01, 8.0040e-01],\n",
      "          [2.4274e-01, 4.9561e-01, 7.6314e-01, 3.6164e-01, 4.7535e-01,\n",
      "           3.5047e-01, 9.3739e-01, 7.2733e-01, 1.3412e-01, 9.2636e-01,\n",
      "           5.0602e-01, 9.3858e-01, 6.6445e-01, 1.1649e-01, 4.9571e-01,\n",
      "           1.8176e-01, 3.6385e-01, 9.8267e-01, 4.0908e-01, 3.7255e-01,\n",
      "           2.8752e-01, 4.5800e-01, 7.6156e-01, 3.2082e-01, 2.3845e-01,\n",
      "           5.8718e-01, 3.1035e-01, 8.8741e-01],\n",
      "          [4.6171e-01, 2.9621e-01, 8.5851e-01, 7.2874e-02, 6.6822e-01,\n",
      "           7.5645e-01, 7.2479e-04, 4.9645e-01, 4.2992e-01, 1.1101e-01,\n",
      "           1.8914e-01, 6.7735e-01, 7.0713e-02, 7.0407e-01, 5.0639e-01,\n",
      "           8.5345e-01, 6.9867e-01, 1.5208e-01, 1.7596e-01, 5.9074e-01,\n",
      "           6.4318e-01, 8.6784e-01, 9.8230e-01, 1.0074e-01, 8.5196e-01,\n",
      "           7.6066e-01, 4.3410e-01, 4.1655e-03],\n",
      "          [6.5251e-01, 7.5114e-02, 5.4163e-01, 5.4311e-01, 3.4904e-01,\n",
      "           8.6691e-02, 4.8315e-01, 7.8606e-01, 1.3338e-01, 4.7594e-01,\n",
      "           6.6571e-01, 6.8506e-01, 3.3491e-01, 1.1598e-01, 8.2234e-01,\n",
      "           5.2053e-01, 2.1451e-01, 6.1868e-01, 5.6461e-01, 9.1013e-01,\n",
      "           2.9112e-01, 3.7494e-01, 7.7305e-01, 4.2938e-01, 4.0547e-01,\n",
      "           7.0006e-01, 2.3957e-01, 9.8156e-01],\n",
      "          [5.2593e-01, 9.1342e-01, 5.6630e-01, 8.1563e-01, 3.5425e-01,\n",
      "           4.2928e-01, 1.9741e-01, 2.9213e-01, 5.4466e-01, 5.2130e-01,\n",
      "           9.1833e-01, 1.0388e-01, 5.1942e-01, 3.9500e-01, 3.4935e-01,\n",
      "           9.5579e-01, 9.9096e-01, 4.1917e-01, 2.3161e-01, 1.1229e-01,\n",
      "           1.8187e-01, 4.4344e-01, 9.5218e-01, 9.7128e-01, 1.4197e-01,\n",
      "           4.7706e-01, 4.9403e-01, 6.5694e-01],\n",
      "          [2.4129e-01, 4.0292e-02, 6.4574e-01, 6.5254e-01, 6.9893e-01,\n",
      "           5.8096e-02, 5.7828e-02, 8.5984e-01, 7.7101e-01, 9.5480e-01,\n",
      "           8.1311e-01, 9.6261e-01, 1.5567e-01, 3.7334e-01, 7.1536e-01,\n",
      "           3.9450e-01, 7.2890e-01, 1.0575e-01, 9.1549e-01, 7.3774e-02,\n",
      "           3.5678e-03, 4.9924e-01, 3.2212e-01, 1.0569e-01, 1.7865e-02,\n",
      "           8.2837e-01, 1.4567e-01, 4.8658e-01],\n",
      "          [5.8813e-01, 5.0476e-01, 9.0001e-01, 4.4289e-01, 8.7400e-01,\n",
      "           9.9876e-02, 3.0361e-01, 2.6704e-01, 4.6356e-01, 5.2214e-01,\n",
      "           3.9632e-01, 6.9279e-01, 1.7958e-02, 9.5258e-01, 6.1467e-01,\n",
      "           4.3992e-01, 1.1981e-01, 9.5103e-01, 9.8650e-01, 9.2565e-01,\n",
      "           5.0931e-01, 7.3818e-01, 1.4369e-01, 8.8156e-01, 1.7598e-01,\n",
      "           2.1282e-01, 6.0027e-01, 6.8249e-01],\n",
      "          [3.4733e-01, 1.6958e-01, 5.9329e-02, 4.2942e-01, 3.0212e-01,\n",
      "           6.1458e-01, 8.2769e-01, 2.3660e-01, 9.4163e-01, 2.8140e-01,\n",
      "           8.6995e-01, 1.4376e-01, 9.4454e-01, 1.4071e-01, 4.9647e-01,\n",
      "           3.1945e-01, 1.0749e-01, 7.4993e-01, 8.0474e-01, 5.8025e-01,\n",
      "           7.0463e-01, 1.4820e-01, 4.4770e-01, 4.9202e-01, 9.6243e-01,\n",
      "           5.2053e-01, 1.9030e-01, 3.6316e-01],\n",
      "          [8.5441e-01, 6.6720e-01, 2.3364e-01, 4.6865e-01, 6.3932e-01,\n",
      "           7.8292e-01, 4.4171e-01, 9.9985e-01, 7.6040e-01, 9.5671e-01,\n",
      "           2.9049e-01, 6.0692e-02, 9.9482e-01, 8.5951e-01, 3.9319e-02,\n",
      "           1.5196e-01, 9.4999e-01, 2.0113e-01, 8.4764e-01, 7.0978e-01,\n",
      "           7.8372e-01, 8.5126e-01, 8.1378e-01, 5.2532e-01, 1.0751e-01,\n",
      "           9.3857e-01, 3.9731e-01, 5.3313e-01],\n",
      "          [6.8698e-01, 2.4665e-01, 9.3015e-01, 8.5736e-01, 3.7887e-01,\n",
      "           3.0078e-01, 2.0841e-01, 8.7869e-01, 1.4534e-01, 7.4422e-01,\n",
      "           8.9816e-01, 6.8878e-01, 7.3178e-01, 4.9205e-01, 8.2004e-01,\n",
      "           9.6573e-02, 8.4498e-01, 2.0845e-01, 7.1172e-01, 4.2436e-01,\n",
      "           9.6422e-01, 2.2026e-01, 4.7899e-02, 9.2965e-01, 3.3040e-01,\n",
      "           6.5012e-01, 4.9740e-01, 8.7559e-01],\n",
      "          [5.7881e-01, 4.5508e-01, 3.6954e-01, 7.2963e-01, 5.5028e-01,\n",
      "           9.0184e-02, 9.2786e-01, 6.2106e-01, 3.2892e-01, 4.8784e-02,\n",
      "           8.2113e-01, 8.7069e-02, 7.0564e-01, 8.4570e-01, 4.9542e-01,\n",
      "           8.1267e-01, 4.1063e-02, 6.7903e-01, 5.4641e-01, 3.6126e-01,\n",
      "           7.2315e-01, 4.4897e-01, 5.8679e-01, 7.0274e-01, 3.1751e-01,\n",
      "           1.8561e-01, 4.6545e-01, 8.8632e-01],\n",
      "          [2.1365e-01, 1.8774e-01, 6.4135e-01, 5.7872e-02, 4.2343e-01,\n",
      "           6.4080e-02, 8.9446e-01, 5.5880e-01, 4.2801e-01, 2.0671e-01,\n",
      "           9.6503e-01, 3.8500e-01, 8.8036e-02, 4.0558e-01, 5.5752e-01,\n",
      "           7.9479e-01, 2.4561e-01, 9.5558e-01, 3.2423e-02, 4.2280e-01,\n",
      "           5.6126e-01, 3.0718e-01, 2.6485e-01, 9.2720e-01, 2.0154e-01,\n",
      "           6.3067e-01, 5.7918e-01, 4.1636e-02],\n",
      "          [5.7855e-01, 9.3215e-01, 3.8814e-01, 8.8924e-02, 6.0779e-01,\n",
      "           4.2414e-01, 4.6013e-01, 5.9391e-01, 8.9551e-01, 1.4338e-01,\n",
      "           3.0162e-01, 3.9066e-01, 6.2349e-01, 8.9889e-01, 9.1762e-01,\n",
      "           4.8219e-01, 2.0201e-01, 1.5464e-01, 4.3595e-01, 9.0025e-01,\n",
      "           1.4157e-02, 6.9889e-01, 4.5724e-01, 3.4743e-01, 9.4537e-01,\n",
      "           6.2141e-02, 9.0569e-01, 9.1289e-01],\n",
      "          [7.8974e-02, 9.6500e-01, 8.9045e-01, 5.3548e-01, 9.5285e-01,\n",
      "           4.9535e-01, 4.8692e-01, 9.5317e-01, 3.0851e-01, 9.8934e-01,\n",
      "           8.6658e-01, 7.9056e-01, 7.4588e-01, 2.2103e-02, 3.5963e-01,\n",
      "           5.1716e-01, 1.1697e-01, 8.4006e-01, 8.8741e-01, 9.6838e-01,\n",
      "           5.9587e-01, 4.9465e-01, 1.8518e-01, 7.7034e-01, 3.4822e-01,\n",
      "           3.5039e-01, 5.8783e-01, 7.9859e-01],\n",
      "          [8.5189e-01, 8.9060e-01, 8.9009e-01, 5.5255e-01, 7.3701e-01,\n",
      "           8.8224e-01, 6.5004e-01, 6.5175e-01, 3.6240e-01, 5.4655e-01,\n",
      "           9.6517e-01, 9.9476e-01, 1.4348e-01, 6.8050e-01, 6.0567e-01,\n",
      "           1.7439e-01, 7.5314e-01, 5.8044e-01, 7.0583e-01, 7.1953e-01,\n",
      "           6.9857e-01, 7.4857e-01, 4.6166e-02, 5.8714e-01, 8.1154e-01,\n",
      "           3.2695e-01, 5.8285e-02, 4.4466e-02],\n",
      "          [1.5336e-01, 4.2963e-01, 9.1944e-01, 9.8721e-01, 9.6001e-01,\n",
      "           7.5117e-01, 2.6395e-01, 8.8170e-01, 6.5819e-01, 7.8747e-01,\n",
      "           8.5896e-01, 2.8815e-01, 4.9098e-01, 3.7374e-01, 9.4843e-01,\n",
      "           5.8553e-01, 7.6072e-03, 6.6259e-02, 6.9693e-01, 3.1684e-01,\n",
      "           1.7882e-01, 3.2735e-01, 6.7292e-01, 9.8966e-01, 3.7445e-01,\n",
      "           3.3926e-01, 1.1150e-01, 7.5416e-01]]]])\n",
      "Output Scores: [[-0.03371035 -0.08543233  0.07970547 -0.11424842  0.11230226  0.01566492\n",
      "   0.06782366 -0.17989124  0.0696204   0.0026818 ]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_channels = 1  # Grayscale images\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "\n",
    "deep_cnn_model = DeepCNN(input_channels, num_classes, image_size)\n",
    "\n",
    "sample_image = torch.rand((1, input_channels, image_size, image_size))\n",
    "\n",
    "output_scores = deep_cnn_model(sample_image)\n",
    "\n",
    "print(deep_cnn_model)\n",
    "print(\"Input:\", sample_image)\n",
    "print(\"Output Scores:\", output_scores.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59105c",
   "metadata": {},
   "source": [
    "# Recurrent and Memory Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f57745",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e62267",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks that excel at processing sequential data, making them particularly well-suited for tasks involving time-series data, natural language processing, and any scenario where the context or the order of elements is crucial.\n",
    "\n",
    "RNNs are characterized by their ability to maintain a 'memory' of previous inputs by incorporating loops within the network. This memory allows them to exhibit dynamic temporal behavior and understand sequences, distinguishing them from other neural network architectures that treat inputs independently.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "RNNs are adept at handling:\n",
    "\n",
    "- Sequential data\n",
    "- Time-series data\n",
    "- Text and spoken language\n",
    "- Any form of data where the sequence order is significant\n",
    "\n",
    "Their design enables them to process inputs of varying lengths, from short sentences to lengthy documents or extensive time-series datasets.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "RNNs are particularly useful for:\n",
    "\n",
    "- Language modeling and text generation\n",
    "- Speech recognition\n",
    "- Time-series forecasting\n",
    "- Sentiment analysis and other forms of text classification\n",
    "\n",
    "The recurrent nature of RNNs allows them to capture temporal patterns and dependencies, making them a powerful tool for tasks that require an understanding of context within sequences.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "While RNNs theoretically can handle long sequences, they often face challenges with long-term dependencies due to issues like vanishing or exploding gradients. Techniques such as Long Short-Term Memory (LSTM) cells and Gated Recurrent Units (GRUs) have been developed to mitigate these issues and enhance RNNs' ability to scale to longer sequences. These two (LSTM & GRU) follow this section.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "RNNs can be sensitive to noise in the sequence data, especially if the noise affects the temporal dependencies that the network aims to learn. Regularization techniques and careful design of the network architecture are crucial to improve their robustness.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "- **Long Short-Term Memory (LSTM):** Designed to overcome the vanishing gradient problem, allowing RNNs to learn long-term dependencies.\n",
    "- **Gated Recurrent Units (GRUs):** A simpler alternative to LSTMs that achieves similar performance with fewer parameters.\n",
    "- **Bidirectional RNNs:** Process the data in both forward and backward directions, providing additional context and improving performance on tasks like speech recognition.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Recurrent Neural Networks:**\n",
    "\n",
    "- When dealing with sequential data where the order and context significantly impact the output.\n",
    "- For tasks requiring the model to remember and utilize past information over short or long sequences.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- Training RNNs can be computationally intensive and may require sophisticated techniques to deal with challenges like vanishing gradients.\n",
    "- The choice between plain RNNs, LSTMs, and GRUs depends on the specific requirements of the task, including the need for modeling long-term dependencies.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Recurrent Neural Networks represent a cornerstone in the field of sequential data analysis, offering the ability to process and generate predictions based on the context of input sequences.\n",
    "\n",
    "Their unique structure, capable of maintaining a form of internal memory, makes them indispensable for tasks that require understanding the temporal dynamics of data. Despite challenges in training and scalability, advancements like LSTMs and GRUs (seen following this) continue to push the boundaries of what RNNs can achieve, making them a valuable tool in the ever-evolving landscape of neural network architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "354889a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNN, self).__init__(input_size, hidden_size, output_size=1)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.recurrent_layer = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        h_t, _ = self.recurrent_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[:, -1, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88bc4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple RNN Model:\n",
      "SimpleRNN(\n",
      "  (input_layer): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (recurrent_layer): RNN(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Output: 0.4832405745983124\n"
     ]
    }
   ],
   "source": [
    "# Instantiate RNN Model\n",
    "simple_rnn_model = SimpleRNN(input_size,hidden_size)\n",
    "\n",
    "# Forward pass for the simple RNN model\n",
    "sample_input_rnn = torch.rand((1, 4, input_size))\n",
    "output_rnn = simple_rnn_model(sample_input_rnn)\n",
    "\n",
    "print(\"Simple RNN Model:\")\n",
    "print(simple_rnn_model)\n",
    "print(\"Output:\", output_rnn.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e99e4",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f536108",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Long Short Term Memory networks (LSTMs) are a specialized form of Recurrent Neural Networks (RNNs) designed to address the limitations of traditional RNNs, particularly in learning long-term dependencies. \n",
    "\n",
    "LSTMs are equipped with a unique architecture that includes memory cells and multiple gates (input, output, and forget gates), enabling them to maintain information over extended sequences and effectively manage the flow of information.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "LSTMs are versatile and can process a wide range of sequential data, including:\n",
    "\n",
    "- Textual data for natural language processing tasks.\n",
    "- Time-series data for forecasting in finance, weather, and more.\n",
    "- Sequential inputs from sensors for activity recognition or medical diagnosis.\n",
    "- Audio signals for speech recognition and music generation.\n",
    "\n",
    "Their ability to capture temporal dependencies makes them ideal for applications where the sequence and context of the data matter.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "LSTMs excel in tasks that require understanding and remembering context over long sequences, such as:\n",
    "\n",
    "- Language modeling and text generation.\n",
    "- Sequence to sequence translation, e.g., language translation.\n",
    "- Speech recognition and synthesis.\n",
    "- Complex time-series prediction and anomaly detection.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The sophisticated gating mechanisms of LSTMs allow them to scale well to longer sequences, addressing the vanishing gradient problem common in simpler RNNs. However, this complexity can lead to higher computational costs during training and inference, making efficiency and optimization key considerations for large-scale applications.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Thanks to their ability to selectively remember and forget information, LSTMs demonstrate robustness to noisy and irrelevant inputs, making them suitable for real-world applications where data quality can vary.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants and improvements on the original LSTM architecture have been proposed to enhance performance and efficiency, including:\n",
    "\n",
    "- **Bidirectional LSTMs (BiLSTMs):** Process data in both forward and backward directions, improving context understanding.\n",
    "- **Gated Recurrent Units (GRUs):** A simplified version of LSTMs that combines the input and forget gates into a single update gate.\n",
    "- **Peephole LSTMs:** Allow the gates to access the cell state directly, enhancing the control over the memory cell.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use LSTMs:**\n",
    "\n",
    "- For tasks involving sequences where the context and the temporal order of data points are crucial for making accurate predictions or decisions.\n",
    "- In scenarios where learning long-term dependencies is essential for performance.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "- While powerful, LSTMs can be more challenging to train and fine-tune due to their complexity and the larger number of parameters compared to simpler models.\n",
    "- Careful design of the network architecture and selection of hyperparameters are essential to balance performance with computational efficiency.\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Long Short Term Memory networks have revolutionized the handling of sequential data by enabling models to learn and remember over long sequences. Their design mitigates the challenges associated with traditional RNNs, making them a cornerstone for a wide array of applications in natural language processing, time-series analysis, and beyond. As research continues, LSTMs remain a critical tool in the deep learning toolkit, driving advancements in understanding sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16386575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Neural Network\n",
    "class LSTMNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_t, c_t) = self.lstm_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[-1, :, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0792094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNN(\n",
      "  (lstm_layer): LSTM(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "Input: tensor([[[0.2538, 0.5605, 0.0580],\n",
      "         [0.7429, 0.5649, 0.5510],\n",
      "         [0.3359, 0.9129, 0.1333],\n",
      "         [0.4529, 0.0286, 0.2647]]])\n",
      "Output: tensor([[0.4000, 0.5383, 0.6445, 0.6661]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LSTM neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 3  # Number of memory cells\n",
    "output_size = 4  # Number of output nodes\n",
    "lstm_model = LSTMNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, 4, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_lstm = lstm_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(lstm_model)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output:\", output_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0a067",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7a021e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super(GRUNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.gru_layer = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_t, _ = self.gru_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[:, -1, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9d9345c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNN(\n",
      "  (gru_layer): GRU(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "Input: tensor([[[0.1642, 0.7892, 0.1311],\n",
      "         [0.1254, 0.6721, 0.6972],\n",
      "         [0.3529, 0.5542, 0.5755],\n",
      "         [0.7039, 0.1032, 0.5056]]])\n",
      "Output: tensor([[0.4682, 0.5686, 0.4778, 0.5822]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the GRU neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 3  # Number of memory cells\n",
    "output_size = 4  # Number of output nodes\n",
    "gru_model = GRUNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, 4, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_gru = gru_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(gru_model)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Output:\", output_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5104bd",
   "metadata": {},
   "source": [
    "# Auto Encoder (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93670d",
   "metadata": {},
   "source": [
    "AE designed for unsupervised learning & Data compression.\n",
    "\n",
    "Learns compact representation of input data.\n",
    "\n",
    "Used for data denoising, dimensionality reduction, feature learning.\n",
    "\n",
    "Versitile building block in utilizing NN's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abe5d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f1bafde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (decoder): Linear(in_features=5, out_features=10, bias=True)\n",
      ")\n",
      "Output: tensor([[0.4638, 0.5080, 0.6151, 0.4351, 0.5180, 0.5389, 0.4567, 0.5438, 0.5156,\n",
      "         0.3931]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the autoencoder\n",
    "input_size = 10  # Number of input features\n",
    "hidden_size = 5  # Number of hidden nodes (compressed representation)\n",
    "autoencoder = Autoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))\n",
    "\n",
    "# Forward pass to get the reconstructed output\n",
    "output_autoencoder = autoencoder(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(autoencoder)\n",
    "print(\"Output:\", output_autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775f45f",
   "metadata": {},
   "source": [
    "# Variational AE (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1975c04",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Variational Autoencoders (VAEs) are a cornerstone in the field of generative AI, representing a powerful class of deep learning models for generative modeling. They are designed to learn the underlying probability distribution of training data, enabling the generation of new data points with similar properties. VAEs combine traditional autoencoder architecture with variational inference principles, allowing them to compress data into a latent space and then generate data by sampling from this space, thereby facilitating a deep exploration of the continuous latent space representing the data.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "VAEs demonstrate remarkable adaptability across a range of data types, including:\n",
    "- Images\n",
    "- Text\n",
    "- Audio\n",
    "- Continuous numerical data\n",
    "\n",
    "This versatility underscores their prominence in generative AI, making them a popular choice for a wide array of generative tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Emphasizing their role in generative AI, VAEs excel in:\n",
    "- Data generation\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Anomaly detection\n",
    "\n",
    "Their deep learning capabilities enable them not only to model complex distributions but also to generate new, coherent samples, showcasing the transformative potential of generative AI.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "With their deep neural network architecture, VAEs scale effectively to accommodate the complexity and volume of vast datasets, further solidifying their status in generative AI for handling high-dimensional data efficiently.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "VAEs' proficiency in denoising and reconstructing inputs highlights their robustness, making them invaluable for applications in generative AI where data cleanliness cannot be assured.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Reflecting the innovation in generative AI, various VAE models have been developed to address specific challenges or improve upon the original framework, including Conditional VAEs, Beta-VAEs, and Disentangled VAEs, each offering unique advantages for controlled data generation and enhanced interpretability of latent representations.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "In the realm of generative AI, VAEs are particularly suited for:\n",
    "- Generating new data that mimics the properties of specific datasets.\n",
    "- Unsupervised learning of complex data distributions.\n",
    "- Applications requiring a nuanced understanding of data's underlying structure.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "Training VAEs can present challenges, such as mode collapse, underscoring the need for expertise in generative AI to navigate these complexities successfully.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Variational Autoencoders (VAEs) have cemented their place as a fundamental technology in generative AI, offering a sophisticated mechanism for understanding and generating data. Their broad applicability and the depth of insight they provide into data's inherent structure make them a pivotal tool in the advancement of generative modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03ed9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.encoder_fc2_mean = nn.Linear(hidden_size, hidden_size)\n",
    "        self.encoder_fc2_logvar = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "        self.decoder_fc2 = nn.Linear(input_size, input_size)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = torch.relu(self.encoder_fc1(x))\n",
    "        mean = self.encoder_fc2_mean(x)\n",
    "        logvar = self.encoder_fc2_logvar(x)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "\n",
    "        # Decoder\n",
    "        x_hat = torch.relu(self.decoder_fc1(z))\n",
    "        x_hat = torch.sigmoid(self.decoder_fc2(x_hat))\n",
    "\n",
    "        return x_hat, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aa8a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (encoder_fc2_mean): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (encoder_fc2_logvar): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc2): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "Output: tensor([[0.4524, 0.5483, 0.4470, 0.4638]], grad_fn=<SigmoidBackward0>)\n",
      "Mean: tensor([[-0.2343, -0.4617, -0.1976,  0.0995]], grad_fn=<AddmmBackward0>)\n",
      "Log Variance: tensor([[-0.3335,  0.1722, -0.4423,  0.1874]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the variational autoencoder\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 4  # Number of hidden nodes in probabilistic layer\n",
    "vae = VariationalAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))\n",
    "\n",
    "# Forward pass to get the reconstructed output and latent variables\n",
    "output_vae, mean, logvar = vae(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(vae)\n",
    "print(\"Output:\", output_vae)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Log Variance:\", logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf95b0",
   "metadata": {},
   "source": [
    "# Denoising Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92eb90",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Denoising Autoencoders (DAEs) are an advanced type of autoencoder designed to *remove noise from input data*. By intentionally corrupting the input data and then learning to reconstruct the original, uncorrupted data, DAEs are trained to capture the most relevant features. This process enhances the model's ability to generalize from the data, making it highly effective for tasks that require robust feature extraction and data denoising capabilities.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Denoising Autoencoders are capable of processing various data types, including:\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "- Continuous numerical data\n",
    "\n",
    "Their adaptability makes them particularly useful for applications involving noisy or incomplete data.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Denoising Autoencoders are primarily used for:\n",
    "- Data denoising\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Data generation and enhancement\n",
    "\n",
    "By learning to ignore the \"noise\" in data, DAEs excel in recovering clean representations from corrupted inputs.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Similar to other autoencoders, the scalability of DAEs depends on the network architecture. Modern techniques and computational resources allow DAEs to handle large datasets and complex noise patterns effectively, showcasing their scalability in practical applications.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "The core strength of DAEs lies in their robustness to noise. They are specifically trained to identify and ignore irrelevant features (noise), focusing on reconstructing the essential aspects of the data, which makes them exceptionally reliable for denoising tasks.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of DAEs have been developed to address different types of noise or to enhance specific aspects of denoising, including:\n",
    "- **Gaussian Noise DAEs:** Target Gaussian noise in the data.\n",
    "- **Salt-and-Pepper Noise DAEs:** Designed to remove binary noise from images.\n",
    "- **Variational DAEs:** Combine denoising capabilities with variational autoencoder frameworks for improved generative properties.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Denoising Autoencoders:**\n",
    "- For cleaning noisy data before further processing or analysis.\n",
    "- In feature extraction tasks where maintaining data integrity is crucial.\n",
    "- As a preprocessing step to improve the performance of subsequent machine learning models.\n",
    "\n",
    "**Considerations:**\n",
    "- The effectiveness of a DAE can vary based on the noise type and level; selecting the appropriate model variant is key.\n",
    "- Training DAEs requires a balance between denoising capability and preserving relevant features, necessitating careful tuning of model parameters.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Denoising Autoencoders offer a powerful solution for improving data quality, with their unique training strategy enabling them to extract clean, relevant features from noisy inputs. Their versatility across different data types and robustness to various noise patterns make them an invaluable tool in the data preprocessing pipeline, enhancing the performance of machine learning and deep learning models across a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0befd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = torch.relu(self.encoder_fc1(x))\n",
    "\n",
    "        # Decoder\n",
    "        x_hat = torch.sigmoid(self.decoder_fc1(x))\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec37cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenoisingAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "Noisy Input: tensor([[0.4459, 0.3527, 0.6623, 0.0946]])\n",
      "Reconstructed Output: tensor([[0.5092, 0.5703, 0.4878, 0.3946]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the denoising autoencoder\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 4  # Number of hidden nodes\n",
    "dae = DenoisingAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input (noisy data)\n",
    "noisy_input = torch.rand((1, input_size))  # Example Noisy Data\n",
    "\n",
    "# Forward pass to get the reconstructed output\n",
    "output_dae = dae(noisy_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(dae)\n",
    "print(\"Noisy Input:\", noisy_input)\n",
    "print(\"Reconstructed Output:\", output_dae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302da45",
   "metadata": {},
   "source": [
    "# Sparse Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17273f9a",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Sparse Autoencoders represent a specialized variant of autoencoders, aimed at *unsupervised learning of compressed representations*. By introducing sparsity constraints, they enforce most neurons to be inactive, enhancing feature detection and data representation efficiency. This approach improves generalization, making them suitable for tasks requiring robust feature extraction.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Sparse Autoencoders efficiently process:\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "- Continuous numerical data\n",
    "\n",
    "Their versatility across different data types highlights their utility in feature extraction and data compression tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Key applications include:\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Data denoising\n",
    "- Pretraining for deeper neural networks\n",
    "\n",
    "Sparsity constraints enable these models to learn higher-level features, distinguishing them from traditional autoencoders.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Despite sparsity aiding in learning efficient representations, the network's size and depth impact its ability to model complex distributions and computational requirements.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "They demonstrate significant robustness to noise, attributed to their focus on essential features, making them ideal for denoising and robust representation learning.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Variants are based on the sparsity enforcement method:\n",
    "- **KL Divergence Sparse Autoencoder:** Penalizes deviations from a target sparsity level using Kullback-Leibler divergence.\n",
    "- **L1 Regularization Sparse Autoencoder:** Applies L1 penalty on hidden units' activations to encourage sparsity.\n",
    "- **Winner-Take-All (WTA) Sparse Autoencoder:** Only a fraction of the most active hidden units are allowed to update their weights, enhancing sparsity.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Sparse Autoencoders:**\n",
    "- In extracting meaningful features from high-dimensional data.\n",
    "- For dimensionality reduction with interpretability.\n",
    "- During pretraining phases for deep learning models, providing a good initial weight set that captures useful data patterns.\n",
    "\n",
    "**Considerations:**\n",
    "- Selecting the appropriate sparsity constraint and regularization technique is critical for balancing feature selectivity and model complexity.\n",
    "- Hyperparameters require careful tuning to achieve desired sparsity levels and optimal performance.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Sparse Autoencoders stand out for learning efficient and interpretable data representations, with enforced sparsity offering clear advantages in feature selection and model robustness. They are invaluable in preprocessing, feature extraction, and as a pretraining step, enhancing subsequent models' performance across various data types and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e588e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, sparsity_target=0.1, sparsity_weight=0.2):\n",
    "        super(SparseAutoencoder, self).__init__(input_size, hidden_size, output_size=input_size)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        # Sparsity parameters\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoded = self.encoder_fc1(x)\n",
    "        encoded = self.relu(encoded)\n",
    "\n",
    "        # Decoder\n",
    "        decoded = torch.sigmoid(self.decoder_fc1(encoded))\n",
    "\n",
    "        return decoded, encoded\n",
    "\n",
    "    def loss_function(self, x, x_hat, encoded):\n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='mean')\n",
    "\n",
    "        # Sparsity loss\n",
    "        sparsity_loss = torch.sum(self.kl_divergence(self.sparsity_target, encoded))\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + self.sparsity_weight * sparsity_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def kl_divergence(self, target, activations):\n",
    "        # KL Divergence to enforce sparsity\n",
    "        p = torch.mean(activations, dim=0)  # Average activation over the dataset\n",
    "        return target * torch.log(target / p) + (1 - target) * torch.log((1 - target) / (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3951074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Input: tensor([[0.9295, 0.4682, 0.1434, 0.8834, 0.6853]])\n",
      "Reconstructed Output: tensor([[0.3732, 0.6395, 0.5619, 0.5443, 0.6051]], grad_fn=<SigmoidBackward0>)\n",
      "Encoded Representation: tensor([[0.0000, 0.3336, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "Loss: inf\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the sparse autoencoder\n",
    "input_size = 5  # Number of input features\n",
    "hidden_size = 3  # Number of hidden nodes\n",
    "sae = SparseAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the reconstructed output and encoded representation\n",
    "output_sae, encoded_sae = sae(sample_input)\n",
    "\n",
    "# Calculate the loss\n",
    "loss_sae = sae.loss_function(sample_input, output_sae, encoded_sae)\n",
    "\n",
    "# Print the model architecture, output, and loss\n",
    "print(sae)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Reconstructed Output:\", output_sae)\n",
    "print(\"Encoded Representation:\", encoded_sae)\n",
    "print(\"Loss:\", loss_sae.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b869e1",
   "metadata": {},
   "source": [
    "# Markov Chain (MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf55135",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Markov Chains represent a stochastic model describing a sequence of possible events where the probability of each event depends only on the state attained in the previous event. This mathematical framework is fundamental in the study of random processes and is widely applicable across various domains, including statistical mechanics, economics, and predictive modeling. Markov Chains are particularly valued for their simplicity and power in modeling the randomness of systems evolving over time.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Markov Chains are applicable to:\n",
    "- Discrete events or states\n",
    "- Temporal or spatial sequences\n",
    "\n",
    "Their adaptability allows them to model a wide array of processes, from simple random walks to complex decision-making scenarios.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Markov Chains excel in:\n",
    "- Predicting state transitions\n",
    "- Modeling random processes\n",
    "- Decision making under uncertainty\n",
    "\n",
    "Their predictive capabilities make them an essential tool for scenarios where future states depend on the current state, without the need for historical data.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Markov Chains scale well with the complexity of the model, primarily influenced by the number of states. While larger state spaces increase computational demands, advancements in algorithms and computing power have made it feasible to tackle complex chains efficiently.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Given their probabilistic nature, Markov Chains naturally incorporate and manage uncertainty and noise within their models. This robustness makes them suitable for applications where data may be incomplete or inherently random.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Markov Chains come in various forms, including:\n",
    "- **Discrete-Time Markov Chains:** Model transitions in discrete time steps.\n",
    "- **Continuous-Time Markov Chains:** Allow for transitions at any point in time.\n",
    "- **Hidden Markov Models (HMMs):** Extend Markov Chains by allowing observations to be a probabilistic function of the state, useful in scenarios where states are not directly observable.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Markov Chains:**\n",
    "- For modeling sequential or temporal data where future states depend on the current state.\n",
    "- In decision-making processes to evaluate different strategies under uncertainty.\n",
    "- When analyzing systems or processes that evolve over time in predictable patterns.\n",
    "\n",
    "**Considerations:**\n",
    "- Markov Chains assume the future is independent of the past given the present state, which may not hold in systems with memory or where historical context is crucial.\n",
    "- They are best applied to processes where this assumption of memorylessness (the Markov property) is reasonable or where state transitions are primarily influenced by the current state.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Markov Chains offer a powerful and flexible framework for modeling random processes and making predictions based on state transitions. By understanding their structure, capabilities, and the variety of their applications, one can effectively leverage Markov Chains to gain insights into complex systems, predict future events, and make informed decisions under uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89af0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChainNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MarkovChainNN, self).__init__(input_size, hidden_size, output_size=input_size)\n",
    "        self.transition_matrix = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply a simple linear transformation based on the transition matrix\n",
    "        x = torch.matmul(x, self.transition_matrix)\n",
    "        # Apply a linear layer to get the final output\n",
    "        output = self.output_layer(x)\n",
    "        # You might want to apply some non-linearity here based on your specific needs\n",
    "        # For example, you can use torch.relu(output) or torch.sigmoid(output) depending on the task\n",
    "        return output\n",
    "\n",
    "# Instantiate the Markov Chain neural network\n",
    "input_size = 4  # Number of input features \n",
    "hidden_size = 8  # Number of hidden states\n",
    "markov_chain_model = MarkovChainNN(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_markov_chain = markov_chain_model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbcf5cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarkovChainNN(\n",
      "  (output_layer): Linear(in_features=8, out_features=4, bias=True)\n",
      ")\n",
      "Output: tensor([[-0.2257,  0.1200, -0.0358,  0.9254]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the model architecture and output\n",
    "print(markov_chain_model)\n",
    "print(\"Output:\", output_markov_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed412a",
   "metadata": {},
   "source": [
    "# Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ee1d6",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Hopfield Networks are a form of recurrent neural network with a unique structure that allows them to serve as associative memory systems. These networks are characterized by fully connected neurons with symmetric weight matrices, enabling them to converge to stable states or \"memories\". This architecture makes Hopfield Networks particularly adept at solving optimization and memory recall tasks, leveraging their ability to find energy minima to recall stored patterns.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Hopfield Networks primarily deal with:\n",
    "- Binary data\n",
    "- Bipolar data\n",
    "\n",
    "Their structure is optimized for patterns represented in these formats, making them suitable for tasks that can be encoded as binary or bipolar vectors.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Hopfield Networks are well-suited for:\n",
    "- Pattern recognition\n",
    "- Associative memory recall\n",
    "- Optimization problems\n",
    "\n",
    "Their ability to serve as content-addressable (\"associative\") memory systems allows them to recall entire patterns based on partial or noisy inputs, showcasing their strength in tasks requiring robust pattern completion and error correction.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "While Hopfield Networks provide powerful capabilities for pattern recognition and memory recall, their scalability is limited by the network size due to the fully connected nature of the architecture. The capacity of a Hopfield Network to store memories without error is approximately 15% of the number of neurons, limiting the size of problems they can effectively solve without modifications or extensions.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "A key feature of Hopfield Networks is their robustness to noise in input patterns. They can recover original stored patterns from inputs that are partially incorrect or incomplete, making them highly effective for tasks requiring error tolerance and noise reduction in pattern recall.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "To address scalability and efficiency, several variants of Hopfield Networks have been developed, including:\n",
    "- **Continuous Hopfield Networks:** Extend the binary model to continuous values, allowing for application to a wider range of problems.\n",
    "- **Stochastic Hopfield Networks:** Introduce randomness in the update rules, enhancing the network's ability to escape local minima and find better solutions for optimization problems.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Hopfield Networks:**\n",
    "- When the task involves recovering or completing partial patterns.\n",
    "- For optimization problems where potential solutions can be encoded as binary or bipolar vectors.\n",
    "- In applications where associative memory models offer a natural solution.\n",
    "\n",
    "**Considerations:**\n",
    "- Hopfield Networks are not well-suited for large-scale problems due to their limited storage capacity and the computational cost of fully connected networks.\n",
    "- They may not be the best choice for new tasks with high-dimensional data or where deep learning approaches have demonstrated superior performance.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Hopfield Networks offer a approach to associative memory and optimization problems, with their unique ability to recall stored patterns from noisy or incomplete inputs. Understanding their structure, capabilities, and limitations is crucial for leveraging their strengths in relevant applications, while recognizing when alternative neural network models might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7c8d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HopfieldNetwork(BaseNN):\n",
    "    def __init__(self, input_size):\n",
    "        super(HopfieldNetwork, self).__init__(input_size, hidden_size=None, output_size=input_size)\n",
    "        \n",
    "        # Weight matrix for the Hopfield Network\n",
    "        self.weights = nn.Parameter(torch.zeros((input_size, input_size), dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the Hopfield Network dynamics\n",
    "        y = torch.sign(x @ self.weights).long()  # Convert to torch.long after applying sign\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "221dcea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HopfieldNetwork()\n",
      "Output: [[0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 5  # Change this based on your needs\n",
    "hopfield_model = HopfieldNetwork(input_size)\n",
    "\n",
    "# Define a sample input pattern (1 or -1)\n",
    "sample_input = torch.tensor([[1, -1, 1, -1, 1]], dtype=torch.float)  # Change the data type to torch.float\n",
    "\n",
    "# Forward pass to retrieve the output\n",
    "output_hopfield = hopfield_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(hopfield_model)\n",
    "print(\"Output:\", output_hopfield.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f9cec",
   "metadata": {},
   "source": [
    "# Boltzmann Machine (BM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d81af",
   "metadata": {},
   "source": [
    "BM is a stochastic RNN designed to find a probability distribution over its set of binary-valued patterns. \n",
    "\n",
    "*Main Objective* is to learn the joint probablity distribution of its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8f37f",
   "metadata": {},
   "source": [
    "Unique Features:\n",
    "-visible & hidden units forming bipartite graph\n",
    "-connects between units have weights & model learns weights during training\n",
    "-stochastic update process for both training & inference\n",
    "\n",
    "Common Uses:\n",
    "-unsupervised learning tasks like feature learning, dimensionality reduction, density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7668407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzmannMachine(nn.Module):\n",
    "    def __init__(self, num_visible, num_hidden):\n",
    "        super(BoltzmannMachine, self).__init__()\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        # Define the parameters (weights and biases)\n",
    "        self.weights = nn.Parameter(torch.randn(num_visible, num_hidden))\n",
    "        self.visible_bias = nn.Parameter(torch.randn(num_visible))\n",
    "        self.hidden_bias = nn.Parameter(torch.randn(num_hidden))\n",
    "\n",
    "    def forward(self, visible_states):\n",
    "        # Ensure visible_states has the correct dimensions (batch_size x num_visible)\n",
    "        if visible_states.dim() == 1:\n",
    "            visible_states = visible_states.view(1, -1)\n",
    "\n",
    "        # Compute the hidden probabilities given visible states\n",
    "        hidden_probabilities = F.sigmoid(F.linear(visible_states, self.weights.t(), self.hidden_bias))\n",
    "\n",
    "        # Sample hidden states from the computed probabilities\n",
    "        hidden_states = torch.bernoulli(hidden_probabilities)\n",
    "\n",
    "        # Compute the visible probabilities given the sampled hidden states\n",
    "        visible_probabilities = F.sigmoid(F.linear(hidden_states, self.weights, self.visible_bias))\n",
    "\n",
    "        # Sample visible states from the computed probabilities\n",
    "        visible_states = torch.bernoulli(visible_probabilities)\n",
    "\n",
    "        return visible_states, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ebd1d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoltzmannMachine()\n",
      "Sampled Visible State: [[0. 1. 1. 1. 1.]]\n",
      "Sampled Hidden State: [[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_visible = 5\n",
    "num_hidden = 3\n",
    "\n",
    "boltzmann_machine = BoltzmannMachine(num_visible, num_hidden)\n",
    "\n",
    "# Define a sample visible state (binary values)\n",
    "sample_visible_state = torch.tensor([1, 0, 1, 0, 1.], dtype=torch.float)\n",
    "\n",
    "# Perform a Gibbs sampling step\n",
    "sampled_visible, sampled_hidden = boltzmann_machine(sample_visible_state)\n",
    "\n",
    "# Print the model architecture and sampled states\n",
    "print(boltzmann_machine)\n",
    "print(\"Sampled Visible State:\", sampled_visible.detach().numpy())\n",
    "print(\"Sampled Hidden State:\", sampled_hidden.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba54b75",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ab0c6",
   "metadata": {},
   "source": [
    "Differences from normal Boltzmann Machine:\n",
    "\n",
    "    -No connections between units within same layer (no hidden-hidden or visible-visible connections)\n",
    "    \n",
    "    -Bipartite graph with one layer of visible units and one layer of hidden units\n",
    "    \n",
    "Objective: \n",
    "\n",
    "    -RBM objective with changes for potentially more effective feature learning\n",
    "\n",
    "Unique Features:\n",
    "\n",
    "    -Widely used for feature learning & are building blocks in deep learning architectures.\n",
    "    \n",
    "    -Efficent training, Contrastive Divergence (CD) is often used for training RBMs\n",
    "Common Uses:\n",
    "\n",
    "    -Feature Learning; pre-training deep NN\n",
    "    \n",
    "    -Collaborative filtering, topic modeling, other unsupervised learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "27c65354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(BaseNN):\n",
    "    def __init__(self, visible_size, hidden_size):\n",
    "        super(RBM, self).__init__(visible_size, hidden_size, None)\n",
    "        self.weights = nn.Parameter(torch.randn(visible_size, hidden_size))\n",
    "        self.visible_bias = nn.Parameter(torch.zeros(visible_size))\n",
    "        self.hidden_bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_prob = F.sigmoid(F.linear(x, self.weights.t(), self.hidden_bias))\n",
    "        hidden_state = torch.bernoulli(hidden_prob)\n",
    "        reconstructed_prob = F.sigmoid(F.linear(hidden_state, self.weights, self.visible_bias))\n",
    "        return hidden_state, reconstructed_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0741e106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM()\n",
      "Sampled Visible State: [[1. 0. 1. 0. 1.]]\n",
      "Hidden States: [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "visible_size = 5\n",
    "hidden_size = 3\n",
    "\n",
    "# Create an RBM model\n",
    "rbm_model = RBM(visible_size, hidden_size)\n",
    "\n",
    "# Define a sample visible state (binary values)\n",
    "sample_visible_state = torch.tensor([[1, 0, 1, 0, 1.]], dtype=torch.float)\n",
    "\n",
    "# Forward pass to get the hidden states\n",
    "hidden_states, _ = rbm_model(sample_visible_state)\n",
    "\n",
    "# Print the model architecture and hidden states\n",
    "print(rbm_model)\n",
    "print(\"Sampled Visible State:\", sample_visible_state.detach().numpy())\n",
    "print(\"Hidden States:\", hidden_states.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee6f13",
   "metadata": {},
   "source": [
    "# Deep Belief Network (DBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930ddac",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "    -Unsupervised learning & feature learning\n",
    "    -Model complex hierarchical representations of data\n",
    "Unique Features:\n",
    "    \n",
    "    -Multiple layer of stochastic, latent variables (usually binary)\n",
    "    -Stack of Restricted Boltzmann Machines (See super in the DBN class init for implementing via making DBN layers)\n",
    "    - Uses a layer-wise pre-training approach followed by fine-tuning using backprop\n",
    "    \n",
    "Common Uses:\n",
    "    \n",
    "    - Feature Learning\n",
    "    - Generative tasks (new samples from learned distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a12d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN(BaseNN):\n",
    "    def __init__(self, visible_size, hidden_sizes):\n",
    "        super(DBN, self).__init__(visible_size, None, None)\n",
    "        self.rbm_layers = nn.ModuleList([RBM(visible_size, hidden_size) for hidden_size in hidden_sizes])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through each RBM layer\n",
    "        for rbm_layer in self.rbm_layers:\n",
    "            x, _ = rbm_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cd7ea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN(\n",
      "  (rbm_layers): ModuleList(\n",
      "    (0-1): 2 x RBM()\n",
      "  )\n",
      ")\n",
      "Output: [[0.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "visible_size = 5\n",
    "hidden_sizes = [5, 1]\n",
    "\n",
    "# Create a Deep Belief Network\n",
    "dbn_model = DBN(visible_size, hidden_sizes)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, visible_size))\n",
    "\n",
    "# Forward pass through the DBN\n",
    "output_dbn = dbn_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(dbn_model)\n",
    "print(\"Output:\", output_dbn.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf783d",
   "metadata": {},
   "source": [
    "# Deep Convolutional Network (DCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878295fe",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    \n",
    "    -Processing structured grid data like images\n",
    "    -Excels @ capturing hierarchical spatial patterns\n",
    "\n",
    "Unique Features:\n",
    "    \n",
    "    -Uses convolutional layers w/ learnable filters that capture local patterns\n",
    "    -Typically includes pooling layers to reduce spatial dimensions & increase computational efficency\n",
    "    -Uses shared weights in convolutional layers for translation invariance\n",
    "    \n",
    "Common uses:\n",
    "    \n",
    "    -Image classification\n",
    "    -Feature learning in spatial data (hierarchical representations of spatial features)\n",
    "    -Transfer learning (pre-trained CNNs on large datasets often fine-tuned for specific tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "298e0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(BaseNN):\n",
    "    def __init__(self, input_channels, num_classes, image_size):\n",
    "        hidden_size = 64\n",
    "\n",
    "        super(DeepCNN, self).__init__(input_size=image_size, hidden_size=hidden_size, output_size=num_classes)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * (image_size // 4) * (image_size // 4), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df257385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Output Scores: [[-0.06619537 -0.02664012 -0.06680252 -0.03263853 -0.06080712  0.02075643\n",
      "  -0.01576368 -0.11744033 -0.03575525  0.09718715]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_channels = 1  # Grayscale images\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "\n",
    "deep_cnn_model = DeepCNN(input_channels, num_classes, image_size)\n",
    "\n",
    "sample_image = torch.rand((1, input_channels, image_size, image_size))\n",
    "\n",
    "output_scores = deep_cnn_model(sample_image)\n",
    "\n",
    "print(deep_cnn_model)\n",
    "print(\"Output Scores:\", output_scores.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319cc98",
   "metadata": {},
   "source": [
    "# Deconvolutional Network (DN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51fa2d",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    \n",
    "    -Reconstruction & generation of struuctured data (especially images)\n",
    "    -specialize in capturing hierarchical spatial patterns\n",
    "\n",
    "Unique Features:\n",
    "    \n",
    "    -deconvolutional layers w/ learnable filters for reconstructing spatial patterns\n",
    "    -usually include unpooling layers to increase spatial dimensions while maintaining computational efficency\n",
    "    -shared weights in deconvolutional layers to introduce translation invariance during reconstruction\n",
    "    \n",
    "Common uses:\n",
    "    \n",
    "    -image reconstruction & generation\n",
    "    -feature learning in spatial data w/ focus on capturing hierarchical spatial patterns\n",
    "    -semantic segmentation in images\n",
    "    -inverse problems (ex.image restoration)\n",
    "    -transfer learning (pre-trained on larger dataset -> fine-tuned for specicific reconstruction task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a3aa09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDeconvNet(BaseNN):\n",
    "    def __init__(self, input_channels, output_channels, output_size):\n",
    "        hidden_size = 64\n",
    "\n",
    "        super(DeepDeconvNet, self).__init__(input_size=None, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, 64 * (output_size // 4) * (output_size // 4))\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=output_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 64, (self.output_size // 4), (self.output_size // 4))\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.sigmoid(self.deconv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b62381",
   "metadata": {},
   "source": [
    "notes on parameters above: \n",
    "\n",
    "    channels: in context of images, 1=grayscale, 3=RGB\n",
    "    kernel_size: size of convolutional kernel-filter, size of local region considered for each convolutional operation\n",
    "    stride: step-size\n",
    "    padding: zero-padding addied to input of each side, helps maintain/adjust spatial dimensions\n",
    "    output_size: shape of output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec26ccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepDeconvNet(\n",
      "  (fc1): Linear(in_features=64, out_features=3136, bias=True)\n",
      "  (deconv1): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv2): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      ")\n",
      "Output Image Shape: torch.Size([1, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_channels = 1  # Grayscale images\n",
    "output_channels = 3  # Number of channels in the output image (e.g., RGB)\n",
    "output_size = 28\n",
    "\n",
    "deep_deconv_model = DeepDeconvNet(input_channels, output_channels, output_size)\n",
    "\n",
    "sample_latent_vector = torch.rand((1, deep_deconv_model.hidden_size))\n",
    "\n",
    "output_image = deep_deconv_model(sample_latent_vector)\n",
    "\n",
    "print(deep_deconv_model)\n",
    "print(\"Output Image Shape:\", output_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03054440",
   "metadata": {},
   "source": [
    "# Deep Convolutional Inverse Graphics Network (DCIGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faddcc1b",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "    -Inverting image rendering process to understand & reconstruct 3D structure from 2D images\n",
    "    -Specializes network for specific tasks involving 3D object manipulation & scene understanding \n",
    "Unique Features:\n",
    "\n",
    "    -Combines convolutional layers for feature extraction w/ inverse graphics layers for 3D reconstruction\n",
    "    -Capable of learning interpretable, manipulable representations of image elements\n",
    "    -Adaptable architecture for varying levels of detail and types of 3D reconstruction\n",
    "    \n",
    "Common uses:\n",
    "   \n",
    "    -3D object reconstruction from 2D images in computer vision and graphics\n",
    "    -Pose estimation for objects and characters in images\n",
    "    -comprehensive scene understanding for robotics & autonomous navigation\n",
    "    -applications in AR & VR for real-time image manipulation\n",
    "    -Image restoration & completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a00a65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCIGN(BaseNN):\n",
    "    def __init__(self, input_channels, input_size, output_size):\n",
    "        hidden_size = 64 \n",
    "\n",
    "        super(DCIGN, self).__init__(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the size of the flattened output after convolutional and pooling layers\n",
    "        self.flattened_size = 64 * (input_size // 4) * (input_size // 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "34963764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor: tensor([[ 0.0689, -0.0648,  0.0030, -0.1889, -0.0729,  0.0434, -0.1052,  0.0375,\n",
      "         -0.0234, -0.0689]], grad_fn=<AddmmBackward0>)\n",
      "Output Shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "input_channels = 3  # for RGB images\n",
    "input_size = 32  # Example size, adjust as needed\n",
    "output_size = 10  # Example output size\n",
    "\n",
    "dcign = DCIGN(input_channels, input_size, output_size)\n",
    "\n",
    "sample_input = torch.randn(1, input_channels, input_size, input_size)\n",
    "\n",
    "output = dcign(sample_input)\n",
    "\n",
    "print(\"Output Tensor:\", output)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd278c",
   "metadata": {},
   "source": [
    "# General Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbe30c",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "    -Generate images from random noise through adversarial training\n",
    "    - Improve generative models' performance by using 2 networks against each other (generator & discriminator) \n",
    "Unique Features:\n",
    "\n",
    "    -Adversarial training: Uses 2 NN's (generator/discriminator) that are trained simultaneously through adversarial processes. \n",
    "        -Generator learns to produce increasingly realistic data\n",
    "        -Discriminator learns to better distinguish between real & generated data\n",
    "    \n",
    "    -Feedback loop: Generator is updated based on the feedback from the discriminator, guiding it to product more realistic outputs\n",
    "    \n",
    "Common uses:\n",
    "   \n",
    "    -Image generation\n",
    "    -Style transfer\n",
    "    -Anomoly detection\n",
    "    -Super resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa33e6",
   "metadata": {},
   "source": [
    "## Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4611dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c8e3c",
   "metadata": {},
   "source": [
    "## Discriminator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1f612a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e8216",
   "metadata": {},
   "source": [
    "## GAN class\n",
    "Utilizes the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54a18ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(BaseNN):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__(input_size=None, hidden_size=None, output_size=None)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, x):\n",
    "        generated_data = self.generator(x)\n",
    "        discriminator_output = self.discriminator(generated_data)\n",
    "        return generated_data, discriminator_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15f75f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN(\n",
      "  (generator): Generator(\n",
      "    (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      "  (discriminator): Discriminator(\n",
      "    (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Generated Data: tensor([[0.5185, 0.5771, 0.5177, 0.5572, 0.5355, 0.4523, 0.5175, 0.4826, 0.5086,\n",
      "         0.4658]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator Output: tensor([[0.4669]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "\n",
    "generator = Generator(input_size, hidden_size, output_size)\n",
    "discriminator = Discriminator(output_size, hidden_size, 1)\n",
    "\n",
    "gan_model = GAN(generator, discriminator)\n",
    "\n",
    "sample_noise = torch.randn((1, input_size))\n",
    "\n",
    "generated_data, discriminator_output = gan_model(sample_noise)\n",
    "\n",
    "print(gan_model)\n",
    "print(\"Generated Data:\", generated_data)\n",
    "print(\"Discriminator Output:\", discriminator_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2bc21a",
   "metadata": {},
   "source": [
    "# Spiking Neural Network (SNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3386e",
   "metadata": {},
   "source": [
    "\n",
    "**Objective:**\n",
    "\n",
    "- Mimic the biological processes of the human brain more closely than traditional artificial neural networks by using neurons that fire in discrete spikes.\n",
    "- Process information through the timing of these spikes, enabling the network to efficiently handle spatiotemporal data and perform dynamic pattern recognition.\n",
    "\n",
    "**Unique Features:**\n",
    "\n",
    "- **Biologically Inspired:** Incorporates models of neurons that generate discrete spikes, a form of communication used in the biological nervous system.\n",
    "- **Temporal Dynamics:** Capable of capturing and processing temporal information inherent in the input data through the sequence and timing of spikes.\n",
    "- **Energy Efficiency:** Designed to be inherently more energy-efficient for certain computations, mirroring the energy efficiency seen in the human brain.\n",
    "- **Learning Through Time:** Utilizes learning mechanisms such as Spike-Timing-Dependent Plasticity (STDP), allowing the network to adapt based on the timing between spikes.\n",
    "\n",
    "**Common Uses:**\n",
    "\n",
    "- **Neurobiological Research:** Offers a platform for exploring theories of brain function and the principles underlying neural computation.\n",
    "- **Sensory Processing:** Applied in processing and interpreting data from sensory inputs, such as visual and auditory systems, in a manner similar to biological systems.\n",
    "- **Edge Computing:** Ideal for deployment in edge devices due to their low power consumption, where they can perform real-time data analysis.\n",
    "- **Pattern Recognition:** Utilized in tasks requiring the detection of patterns over time, such as speech recognition or gesture analysis.\n",
    "- **Robotic Control:** Empowers robots with the ability to process sensory inputs in real-time, leading to more adaptive and responsive behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6d2802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        # Define a simple linear layer to simulate neuron connections\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        # Spike function could be a Heaviside step function or similar\n",
    "        self.spike_fn = lambda x: torch.heaviside(x - 0.5, torch.tensor([0.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.spike_fn(x)  # Simulate spiking behavior\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa46ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([[-1.0168,  0.8071,  1.0896, -0.0985, -0.7639,  0.7797, -1.1863,  1.1127,\n",
      "          0.0859,  0.4825]])\n",
      "Spiked Output: tensor([[1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.]], grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 10  # Output size is not used in this simplified example but included for consistency with the class definition\n",
    "\n",
    "# Instantiate the SNN model\n",
    "snn_model = SNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate a sample input (batch size, input size)\n",
    "sample_input = torch.randn((1, input_size))\n",
    "\n",
    "# Forward pass through the SNN\n",
    "spiked_output = snn_model(sample_input)\n",
    "\n",
    "print(\"Sample Input:\", sample_input)\n",
    "print(\"Spiked Output:\", spiked_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ada33",
   "metadata": {},
   "source": [
    "Output explanation: The output tensor is of size *hidden size* with values either 0 or 1, representing whether each neuron in the hidden layer fired (1) or did not fire(0) based on the simplified spike function.\n",
    "\n",
    "This example demonstrates the instantation and baic usage, however it is a highly abstracted version for practical implementation and scope limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b21f0",
   "metadata": {},
   "source": [
    "# Liquid State Machine (LSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0646524",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    \n",
    "    -Process time-varying inputs\n",
    "    -Utilize high-dimensional transient states (liquid states) induced by input stimuli for copmutation allowing the network to perform temporal pattern regonition and time-series prediction\n",
    "\n",
    "Unique Features:\n",
    "    \n",
    "    -Utilizes a network of spiking neurons to create a responsive state to input stimuli\n",
    "    -Specializes in handling sequences and temporal patterns\n",
    "    -Flexible readout layer interprets the reservoir's state for varied tasks\n",
    "\n",
    "Common Uses:\n",
    "    \n",
    "    -Neurobiological simulations and understanding brain functions\n",
    "    Speech and gesture recognition for interactive systems\n",
    "    -Time-series forcasting in finance & weather prediction\n",
    "    -Robotic sensory processing for adaptive control\n",
    "    -Biometric authentication through pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c70767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSM(SNN):\n",
    "    def __init__(self, input_size, reservoir_size, output_size):\n",
    "        super(LSM, self).__init__(input_size, reservoir_size, output_size)\n",
    "        # In a true LSM, the reservoir would be more complex and involve dynamic connections.\n",
    "        # Here, we simulate it with a single RNN layer for simplicity.\n",
    "        self.reservoir = nn.RNN(input_size, reservoir_size, batch_first=True)\n",
    "        # The readout layer\n",
    "        self.readout = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input through the simplified 'reservoir'\n",
    "        reservoir_state, _ = self.reservoir(x)\n",
    "        \n",
    "        # Assuming the last state as the representation\n",
    "        reservoir_state = reservoir_state[:, -1, :]\n",
    "        output = self.readout(reservoir_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68dcf3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSM Model: LSM(\n",
      "  (linear): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (reservoir): RNN(10, 128, batch_first=True)\n",
      "  (readout): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Output Shape: torch.Size([5, 1])\n",
      "Output: tensor([[ 0.0650],\n",
      "        [-0.2211],\n",
      "        [-0.1590],\n",
      "        [-0.2507],\n",
      "        [ 0.0578]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "reservoir_size = 128\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the LSM model\n",
    "lsm_model = LSM(input_size, reservoir_size, output_size)\n",
    "\n",
    "# Generate a sample input (batch size, sequence length, input size)\n",
    "# Let's create a batch of 5 sequences, each of length 7 (time steps) with 10 features\n",
    "sample_input = torch.randn((5, 7, input_size))\n",
    "\n",
    "# Forward pass through the LSM\n",
    "output = lsm_model(sample_input)\n",
    "\n",
    "print(\"LSM Model:\", lsm_model)\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4109c9e",
   "metadata": {},
   "source": [
    "### Expected Output and Understanding\n",
    "\n",
    "- **LSM Model:** This print statement will display the structure of the LSM model, including the RNN (reservoir) and the readout linear layer.\n",
    "- **Output Shape:** Since the readout layer's output size is 1, and you're processing a batch of 5 sequences, the output shape should be `[5, 1]`, indicating that for each sequence in the batch, you get a single output value.\n",
    "- **Output:** This will show the actual output values from the LSM. These values are generated by processing the synthetic sequential data through the LSM's reservoir and readout layer.\n",
    "\n",
    "This example is a straightforward demonstration meant to illustrate how you might set up and use an LSM model with PyTorch for sequence processing tasks. The synthetic data doesn't represent a specific real-world problem, but in practice, you could adapt this setup to work on tasks like time-series forecasting, sequence classification, or any problem where understanding temporal dynamics is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce46100",
   "metadata": {},
   "source": [
    "# Extreme Learning Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc53838",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Extreme Learning Machines (ELMs) represent an innovative class of single-hidden layer feedforward neural networks (SLFNs) that streamline the learning process by randomly assigning input weights and biases, focusing instead on analytically determining the output weights. This unique approach reduces training complexity and time, making ELMs particularly suitable for rapid prototyping and handling large or noisy datasets efficiently.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "ELMs are versatile, capable of processing:\n",
    "- Numerical\n",
    "- Time-series\n",
    "- Images\n",
    "- Continuous data\n",
    "\n",
    "This adaptability makes them applicable across a broad spectrum of data-intensive fields.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "ELMs excel in:\n",
    "- Classification\n",
    "- Regression\n",
    "- Feature Learning\n",
    "\n",
    "Their fast learning speed and high efficiency position them as a powerful tool for both predictive modeling and data representation tasks.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "ELMs demonstrate remarkable scalability, efficiently managing large datasets and complex models with adjustable hidden nodes. This attribute is pivotal for applications in big data, where the volume and dimensionality of data can significantly impact computational performance.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "One of the standout features of ELMs is their robustness to noise, making them exceptionally reliable in real-world scenarios where data quality may vary. This robustness ensures that ELMs maintain high performance even when data is imperfect or incomplete.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of ELMs have been developed to cater to specific needs, including:\n",
    "- **Kernel ELM (KELM):** Offers enhanced capabilities for non-linear problem solving.\n",
    "- **Online Sequential ELM (OSELM):** Ideal for dynamic environments where data is available in sequences or streams.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use ELMs:**\n",
    "- Rapid model development is required.\n",
    "- Dealing with large or noisy datasets.\n",
    "- The task involves linear or non-linear problems where quick training is beneficial.\n",
    "\n",
    "**Considerations:**\n",
    "- While ELMs offer significant advantages in terms of speed and simplicity, they may not be the best fit for tasks requiring deep interpretability of model decisions. \n",
    "- For highly unstructured data, such as raw text or images that necessitate deep learning techniques, exploring other neural network models might yield better results.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Extreme Learning Machines offer a unique combination of speed, efficiency, and versatility, making them a valuable addition to the neural network toolkit. By understanding their capabilities, implementation variants, and practical applications, researchers and practitioners can effectively leverage ELMs to address a wide range of challenges in data analysis and predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2eacf9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELM(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ELM, self).__init__(input_size, hidden_size, output_size)\n",
    "        \n",
    "        # Initialize random weights for the hidden layer\n",
    "        self.hidden_weights = torch.randn(input_size, hidden_size)\n",
    "        self.hidden_bias = torch.randn(hidden_size)\n",
    "        \n",
    "        # No learning required for hidden layer, so no need for parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Linear output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate the output of the hidden layer\n",
    "        hidden_output = torch.matmul(x, self.hidden_weights) + self.hidden_bias\n",
    "        hidden_output = F.relu(hidden_output)  # Apply ReLU activation function\n",
    "        \n",
    "        # Pass the hidden layer output through the output layer\n",
    "        output = self.output_layer(hidden_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d65d24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([[-1.4513, -2.6494]])\n",
      "Output Prediction: [[0.90435386]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "\n",
    "elm_model = ELM(input_size, hidden_size, output_size)\n",
    "sample_input = torch.randn((1, input_size))\n",
    "\n",
    "output_prediction = elm_model(sample_input)\n",
    "print(\"Sample Input:\", sample_input)\n",
    "print(\"Output Prediction:\", output_prediction.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d522611",
   "metadata": {},
   "source": [
    "# Echo State Network (ESN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720ccbe",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Echo State Networks (ESNs) belong to the reservoir computing family, distinguished for their novel approach to training recurrent neural networks (RNNs). ESNs simplify the training process by keeping the internal connections of the network (the \"reservoir\") fixed while only adjusting the weights of the output layer. This architecture enables ESNs to efficiently process temporal or sequential data, making them particularly adept at tasks requiring memory of past inputs, such as time-series forecasting and sequence modeling.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Echo State Networks are tailored for:\n",
    "- Time-series data\n",
    "- Sequential data\n",
    "- Temporal patterns in audio and speech\n",
    "\n",
    "Their specialization in handling sequences makes them versatile tools for dynamic systems analysis and prediction.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "ESNs excel in / are suited for:\n",
    "- Time-series forecasting from financial markets, environmental sensors, or health monitoring.\n",
    "- Sequential data in natural language processing or music generation.\n",
    "- Dynamic system modeling\n",
    "- Temporal patterns in audio and speech recognition, benefiting from ESNs' capacity to handle varying lengths of sequences.\n",
    "\n",
    "The ability of ESNs to retain information over time and their sensitivity to initial conditions enable complex temporal pattern recognition and prediction capabilities.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Thanks to their fixed reservoir, ESNs efficiently manage large datasets and sequences without the computational burden typical of fully trainable RNNs. This attribute allows ESNs to excel in tasks with long dependencies, providing scalability and performance advantages.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Echo State Networks demonstrate considerable robustness to noise in input data, thanks to the reservoir's capacity to absorb and process noisy signals without significant degradation in performance. This characteristic is invaluable in real-world applications where data often comes with a degree of uncertainty and variability.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Variations of Echo State Networks have been developed to optimize performance across different domains, including:\n",
    "- **Leaky Integrator ESNs:** Introduce a mechanism to control the speed at which the reservoir's state updates, improving memory and stability.\n",
    "- **Deep ESNs:** Stack multiple reservoirs to enhance representational capacity and capture more complex patterns in data.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Echo State Networks:**\n",
    "- In scenarios where capturing long-term dependencies in sequential data is crucial.\n",
    "- For tasks that benefit from quick, efficient training of recurrent structures.\n",
    "- When working with noisy time-series or dynamic systems requiring predictive modeling.\n",
    "\n",
    "**Considerations:**\n",
    "- While ESNs offer a compelling approach for specific sequential tasks, their performance is heavily dependent on the reservoir's configuration and the quality of the output layer's training.\n",
    "- Ensuring the reservoir possesses the \"echo state property\" is critical for model stability and effectiveness, requiring careful tuning of parameters such as connectivity and spectral radius.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Echo State Networks provide a powerful yet efficient solution for modeling and predicting temporal sequences, standing out for their simplicity and performance in sequence-related tasks. Their design philosophy, emphasizing the \"echo state property\" and focusing training on the output layer, offers a unique approach to recurrent neural network training. By leveraging ESNs, practitioners can tackle a wide array of challenges in temporal data analysis, benefiting from their robustness to noise and their capacity to capture complex dynamic behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3407498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESN(nn.Module):\n",
    "    def __init__(self, input_size, reservoir_size, output_size):\n",
    "        super(ESN, self).__init__()\n",
    "        # Initialize weights\n",
    "        self.W_in = nn.Parameter(torch.randn(input_size, reservoir_size) * 0.1)  # Scaled for stability\n",
    "        self.W_res = nn.Parameter(torch.randn(reservoir_size, reservoir_size) * 0.1)\n",
    "        self.W_out = nn.Parameter(torch.randn(reservoir_size, output_size) * 0.1)\n",
    "        \n",
    "        # Ensure the echo state property through spectral radius adjustment\n",
    "        spectral_radius = torch.max(abs(torch.linalg.eigvals(self.W_res)))\n",
    "        self.W_res.data /= spectral_radius / 0.95  # Adjust to slightly below 1 for stability\n",
    "        \n",
    "        # Initialize the reservoir state\n",
    "        self.reservoir_state = torch.zeros(1, reservoir_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update the reservoir with current input and previous state\n",
    "        self.reservoir_state = torch.tanh(x @ self.W_in + self.reservoir_state @ self.W_res)\n",
    "        \n",
    "        # Compute the output\n",
    "        output = self.reservoir_state @ self.W_out\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc9aad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([[-0.4273, -0.6233]])\n",
      "Output Prediction: [[-0.01071815]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 2\n",
    "reservoir_size = 5\n",
    "output_size = 1\n",
    "\n",
    "esn_model = ESN(input_size, reservoir_size, output_size)\n",
    "sample_input = torch.randn((1, input_size))\n",
    "\n",
    "output_prediction = esn_model(sample_input)\n",
    "print(\"Sample Input:\", sample_input)\n",
    "print(\"Output Prediction:\", output_prediction.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b37044",
   "metadata": {},
   "source": [
    "# Deep Residual Network (DRN) (ResNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6072a6",
   "metadata": {},
   "source": [
    "### Deep Residual Networks: A Detailed Exploration\n",
    "\n",
    "**High-Level Overview**\n",
    "\n",
    "Deep Residual Networks (ResNets) revolutionized deep learning architectures through the introduction of \"residual blocks,\" allowing models to learn identity functions and effectively address the vanishing gradient problem. This innovation enables the training of networks that are much deeper than previously possible, significantly improving performance on tasks requiring the recognition of complex patterns, such as image classification and object detection.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "ResNets are particularly effective with:\n",
    "- Images\n",
    "- Video data\n",
    "\n",
    "Their design makes them a powerhouse in computer vision applications, from basic image recognition to complex video analysis tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Deep Residual Networks excel in:\n",
    "- Image classification\n",
    "- Object detection\n",
    "- Semantic segmentation\n",
    "- Face recognition\n",
    "\n",
    "By enabling the training of deeper networks without degradation, ResNets push the boundaries of what's achievable in visual recognition tasks.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The architecture of ResNets, characterized by residual blocks that skip one or more layers, directly addresses the scalability issue in deep learning. This allows ResNets to scale to hundreds or even thousands of layers while maintaining or even improving performance, a feat unattainable by traditional deep neural networks.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "ResNets show remarkable robustness to noise in the input data, thanks to their deep and complex architectures capable of extracting essential features while discarding irrelevant information. This robustness is critical for real-world applications where data imperfections are common.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of Deep Residual Networks have been developed to optimize performance across different domains and tasks, including:\n",
    "- **ResNet-50, ResNet-101, and ResNet-152:** Variations with different depths, balancing computational efficiency and performance.\n",
    "- **ResNeXt:** Introduces a \"cardinality\" dimension as an alternative to going deeper or wider, further improving model performance.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Deep Residual Networks:**\n",
    "- In tasks that benefit from deep architectures, such as complex image and video analysis.\n",
    "- When looking for models that provide state-of-the-art results in computer vision.\n",
    "\n",
    "**Considerations:**\n",
    "- While ResNets significantly mitigate the vanishing gradient problem, they also increase computational complexity. Efficient hardware or optimization techniques may be necessary for training.\n",
    "- The choice among ResNet variants depends on the specific requirements of the task, including computational resources and performance targets.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Deep Residual Networks represent a major milestone in the evolution of neural network architectures, offering unparalleled depth and performance in a wide range of computer vision tasks. Their innovative design not only tackles longstanding challenges in training deep networks but also sets a new standard for what's achievable in machine learning and artificial intelligence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e316558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66aef728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_blocks):\n",
    "        super(ResNet, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.in_channels = 64\n",
    "        self.conv = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(self.in_channels, hidden_size, num_blocks, stride=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # The number of output channels from the last block will be `hidden_size`\n",
    "        # Adjust this based on architecture\n",
    "        final_out_channels = hidden_size\n",
    "\n",
    "        # Initialize the fully connected layer with the correct number of input features\n",
    "        self.fc = nn.Linear(final_out_channels, output_size)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "618f712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Output shape: torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "# Instantiate the ResNet model\n",
    "resnet_model = ResNet(input_size=3, hidden_size=64, output_size=10, num_blocks=2)\n",
    "\n",
    "# Define a sample input tensor\n",
    "sample_input = torch.rand((4, 3, 224, 224))  # Batch size, Channels, Height, Width\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_resnet = resnet_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(resnet_model)\n",
    "print(\"Output shape:\", output_resnet.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7afff0",
   "metadata": {},
   "source": [
    "# Kohonen Networks (KN) / Self-Organizing Feature Map (SOFM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7adf3",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Kohonen Networks, also known as Self-Organizing Maps (SOMs), are a type of artificial neural network that uses unsupervised learning to produce a two-dimensional, discretized representation of the input space of the training samples. Developed by Teuvo Kohonen in the 1980s, these networks are particularly effective at preserving the topological properties of the input space, making them powerful tools for visualizing high-dimensional data in lower-dimensional spaces.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "SOMs can process a wide range of data types, including:\n",
    "- Numerical data\n",
    "- High-dimensional datasets\n",
    "- Multivariate data\n",
    "\n",
    "Their ability to handle high-dimensional data makes them suitable for complex pattern recognition tasks across various domains.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Kohonen Networks are utilized for:\n",
    "- Dimensionality reduction\n",
    "- Clustering and visualization of high-dimensional data\n",
    "- Feature mapping and extraction\n",
    "- Anomaly detection in datasets\n",
    "\n",
    "By organizing data into clusters that preserve the topological properties, SOMs facilitate a deeper understanding of data structure and relationships.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "SOMs are relatively scalable to large datasets, with their computational complexity mainly depending on the size of the map and the dimensionality of the input data. However, the training time can increase significantly with the size of the input space and the resolution of the map.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "SOMs demonstrate a good level of robustness to noise due to their competitive learning process, where only the neuron most similar to the input (and its neighbors) is updated. This local update mechanism allows SOMs to smooth out noise and focus on the underlying patterns in the data.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "There are several variants of Kohonen Networks, including:\n",
    "- **Batch SOMs:** Update the map using all training data at once, rather than one sample at a time, often leading to faster convergence.\n",
    "- **Toroidal SOMs:** Implement boundary conditions that wrap around the map, useful for cyclic or continuous data.\n",
    "- **Growing SOMs:** Dynamically adjust the size of the map during training to better fit the data.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Kohonen Networks:**\n",
    "- For exploratory data analysis to visualize and understand the structure of high-dimensional data.\n",
    "- In clustering tasks where the preservation of input space topology is desired.\n",
    "- For feature extraction and reduction before applying other machine learning models.\n",
    "\n",
    "**Considerations:**\n",
    "- The choice of map size and learning parameters can significantly affect the quality of the resulting SOM and requires careful tuning.\n",
    "- While SOMs provide valuable insights into data structure, they may not directly optimize for specific predictive tasks and are often used in a complementary role with other models.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Kohonen Networks or Self-Organizing Maps offer a unique approach to the unsupervised learning of high-dimensional data, providing a structured way to visualize and cluster complex datasets. By preserving the topological and metric relationships of the input space, SOMs serve as a valuable tool in the data scientist's toolkit, especially for tasks involving data exploration and understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "782be121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOFM(nn.Module):\n",
    "    def __init__(self, input_dim, map_size, lr=0.1, sigma=None):\n",
    "        super(SOFM, self).__init__()\n",
    "        self.map_size = map_size\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma if sigma is not None else max(map_size) / 2  # Initial radius\n",
    "        self.weight = nn.Parameter(torch.randn(map_size[0], map_size[1], input_dim))\n",
    "        self.locations = torch.tensor(np.array([[i, j] for i in range(map_size[0]) for j in range(map_size[1])])).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure x is compatible with weight dimensions for broadcasting\n",
    "        x = x.view(1, 1, -1)  # Reshape x to [1, 1, input_dim]\n",
    "\n",
    "        # Calculate square difference\n",
    "        sq_diff = torch.sum((self.weight - x) ** 2, dim=2)\n",
    "\n",
    "        # Find the best matching unit (BMU)\n",
    "        _, bmu_idx = torch.min(sq_diff.view(-1), dim=0)  # Flatten sq_diff and find index of BMU\n",
    "\n",
    "        # Retrieve the BMU location\n",
    "        bmu_location = self.locations[bmu_idx]  # Correctly index into self.locations\n",
    "\n",
    "        # Calculate distance squared from all neurons to BMU\n",
    "        distance_sq = torch.sum((self.locations - bmu_location) ** 2, dim=1)\n",
    "        lr = self.lr * torch.exp(-distance_sq / (2 * self.sigma ** 2))  # Adjust learning rate based on distance\n",
    "\n",
    "        # Apply learning rate to weight update\n",
    "        # Ensure lr is correctly shaped for broadcasting\n",
    "        lr = lr.view(self.map_size[0], self.map_size[1], 1)\n",
    "        weight_update = lr * (x - self.weight)\n",
    "\n",
    "        # Update the weights\n",
    "        self.weight.data += weight_update\n",
    "\n",
    "        return bmu_idx\n",
    "\n",
    "\n",
    "\n",
    "    def train_sofm(self, data, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for x in data:\n",
    "                self.forward(x)\n",
    "            # Decay learning parameters\n",
    "            self.lr *= 0.995  # Learning rate decay\n",
    "            self.sigma *= 0.995  # Radius decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd1f9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_dim = 3  # Dimensionality of input data\n",
    "map_size = (10, 10)  # Size of the SOFM map\n",
    "\n",
    "sofm = SOFM(input_dim=input_dim, map_size=map_size)\n",
    "data = torch.rand(100, input_dim)  # Example dataset\n",
    "\n",
    "sofm.train_sofm(data, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565cbd99",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a90307",
   "metadata": {},
   "source": [
    "### Support Vector Machines: A Comprehensive Analysis\n",
    "\n",
    "**High-Level Overview**\n",
    "\n",
    "Support Vector Machines (SVMs) are a powerful class of supervised learning models used for classification and regression tasks. Developed in the 1960s and refined in the 1990s, SVMs are based on the concept of finding the hyperplane that best separates different classes in the feature space. By maximizing the margin between the nearest points of the classes (support vectors), SVMs achieve high generalization ability, making them highly effective for a wide range of pattern recognition tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "SVMs are versatile and can process:\n",
    "- Numerical data\n",
    "- Categorical data (after encoding)\n",
    "- Text data (using TF-IDF or word embeddings)\n",
    "- Image data (using feature extraction techniques)\n",
    "\n",
    "This flexibility allows SVMs to be applied in various domains, from document classification to image recognition.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "SVMs are primarily used for:\n",
    "- Binary classification\n",
    "- Multiclass classification\n",
    "- Regression tasks\n",
    "- Outlier detection\n",
    "\n",
    "Their effectiveness in high-dimensional spaces and with complex decision boundaries makes them suitable for tasks requiring precise and robust classification and regression models.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "While SVMs perform exceptionally well on small to medium-sized datasets, their computational complexity can become a challenge with very large datasets or extremely high-dimensional spaces. Kernel tricks and dimensionality reduction techniques are often used to mitigate these challenges and enhance scalability.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "SVMs exhibit a significant level of robustness to noise and overfitting, especially in scenarios where the margin is maximized with a correct choice of the regularization parameter. Their reliance on support vectors (the most informative data points) rather than the entire dataset contributes to their resilience.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of SVMs exist to cater to specific needs, including:\n",
    "- **Linear SVMs:** Best suited for linearly separable data.\n",
    "- **Kernel SVMs:** Use kernel functions to operate in a transformed feature space, allowing them to handle non-linear data.\n",
    "- **Nu-SVMs and C-SVMs:** Offer different formulations for the optimization problem, providing flexibility in controlling the trade-off between margin size and misclassification error.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use SVMs:**\n",
    "- In binary or multiclass classification problems with clear margin separation.\n",
    "- For text and image classification tasks where high-dimensional feature spaces are common.\n",
    "- In applications where model interpretability is important, as the support vectors and the decision boundary provide insights into the model's predictions.\n",
    "\n",
    "**Considerations:**\n",
    "- Choosing the right kernel and tuning hyperparameters (like C and gamma) are crucial steps that significantly impact SVM performance.\n",
    "- SVMs may require more preprocessing effort, such as normalization and encoding, to ensure optimal model training.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Support Vector Machines stand out for their robustness, versatility, and efficacy in handling classification and regression tasks across a broad spectrum of domains. By effectively navigating the trade-offs between complexity and performance, practitioners can leverage SVMs to build highly accurate, generalizable models for a diverse array of challenges in machine learning and pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4a8cf267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearSVM, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)  # Output size is 1 for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def hinge_loss(y_pred, y_true):\n",
    "    return torch.mean(torch.clamp(1 - y_pred * y_true, min=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c9a383",
   "metadata": {},
   "source": [
    "Hinge Loss: A custom function that computes the hinge loss, encouraging the model to not only correctly classify training samples but also to maximize the margin between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a454ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.8118\n",
      "Epoch [20/100], Loss: 0.7104\n",
      "Epoch [30/100], Loss: 0.6596\n",
      "Epoch [40/100], Loss: 0.6342\n",
      "Epoch [50/100], Loss: 0.6113\n",
      "Epoch [60/100], Loss: 0.5884\n",
      "Epoch [70/100], Loss: 0.5655\n",
      "Epoch [80/100], Loss: 0.5426\n",
      "Epoch [90/100], Loss: 0.5198\n",
      "Epoch [100/100], Loss: 0.4969\n",
      "Accuracy: 0.7900\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 2  # Number of features\n",
    "output_size = 1  # Binary classification\n",
    "\n",
    "# Generate synthetic data for binary classification\n",
    "# Class 0: centered at (0.5, 0.5), Class 1: centered at (1.5, 1.5)\n",
    "n_samples = 100\n",
    "x_class0 = torch.rand((n_samples//2, input_size)) * 0.5\n",
    "y_class0 = torch.zeros(n_samples//2, 1)\n",
    "x_class1 = torch.rand((n_samples//2, input_size)) * 0.5 + 1\n",
    "y_class1 = torch.ones(n_samples//2, 1)\n",
    "\n",
    "x_train = torch.cat([x_class0, x_class1], dim=0)\n",
    "y_train = torch.cat([y_class0, y_class1], dim=0)\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = LinearSVM(input_size, output_size)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(svm_model.parameters(), lr=0.02)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = svm_model(x_train).squeeze()  # Ensure output matches dimension of y_train\n",
    "    labels = 2 * y_train.squeeze() - 1  # Convert labels to {-1, 1}\n",
    "    loss = hinge_loss(outputs, labels)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    y_pred = svm_model(x_train).squeeze()\n",
    "    y_pred_labels = (y_pred > 0).float()\n",
    "    accuracy = (y_pred_labels == y_train.squeeze()).float().mean()\n",
    "    print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88337c",
   "metadata": {},
   "source": [
    "#### Simplifications and Disclaimers\n",
    "Linear Decision Boundary: This example focuses on a linear SVM, suitable for linearly separable data. Real-world datasets often require more complex models, such as kernel SVMs, to capture non-linear relationships.\n",
    "Gradient Descent Optimization: While traditional SVMs are typically trained using quadratic programming solvers to directly solve the convex optimization problem, this example employs gradient descent for simplicity and educational clarity.\n",
    "Feature Space and Data Type: The demonstration uses synthetic numerical data. In practice, SVMs can be applied to a wide range of data types, including categorical and text data, often requiring preprocessing steps like encoding and feature extraction.\n",
    "Performance Metrics: The example primarily evaluates the model based on accuracy. Comprehensive model evaluation might include additional metrics and validation techniques to assess performance thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65807e79",
   "metadata": {},
   "source": [
    "# Neural Turing Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e0ba73",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Neural Turing Machines (NTMs) introduced an innovative blend of neural networks with an external memory component, reminiscent of traditional Turing machines. This combination allows NTMs to perform data processing while also having the capability to store and retrieve information dynamically. Initially proposed to enhance problem-solving and data manipulation, NTMs marked a significant evolution in the quest to imbue neural networks with memory and learning flexibility similar to human cognition.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Neural Turing Machines are adept at handling diverse data types, including:\n",
    "- Sequential data\n",
    "- Numerical data\n",
    "- Symbolic data\n",
    "\n",
    "This versatility stems from their unique memory system, enabling them to manage tasks that necessitate a nuanced understanding of sequences and historical context.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "NTMs have shown promise in various applications, such as:\n",
    "- Sequence prediction and generation\n",
    "- Executing simple algorithms (e.g., sorting, copying)\n",
    "- Complex problem-solving requiring memory and deduction\n",
    "\n",
    "Despite their potential, the practical adoption of NTMs in solving broad AI challenges remains exploratory, with newer models and architectures often preferred for specific applications.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "The scalability of Neural Turing Machines is influenced by the neural network's architecture and the external memory's design. While theoretically capable of handling tasks with extensive memory requirements, practical implementations face challenges related to training complexity and computational demands.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "NTMs offer a degree of robustness to noise, leveraging the neural network's capacity for pattern recognition alongside structured memory access to mitigate the effects of noisy inputs. However, the intricacies of memory management can introduce complexities in achieving consistent performance across varied tasks.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Advancements in the concept of NTMs have led to the development of variants such as:\n",
    "- **Differentiable Neural Computers (DNCs):** DNCs build on NTMs by refining memory access mechanisms, aiming for greater efficiency and applicability.\n",
    "- **Memory Networks:** While not direct descendants, Memory Networks share the ethos of integrating memory with neural processing, tailored for tasks requiring reasoning and long-term context.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Explore Neural Turing Machines:**\n",
    "- For academic and research endeavors focused on enhancing neural networks with memory capabilities.\n",
    "- In experimental projects aiming to understand and innovate on the integration of external memory systems with machine learning models.\n",
    "\n",
    "**Considerations:**\n",
    "- The complexity of NTMs and their variants necessitates a deep understanding of both neural networks and memory systems, making them more suited for research than immediate practical application.\n",
    "- Given the rapid advancement in AI, practitioners often turn to more recent developments tailored to specific use cases, such as Transformer models for sequence understanding and GANs for generative tasks.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While Neural Turing Machines represent a pivotal step towards more intelligent and flexible AI systems, their role in current neural network research has evolved. Today, NTMs are appreciated more for their conceptual contributions to integrating memory with neural processing rather than as a go-to solution for practical applications. As AI continues to advance, the principles underlying NTMs inspire ongoing innovations in creating models that more closely mimic human cognitive abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc6a0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, controller_size, memory_units, memory_unit_size):\n",
    "        super(NTM, self).__init__()\n",
    "        self.controller = nn.LSTM(input_size, controller_size, batch_first=True)  # Note: batch_first=True\n",
    "        self.memory = torch.zeros(memory_units, memory_unit_size)  # Initialize memory\n",
    "        \n",
    "        # Parameters for read and write heads (simplified)\n",
    "        self.read_weights = nn.Parameter(torch.randn(memory_units))\n",
    "        self.write_weights = nn.Parameter(torch.randn(memory_units))\n",
    "        self.read_vector = nn.Parameter(torch.randn(memory_unit_size))\n",
    "        self.output_layer = nn.Linear(controller_size + memory_unit_size, output_size)\n",
    "        \n",
    "    def read_from_memory(self):\n",
    "        # Simplified read mechanism: weighted sum of memory content\n",
    "        return torch.matmul(self.read_weights, self.memory)\n",
    "    \n",
    "    def write_to_memory(self, write_vector):\n",
    "        # Simplified write mechanism: update memory with new content\n",
    "        self.memory += torch.outer(self.write_weights, write_vector)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Controller processing. Note: Assuming x is of shape [batch, seq_len, input_size]\n",
    "        controller_output, _ = self.controller(x)\n",
    "        \n",
    "        # For simplicity, assuming we're dealing with single time-step sequences or interested in the last time step\n",
    "        # Adjusting controller_output to ensure it's [batch, controller_size]\n",
    "        controller_output = controller_output[:, -1, :]\n",
    "        \n",
    "        # Read from memory\n",
    "        read_vector = self.read_from_memory()  # This will be [memory_units, memory_unit_size]\n",
    "        # Ensuring read_vector is [batch, memory_unit_size] for batch processing\n",
    "        read_vector = read_vector.unsqueeze(0).expand(controller_output.size(0), -1)  # Expand to match batch size\n",
    "        \n",
    "        # Prepare controller output and read vector for final output\n",
    "        combined_output = torch.cat([controller_output, read_vector], dim=1)\n",
    "        final_output = self.output_layer(combined_output)\n",
    "        \n",
    "        # Example write operation (could be based on controller output or other logic)\n",
    "        write_vector = torch.randn_like(read_vector[0])  # Assuming write_vector needs to match a single read_vector\n",
    "        self.write_to_memory(write_vector)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27b2b6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NTM parameters\n",
    "input_dim = 4\n",
    "output_dim = 2\n",
    "controller_size = 10\n",
    "memory_units = 20\n",
    "memory_unit_size = 5\n",
    "\n",
    "# Instantiate the NTM model\n",
    "ntm_model = NTM(input_dim, output_dim, controller_size, memory_units, memory_unit_size)\n",
    "\n",
    "# Generate a sample input sequence\n",
    "sample_input = torch.randn(10, 1, input_dim)  # Example: 10 sequences, each with 1 timestep and input_dim features\n",
    "\n",
    "# Forward pass through the NTM\n",
    "output_sequences = [ntm_model(seq.unsqueeze(0)) for seq in sample_input]  # Note the addition of unsqueeze(0) for batch dimension\n",
    "\n",
    "# Convert output sequences to a tensor\n",
    "output_tensor = torch.cat(output_sequences, dim=0)  # Concatenating along the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9cadfd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor Shape: torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Tensor Shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d53c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0000, -0.9340, -0.0189,  0.1921]],\n",
      "\n",
      "        [[ 0.5453, -0.6682,  0.6663, -0.8751]],\n",
      "\n",
      "        [[ 0.7785, -0.0538,  0.0947, -0.9699]],\n",
      "\n",
      "        [[ 0.2586, -1.4413,  0.9093, -0.1881]],\n",
      "\n",
      "        [[ 0.2230, -0.1907, -0.4377,  0.2058]],\n",
      "\n",
      "        [[-0.0976,  0.1953,  1.5992,  1.0664]],\n",
      "\n",
      "        [[ 0.3100, -2.2921, -0.7467, -0.9357]],\n",
      "\n",
      "        [[-0.5578,  1.2146, -0.5251,  0.5620]],\n",
      "\n",
      "        [[-0.1448, -0.1506, -1.3177,  0.1048]],\n",
      "\n",
      "        [[ 0.1158, -1.2906, -1.9126,  0.3576]]])\n"
     ]
    }
   ],
   "source": [
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34f20bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1866, -0.2697],\n",
      "        [ 0.1113, -0.3211],\n",
      "        [ 0.1246, -0.2864],\n",
      "        [ 0.1439, -0.2877],\n",
      "        [ 0.1093, -0.1458],\n",
      "        [ 0.1600, -0.3097],\n",
      "        [ 0.1537, -0.3854],\n",
      "        [ 0.3131, -0.5121],\n",
      "        [ 0.3009, -0.4569],\n",
      "        [ 0.3299, -0.4673]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
