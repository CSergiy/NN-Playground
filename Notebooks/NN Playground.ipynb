{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98030d3c",
   "metadata": {},
   "source": [
    "# NN Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cef344",
   "metadata": {},
   "source": [
    "This notebook will be testing out variations on different neural networks, utilizing various forms of cells/nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951cbb3",
   "metadata": {},
   "source": [
    "Cell types include:\n",
    "\n",
    "-Backfed input Cell\n",
    "-Input Cell\n",
    "-Noisy Input Cell\n",
    "\n",
    "-Hidden Cell\n",
    "-Probablistic Hidden Cell\n",
    "-Spiking Hidden Cell\n",
    "\n",
    "-Output Cell\n",
    "-Matching Input Output Cell\n",
    "\n",
    "-Recurrent Cell\n",
    "-Memory Cell\n",
    "-Different Memory Cell\n",
    "\n",
    "-Kernel\n",
    "-Convolution or Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f39bb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0495dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ae600",
   "metadata": {},
   "source": [
    "# Base NN Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7db57",
   "metadata": {},
   "source": [
    "Creating a BaseNN class intended to use inheritance in later implementations of different NN's when abstracting the base class to make specialized classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdcf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for neural networks\n",
    "class BaseNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BaseNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"forward method must be implemented in derived classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a7800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the neural networks\n",
    "input_size = 2\n",
    "hidden_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab62054",
   "metadata": {},
   "source": [
    "# Basic Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f877d9a",
   "metadata": {},
   "source": [
    "## Perceptron (P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48ab51",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "The Perceptron represents the simplest form of a feedforward neural network, consisting of a single neuron with adjustable weights and a bias. Developed in 1957 by Frank Rosenblatt, it laid the groundwork for understanding neural networks. The Perceptron algorithm is a binary classifier that linearly separates data into two parts, making it a cornerstone in the study of machine learning for simple predictive modeling tasks.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Perceptrons can process:\n",
    "- Numerical data\n",
    "- Binary features\n",
    "\n",
    "Given its simplicity, it's primarily suited for linearly separable datasets where inputs can be categorized into two distinct groups.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Perceptrons are utilized for:\n",
    "- Binary classification tasks\n",
    "- Basic pattern recognition\n",
    "\n",
    "Their straightforward approach allows them to make decisions by weighing input features, showcasing early neural network capabilities in distinguishing between two classes.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Due to its simplicity, the scalability of a single-layer Perceptron is limited to problems that are linearly separable. For more complex datasets or non-linear problems, multi-layer networks or different algorithms are recommended.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Perceptrons can be sensitive to noise in the data, especially since they do not incorporate error minimization in the same way as more advanced models. They perform best with clean, well-defined datasets.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "While the basic Perceptron is foundational, several key developments have been made to extend its utility, including:\n",
    "- **Multi-layer Perceptrons (MLPs):** Comprising multiple layers of neurons to tackle non-linearly separable data.\n",
    "- **Stochastic Gradient Descent:** An optimization method allowing Perceptrons and their multi-layer successors to learn from training data iteratively.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Perceptrons:**\n",
    "- For simple linear classification problems.\n",
    "- As a learning tool to understand the basics of neural network architecture and linear decision boundaries.\n",
    "\n",
    "**Considerations:**\n",
    "- The Perceptron's inability to solve non-linear problems limits its application in complex real-world scenarios.\n",
    "- It serves as a building block for more sophisticated networks that can handle a broader range of tasks.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Perceptron model, with its simplicity, offers a fundamental understanding of neural network principles. Although its direct applications are limited to linearly separable tasks, the Perceptron remains an essential concept in machine learning, providing a stepping stone to more advanced neural network architectures and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83688fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize weights and bias randomly\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand()\n",
    "\n",
    "    def activate(self, x):\n",
    "        # Simple step function as activation\n",
    "        return 1 if x > 0 else 0\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Calculate the weighted sum of inputs\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "        # Apply the activation function\n",
    "        output = self.activate(weighted_sum)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f716aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0.5 0.8]\n",
      "Output: 1\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a perceptron with 2 input cells\n",
    "    perceptron = Perceptron(input_size=2)\n",
    "\n",
    "    # Example input\n",
    "    input_data = np.array([0.5, 0.8])\n",
    "\n",
    "    # Get the output from the perceptron\n",
    "    output = perceptron.forward(input_data)\n",
    "\n",
    "    print(f\"Input: {input_data}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e298f2a",
   "metadata": {},
   "source": [
    "## Feed Forward (FF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3c747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)  # Single output neuron\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        x = torch.relu(self.hidden_layer(x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the neural network\n",
    "input_size = 2  # Number of input features \n",
    "hidden_size = 3  # Number of neurons in the hidden layers\n",
    "model = FeedforwardNN(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09fa362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNN(\n",
      "  (input_layer): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (hidden_layer): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Output: 0.4091791808605194\n"
     ]
    }
   ],
   "source": [
    "# Define a sample input\n",
    "sample_input = torch.tensor([[0.5, 0.3]])  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output = model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(model)\n",
    "print(\"Output:\", output.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a478bc1",
   "metadata": {},
   "source": [
    "# NN Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef0b524",
   "metadata": {},
   "source": [
    "From this point onward, implementations are of different NN's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03243f0",
   "metadata": {},
   "source": [
    "# Radial Basis Network (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32822c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFunction:\n",
    "    def __init__(self, input_size, num_centers):\n",
    "        # Initialize centers and width parameters randomly\n",
    "        self.centers = np.random.rand(num_centers, input_size)\n",
    "        self.width = np.random.rand()\n",
    "        self.weights = np.random.rand(num_centers)\n",
    "    \n",
    "    def gaussian(self, x, center, width):\n",
    "        # Gaussian activation function\n",
    "        return np.exp(-np.sum((x - center)**2) / (2 * width**2))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Calculate the activation for each center\n",
    "        activations = np.array([self.gaussian(inputs, center, self.width) for center in self.centers])\n",
    "        \n",
    "        # Calculate the weighted sum of activations\n",
    "        weighted_sum = np.dot(activations, self.weights)\n",
    "        \n",
    "        # Apply a threshold for binary output\n",
    "        output = 1 if weighted_sum > 0.5 else 0\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ad0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0.5 0.8]\n",
      "Output: 1\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an RBF network with 2 input cells and 3 centers\n",
    "    rbf_network = RadialBasisFunction(input_size=2, num_centers=3)\n",
    "    \n",
    "    # Example input\n",
    "    input_data = np.array([0.5, 0.8])\n",
    "    \n",
    "    # Get the output from the RBF network\n",
    "    output = rbf_network.forward(input_data)\n",
    "    \n",
    "    print(f\"Input: {input_data}\")\n",
    "    print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f57745",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "354889a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNN, self).__init__(input_size, hidden_size, output_size=1)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.recurrent_layer = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        h_t, _ = self.recurrent_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[:, -1, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b1b98",
   "metadata": {},
   "source": [
    "## Comparing FF & RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88bc4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward Model:\n",
      "FeedforwardNN(\n",
      "  (input_layer): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (hidden_layer): Linear(in_features=3, out_features=3, bias=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Output: 0.5601709485054016\n",
      "\n",
      "Simple RNN Model:\n",
      "SimpleRNN(\n",
      "  (input_layer): Linear(in_features=2, out_features=3, bias=True)\n",
      "  (recurrent_layer): RNN(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "Output: 0.4223051071166992\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the models\n",
    "feedforward_model = FeedforwardNN(input_size, hidden_size)\n",
    "simple_rnn_model = SimpleRNN(input_size, hidden_size)\n",
    "\n",
    "# Forward pass for the feedforward model\n",
    "sample_input = torch.tensor([[0.5, 0.3]])\n",
    "output_feedforward = feedforward_model(sample_input)\n",
    "\n",
    "# Forward pass for the simple RNN model\n",
    "sample_input_rnn = torch.rand((1, 4, input_size))\n",
    "output_rnn = simple_rnn_model(sample_input_rnn)\n",
    "\n",
    "# Print the model architectures and outputs\n",
    "print(\"Feedforward Model:\")\n",
    "print(feedforward_model)\n",
    "print(\"Output:\", output_feedforward.item())\n",
    "\n",
    "print(\"\\nSimple RNN Model:\")\n",
    "print(simple_rnn_model)\n",
    "print(\"Output:\", output_rnn.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3ee08",
   "metadata": {},
   "source": [
    "# Deep Feed Forward (DFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d096c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Feed Forward Neural Network\n",
    "class DeepFeedforwardNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DeepFeedforwardNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(2)  # Two hidden layers with 4 nodes each\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = torch.sigmoid(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e01181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepFeedforwardNN(\n",
      "  (input_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=4, out_features=4, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=4, out_features=2, bias=True)\n",
      ")\n",
      "Output: tensor([[0.5978, 0.4909]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the deep feedforward neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 4  # Number of nodes in each hidden layer\n",
    "output_size = 2  # Number of output nodes\n",
    "deep_feedforward_model = DeepFeedforwardNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.tensor([[0.5, 0.3, 0.8]])  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_deep_feedforward = deep_feedforward_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(deep_feedforward_model)\n",
    "print(\"Output:\", output_deep_feedforward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e99e4",
   "metadata": {},
   "source": [
    "# Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16386575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Neural Network\n",
    "class LSTMNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.lstm_layer = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_t, c_t) = self.lstm_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[-1, :, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0792094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMNN(\n",
      "  (lstm_layer): LSTM(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "Output: tensor([[0.5801, 0.3975, 0.6583, 0.6457]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the LSTM neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 3  # Number of memory cells\n",
    "output_size = 4  # Number of output nodes\n",
    "lstm_model = LSTMNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, 4, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_lstm = lstm_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(lstm_model)\n",
    "print(\"Output:\", output_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0a067",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7a021e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        super(GRUNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        self.gru_layer = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_t, _ = self.gru_layer(x)\n",
    "        output = torch.sigmoid(self.output_layer(h_t[:, -1, :]))  # Taking the output from the last time step\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d9345c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUNN(\n",
      "  (gru_layer): GRU(3, 3, batch_first=True)\n",
      "  (output_layer): Linear(in_features=3, out_features=4, bias=True)\n",
      ")\n",
      "Output: tensor([[0.5840, 0.5677, 0.6334, 0.6433]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the GRU neural network\n",
    "input_size = 3  # Number of input features \n",
    "hidden_size = 3  # Number of memory cells\n",
    "output_size = 4  # Number of output nodes\n",
    "gru_model = GRUNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, 4, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_gru = gru_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(gru_model)\n",
    "print(\"Output:\", output_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5104bd",
   "metadata": {},
   "source": [
    "# Auto Encoder (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93670d",
   "metadata": {},
   "source": [
    "AE designed for unsupervised learning & Data compression.\n",
    "\n",
    "Learns compact representation of input data.\n",
    "\n",
    "Used for data denoising, dimensionality reduction, feature learning.\n",
    "\n",
    "Versitile building block in utilizing NN's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abe5d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        decoded = torch.sigmoid(self.decoder(encoded))\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f1bafde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (decoder): Linear(in_features=5, out_features=10, bias=True)\n",
      ")\n",
      "Output: tensor([[0.5548, 0.5509, 0.5095, 0.3987, 0.4270, 0.5357, 0.4606, 0.4512, 0.5785,\n",
      "         0.4374]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the autoencoder\n",
    "input_size = 10  # Number of input features\n",
    "hidden_size = 5  # Number of hidden nodes (compressed representation)\n",
    "autoencoder = Autoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))\n",
    "\n",
    "# Forward pass to get the reconstructed output\n",
    "output_autoencoder = autoencoder(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(autoencoder)\n",
    "print(\"Output:\", output_autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775f45f",
   "metadata": {},
   "source": [
    "# Variational AE (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1975c04",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Variational Autoencoders (VAEs) are a cornerstone in the field of generative AI, representing a powerful class of deep learning models for generative modeling. They are designed to learn the underlying probability distribution of training data, enabling the generation of new data points with similar properties. VAEs combine traditional autoencoder architecture with variational inference principles, allowing them to compress data into a latent space and then generate data by sampling from this space, thereby facilitating a deep exploration of the continuous latent space representing the data.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "VAEs demonstrate remarkable adaptability across a range of data types, including:\n",
    "- Images\n",
    "- Text\n",
    "- Audio\n",
    "- Continuous numerical data\n",
    "\n",
    "This versatility underscores their prominence in generative AI, making them a popular choice for a wide array of generative tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Emphasizing their role in generative AI, VAEs excel in:\n",
    "- Data generation\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Anomaly detection\n",
    "\n",
    "Their deep learning capabilities enable them not only to model complex distributions but also to generate new, coherent samples, showcasing the transformative potential of generative AI.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "With their deep neural network architecture, VAEs scale effectively to accommodate the complexity and volume of vast datasets, further solidifying their status in generative AI for handling high-dimensional data efficiently.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "VAEs' proficiency in denoising and reconstructing inputs highlights their robustness, making them invaluable for applications in generative AI where data cleanliness cannot be assured.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Reflecting the innovation in generative AI, various VAE models have been developed to address specific challenges or improve upon the original framework, including Conditional VAEs, Beta-VAEs, and Disentangled VAEs, each offering unique advantages for controlled data generation and enhanced interpretability of latent representations.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "In the realm of generative AI, VAEs are particularly suited for:\n",
    "- Generating new data that mimics the properties of specific datasets.\n",
    "- Unsupervised learning of complex data distributions.\n",
    "- Applications requiring a nuanced understanding of data's underlying structure.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    "Training VAEs can present challenges, such as mode collapse, underscoring the need for expertise in generative AI to navigate these complexities successfully.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Variational Autoencoders (VAEs) have cemented their place as a fundamental technology in generative AI, offering a sophisticated mechanism for understanding and generating data. Their broad applicability and the depth of insight they provide into data's inherent structure make them a pivotal tool in the advancement of generative modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03ed9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.encoder_fc2_mean = nn.Linear(hidden_size, hidden_size)\n",
    "        self.encoder_fc2_logvar = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "        self.decoder_fc2 = nn.Linear(input_size, input_size)\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = torch.relu(self.encoder_fc1(x))\n",
    "        mean = self.encoder_fc2_mean(x)\n",
    "        logvar = self.encoder_fc2_logvar(x)\n",
    "\n",
    "        # Reparameterization trick\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "\n",
    "        # Decoder\n",
    "        x_hat = torch.relu(self.decoder_fc1(z))\n",
    "        x_hat = torch.sigmoid(self.decoder_fc2(x_hat))\n",
    "\n",
    "        return x_hat, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aa8a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VariationalAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (encoder_fc2_mean): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (encoder_fc2_logvar): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc2): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "Output: tensor([[0.4106, 0.4347, 0.5420, 0.4469]], grad_fn=<SigmoidBackward0>)\n",
      "Mean: tensor([[ 0.2209, -0.2880, -0.0193, -0.0295]], grad_fn=<AddmmBackward0>)\n",
      "Log Variance: tensor([[-0.5089,  0.4982,  0.0301,  0.4687]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the variational autoencoder\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 4  # Number of hidden nodes in probabilistic layer\n",
    "vae = VariationalAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))\n",
    "\n",
    "# Forward pass to get the reconstructed output and latent variables\n",
    "output_vae, mean, logvar = vae(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(vae)\n",
    "print(\"Output:\", output_vae)\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Log Variance:\", logvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf95b0",
   "metadata": {},
   "source": [
    "# Denoising Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92eb90",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Denoising Autoencoders (DAEs) are an advanced type of autoencoder designed to *remove noise from input data*. By intentionally corrupting the input data and then learning to reconstruct the original, uncorrupted data, DAEs are trained to capture the most relevant features. This process enhances the model's ability to generalize from the data, making it highly effective for tasks that require robust feature extraction and data denoising capabilities.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Denoising Autoencoders are capable of processing various data types, including:\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "- Continuous numerical data\n",
    "\n",
    "Their adaptability makes them particularly useful for applications involving noisy or incomplete data.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Denoising Autoencoders are primarily used for:\n",
    "- Data denoising\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Data generation and enhancement\n",
    "\n",
    "By learning to ignore the \"noise\" in data, DAEs excel in recovering clean representations from corrupted inputs.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Similar to other autoencoders, the scalability of DAEs depends on the network architecture. Modern techniques and computational resources allow DAEs to handle large datasets and complex noise patterns effectively, showcasing their scalability in practical applications.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "The core strength of DAEs lies in their robustness to noise. They are specifically trained to identify and ignore irrelevant features (noise), focusing on reconstructing the essential aspects of the data, which makes them exceptionally reliable for denoising tasks.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of DAEs have been developed to address different types of noise or to enhance specific aspects of denoising, including:\n",
    "- **Gaussian Noise DAEs:** Target Gaussian noise in the data.\n",
    "- **Salt-and-Pepper Noise DAEs:** Designed to remove binary noise from images.\n",
    "- **Variational DAEs:** Combine denoising capabilities with variational autoencoder frameworks for improved generative properties.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Denoising Autoencoders:**\n",
    "- For cleaning noisy data before further processing or analysis.\n",
    "- In feature extraction tasks where maintaining data integrity is crucial.\n",
    "- As a preprocessing step to improve the performance of subsequent machine learning models.\n",
    "\n",
    "**Considerations:**\n",
    "- The effectiveness of a DAE can vary based on the noise type and level; selecting the appropriate model variant is key.\n",
    "- Training DAEs requires a balance between denoising capability and preserving relevant features, necessitating careful tuning of model parameters.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Denoising Autoencoders offer a powerful solution for improving data quality, with their unique training strategy enabling them to extract clean, relevant features from noisy inputs. Their versatility across different data types and robustness to various noise patterns make them an invaluable tool in the data preprocessing pipeline, enhancing the performance of machine learning and deep learning models across a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0befd205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = torch.relu(self.encoder_fc1(x))\n",
    "\n",
    "        # Decoder\n",
    "        x_hat = torch.sigmoid(self.decoder_fc1(x))\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec37cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenoisingAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "Noisy Input: tensor([[0.5138, 0.0090, 0.7281, 0.1610]])\n",
      "Reconstructed Output: tensor([[0.5775, 0.4534, 0.5027, 0.5040]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the denoising autoencoder\n",
    "input_size = 4  # Number of input features\n",
    "hidden_size = 4  # Number of hidden nodes\n",
    "dae = DenoisingAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input (noisy data)\n",
    "noisy_input = torch.rand((1, input_size))  # Example Noisy Data\n",
    "\n",
    "# Forward pass to get the reconstructed output\n",
    "output_dae = dae(noisy_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(dae)\n",
    "print(\"Noisy Input:\", noisy_input)\n",
    "print(\"Reconstructed Output:\", output_dae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302da45",
   "metadata": {},
   "source": [
    "# Sparse Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17273f9a",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Sparse Autoencoders represent a specialized variant of autoencoders, aimed at *unsupervised learning of compressed representations*. By introducing sparsity constraints, they enforce most neurons to be inactive, enhancing feature detection and data representation efficiency. This approach improves generalization, making them suitable for tasks requiring robust feature extraction.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Sparse Autoencoders efficiently process:\n",
    "- Images\n",
    "- Text\n",
    "- Audio signals\n",
    "- Continuous numerical data\n",
    "\n",
    "Their versatility across different data types highlights their utility in feature extraction and data compression tasks.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Key applications include:\n",
    "- Feature extraction and representation learning\n",
    "- Dimensionality reduction\n",
    "- Data denoising\n",
    "- Pretraining for deeper neural networks\n",
    "\n",
    "Sparsity constraints enable these models to learn higher-level features, distinguishing them from traditional autoencoders.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Despite sparsity aiding in learning efficient representations, the network's size and depth impact its ability to model complex distributions and computational requirements.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "They demonstrate significant robustness to noise, attributed to their focus on essential features, making them ideal for denoising and robust representation learning.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Variants are based on the sparsity enforcement method:\n",
    "- **KL Divergence Sparse Autoencoder:** Penalizes deviations from a target sparsity level using Kullback-Leibler divergence.\n",
    "- **L1 Regularization Sparse Autoencoder:** Applies L1 penalty on hidden units' activations to encourage sparsity.\n",
    "- **Winner-Take-All (WTA) Sparse Autoencoder:** Only a fraction of the most active hidden units are allowed to update their weights, enhancing sparsity.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Sparse Autoencoders:**\n",
    "- In extracting meaningful features from high-dimensional data.\n",
    "- For dimensionality reduction with interpretability.\n",
    "- During pretraining phases for deep learning models, providing a good initial weight set that captures useful data patterns.\n",
    "\n",
    "**Considerations:**\n",
    "- Selecting the appropriate sparsity constraint and regularization technique is critical for balancing feature selectivity and model complexity.\n",
    "- Hyperparameters require careful tuning to achieve desired sparsity levels and optimal performance.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Sparse Autoencoders stand out for learning efficient and interpretable data representations, with enforced sparsity offering clear advantages in feature selection and model robustness. They are invaluable in preprocessing, feature extraction, and as a pretraining step, enhancing subsequent models' performance across various data types and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e588e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, sparsity_target=0.1, sparsity_weight=0.2):\n",
    "        super(SparseAutoencoder, self).__init__(input_size, hidden_size, output_size=input_size)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        # Sparsity parameters\n",
    "        self.sparsity_target = sparsity_target\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoded = self.encoder_fc1(x)\n",
    "        encoded = self.relu(encoded)\n",
    "\n",
    "        # Decoder\n",
    "        decoded = torch.sigmoid(self.decoder_fc1(encoded))\n",
    "\n",
    "        return decoded, encoded\n",
    "\n",
    "    def loss_function(self, x, x_hat, encoded):\n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='mean')\n",
    "\n",
    "        # Sparsity loss\n",
    "        sparsity_loss = torch.sum(self.kl_divergence(self.sparsity_target, encoded))\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + self.sparsity_weight * sparsity_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def kl_divergence(self, target, activations):\n",
    "        # KL Divergence to enforce sparsity\n",
    "        p = torch.mean(activations, dim=0)  # Average activation over the dataset\n",
    "        return target * torch.log(target / p) + (1 - target) * torch.log((1 - target) / (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3951074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseAutoencoder(\n",
      "  (encoder_fc1): Linear(in_features=5, out_features=3, bias=True)\n",
      "  (decoder_fc1): Linear(in_features=3, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Input: tensor([[0.4828, 0.0561, 0.2652, 0.6145, 0.1415]])\n",
      "Reconstructed Output: tensor([[0.4442, 0.6020, 0.5842, 0.5638, 0.3021]], grad_fn=<SigmoidBackward0>)\n",
      "Encoded Representation: tensor([[0.5875, 0.1028, 0.1637]], grad_fn=<ReluBackward0>)\n",
      "Loss: 0.8145620226860046\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the sparse autoencoder\n",
    "input_size = 5  # Number of input features\n",
    "hidden_size = 3  # Number of hidden nodes\n",
    "sae = SparseAutoencoder(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the reconstructed output and encoded representation\n",
    "output_sae, encoded_sae = sae(sample_input)\n",
    "\n",
    "# Calculate the loss\n",
    "loss_sae = sae.loss_function(sample_input, output_sae, encoded_sae)\n",
    "\n",
    "# Print the model architecture, output, and loss\n",
    "print(sae)\n",
    "print(\"Input:\", sample_input)\n",
    "print(\"Reconstructed Output:\", output_sae)\n",
    "print(\"Encoded Representation:\", encoded_sae)\n",
    "print(\"Loss:\", loss_sae.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b869e1",
   "metadata": {},
   "source": [
    "# Markov Chain (MC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf55135",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Markov Chains represent a stochastic model describing a sequence of possible events where the probability of each event depends only on the state attained in the previous event. This mathematical framework is fundamental in the study of random processes and is widely applicable across various domains, including statistical mechanics, economics, and predictive modeling. Markov Chains are particularly valued for their simplicity and power in modeling the randomness of systems evolving over time.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Markov Chains are applicable to:\n",
    "- Discrete events or states\n",
    "- Temporal or spatial sequences\n",
    "\n",
    "Their adaptability allows them to model a wide array of processes, from simple random walks to complex decision-making scenarios.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Markov Chains excel in:\n",
    "- Predicting state transitions\n",
    "- Modeling random processes\n",
    "- Decision making under uncertainty\n",
    "\n",
    "Their predictive capabilities make them an essential tool for scenarios where future states depend on the current state, without the need for historical data.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "Markov Chains scale well with the complexity of the model, primarily influenced by the number of states. While larger state spaces increase computational demands, advancements in algorithms and computing power have made it feasible to tackle complex chains efficiently.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "Given their probabilistic nature, Markov Chains naturally incorporate and manage uncertainty and noise within their models. This robustness makes them suitable for applications where data may be incomplete or inherently random.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Markov Chains come in various forms, including:\n",
    "- **Discrete-Time Markov Chains:** Model transitions in discrete time steps.\n",
    "- **Continuous-Time Markov Chains:** Allow for transitions at any point in time.\n",
    "- **Hidden Markov Models (HMMs):** Extend Markov Chains by allowing observations to be a probabilistic function of the state, useful in scenarios where states are not directly observable.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Markov Chains:**\n",
    "- For modeling sequential or temporal data where future states depend on the current state.\n",
    "- In decision-making processes to evaluate different strategies under uncertainty.\n",
    "- When analyzing systems or processes that evolve over time in predictable patterns.\n",
    "\n",
    "**Considerations:**\n",
    "- Markov Chains assume the future is independent of the past given the present state, which may not hold in systems with memory or where historical context is crucial.\n",
    "- They are best applied to processes where this assumption of memorylessness (the Markov property) is reasonable or where state transitions are primarily influenced by the current state.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Markov Chains offer a powerful and flexible framework for modeling random processes and making predictions based on state transitions. By understanding their structure, capabilities, and the variety of their applications, one can effectively leverage Markov Chains to gain insights into complex systems, predict future events, and make informed decisions under uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89af0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovChainNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MarkovChainNN, self).__init__(input_size, hidden_size, output_size=input_size)\n",
    "        self.transition_matrix = nn.Parameter(torch.randn(input_size, hidden_size))\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply a simple linear transformation based on the transition matrix\n",
    "        x = torch.matmul(x, self.transition_matrix)\n",
    "        # Apply a linear layer to get the final output\n",
    "        output = self.output_layer(x)\n",
    "        # You might want to apply some non-linearity here based on your specific needs\n",
    "        # For example, you can use torch.relu(output) or torch.sigmoid(output) depending on the task\n",
    "        return output\n",
    "\n",
    "# Instantiate the Markov Chain neural network\n",
    "input_size = 4  # Number of input features \n",
    "hidden_size = 8  # Number of hidden states\n",
    "markov_chain_model = MarkovChainNN(input_size, hidden_size)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, input_size))  # Example Data\n",
    "\n",
    "# Forward pass to get the output\n",
    "output_markov_chain = markov_chain_model(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbcf5cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarkovChainNN(\n",
      "  (output_layer): Linear(in_features=8, out_features=4, bias=True)\n",
      ")\n",
      "Output: tensor([[ 0.6076, -0.3390,  1.0989, -0.3724]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Print the model architecture and output\n",
    "print(markov_chain_model)\n",
    "print(\"Output:\", output_markov_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed412a",
   "metadata": {},
   "source": [
    "# Hopfield Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ee1d6",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Hopfield Networks are a form of recurrent neural network with a unique structure that allows them to serve as associative memory systems. These networks are characterized by fully connected neurons with symmetric weight matrices, enabling them to converge to stable states or \"memories\". This architecture makes Hopfield Networks particularly adept at solving optimization and memory recall tasks, leveraging their ability to find energy minima to recall stored patterns.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "Hopfield Networks primarily deal with:\n",
    "- Binary data\n",
    "- Bipolar data\n",
    "\n",
    "Their structure is optimized for patterns represented in these formats, making them suitable for tasks that can be encoded as binary or bipolar vectors.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "Hopfield Networks are well-suited for:\n",
    "- Pattern recognition\n",
    "- Associative memory recall\n",
    "- Optimization problems\n",
    "\n",
    "Their ability to serve as content-addressable (\"associative\") memory systems allows them to recall entire patterns based on partial or noisy inputs, showcasing their strength in tasks requiring robust pattern completion and error correction.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "While Hopfield Networks provide powerful capabilities for pattern recognition and memory recall, their scalability is limited by the network size due to the fully connected nature of the architecture. The capacity of a Hopfield Network to store memories without error is approximately 15% of the number of neurons, limiting the size of problems they can effectively solve without modifications or extensions.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "A key feature of Hopfield Networks is their robustness to noise in input patterns. They can recover original stored patterns from inputs that are partially incorrect or incomplete, making them highly effective for tasks requiring error tolerance and noise reduction in pattern recall.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "To address scalability and efficiency, several variants of Hopfield Networks have been developed, including:\n",
    "- **Continuous Hopfield Networks:** Extend the binary model to continuous values, allowing for application to a wider range of problems.\n",
    "- **Stochastic Hopfield Networks:** Introduce randomness in the update rules, enhancing the network's ability to escape local minima and find better solutions for optimization problems.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use Hopfield Networks:**\n",
    "- When the task involves recovering or completing partial patterns.\n",
    "- For optimization problems where potential solutions can be encoded as binary or bipolar vectors.\n",
    "- In applications where associative memory models offer a natural solution.\n",
    "\n",
    "**Considerations:**\n",
    "- Hopfield Networks are not well-suited for large-scale problems due to their limited storage capacity and the computational cost of fully connected networks.\n",
    "- They may not be the best choice for new tasks with high-dimensional data or where deep learning approaches have demonstrated superior performance.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Hopfield Networks offer a approach to associative memory and optimization problems, with their unique ability to recall stored patterns from noisy or incomplete inputs. Understanding their structure, capabilities, and limitations is crucial for leveraging their strengths in relevant applications, while recognizing when alternative neural network models might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7c8d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HopfieldNetwork(BaseNN):\n",
    "    def __init__(self, input_size):\n",
    "        super(HopfieldNetwork, self).__init__(input_size, hidden_size=None, output_size=input_size)\n",
    "        \n",
    "        # Weight matrix for the Hopfield Network\n",
    "        self.weights = nn.Parameter(torch.zeros((input_size, input_size), dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the Hopfield Network dynamics\n",
    "        y = torch.sign(x @ self.weights).long()  # Convert to torch.long after applying sign\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "221dcea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HopfieldNetwork()\n",
      "Output: [[0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 5  # Change this based on your needs\n",
    "hopfield_model = HopfieldNetwork(input_size)\n",
    "\n",
    "# Define a sample input pattern (1 or -1)\n",
    "sample_input = torch.tensor([[1, -1, 1, -1, 1]], dtype=torch.float)  # Change the data type to torch.float\n",
    "\n",
    "# Forward pass to retrieve the output\n",
    "output_hopfield = hopfield_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(hopfield_model)\n",
    "print(\"Output:\", output_hopfield.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f9cec",
   "metadata": {},
   "source": [
    "# Boltzmann Machine (BM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d81af",
   "metadata": {},
   "source": [
    "BM is a stochastic RNN designed to find a probability distribution over its set of binary-valued patterns. \n",
    "\n",
    "*Main Objective* is to learn the joint probablity distribution of its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8f37f",
   "metadata": {},
   "source": [
    "Unique Features:\n",
    "-visible & hidden units forming bipartite graph\n",
    "-connects between units have weights & model learns weights during training\n",
    "-stochastic update process for both training & inference\n",
    "\n",
    "Common Uses:\n",
    "-unsupervised learning tasks like feature learning, dimensionality reduction, density estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7668407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzmannMachine(nn.Module):\n",
    "    def __init__(self, num_visible, num_hidden):\n",
    "        super(BoltzmannMachine, self).__init__()\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        # Define the parameters (weights and biases)\n",
    "        self.weights = nn.Parameter(torch.randn(num_visible, num_hidden))\n",
    "        self.visible_bias = nn.Parameter(torch.randn(num_visible))\n",
    "        self.hidden_bias = nn.Parameter(torch.randn(num_hidden))\n",
    "\n",
    "    def forward(self, visible_states):\n",
    "        # Ensure visible_states has the correct dimensions (batch_size x num_visible)\n",
    "        if visible_states.dim() == 1:\n",
    "            visible_states = visible_states.view(1, -1)\n",
    "\n",
    "        # Compute the hidden probabilities given visible states\n",
    "        hidden_probabilities = F.sigmoid(F.linear(visible_states, self.weights.t(), self.hidden_bias))\n",
    "\n",
    "        # Sample hidden states from the computed probabilities\n",
    "        hidden_states = torch.bernoulli(hidden_probabilities)\n",
    "\n",
    "        # Compute the visible probabilities given the sampled hidden states\n",
    "        visible_probabilities = F.sigmoid(F.linear(hidden_states, self.weights, self.visible_bias))\n",
    "\n",
    "        # Sample visible states from the computed probabilities\n",
    "        visible_states = torch.bernoulli(visible_probabilities)\n",
    "\n",
    "        return visible_states, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ebd1d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoltzmannMachine()\n",
      "Sampled Visible State: [[1. 1. 1. 0. 1.]]\n",
      "Sampled Hidden State: [[0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "num_visible = 5\n",
    "num_hidden = 3\n",
    "\n",
    "boltzmann_machine = BoltzmannMachine(num_visible, num_hidden)\n",
    "\n",
    "# Define a sample visible state (binary values)\n",
    "sample_visible_state = torch.tensor([1, 0, 1, 0, 1.], dtype=torch.float)\n",
    "\n",
    "# Perform a Gibbs sampling step\n",
    "sampled_visible, sampled_hidden = boltzmann_machine(sample_visible_state)\n",
    "\n",
    "# Print the model architecture and sampled states\n",
    "print(boltzmann_machine)\n",
    "print(\"Sampled Visible State:\", sampled_visible.detach().numpy())\n",
    "print(\"Sampled Hidden State:\", sampled_hidden.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba54b75",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine (RBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ab0c6",
   "metadata": {},
   "source": [
    "Differences from normal Boltzmann Machine:\n",
    "\n",
    "    -No connections between units within same layer (no hidden-hidden or visible-visible connections)\n",
    "    \n",
    "    -Bipartite graph with one layer of visible units and one layer of hidden units\n",
    "    \n",
    "Objective: \n",
    "\n",
    "    -RBM objective with changes for potentially more effective feature learning\n",
    "\n",
    "Unique Features:\n",
    "\n",
    "    -Widely used for feature learning & are building blocks in deep learning architectures.\n",
    "    \n",
    "    -Efficent training, Contrastive Divergence (CD) is often used for training RBMs\n",
    "Common Uses:\n",
    "\n",
    "    -Feature Learning; pre-training deep NN\n",
    "    \n",
    "    -Collaborative filtering, topic modeling, other unsupervised learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27c65354",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(BaseNN):\n",
    "    def __init__(self, visible_size, hidden_size):\n",
    "        super(RBM, self).__init__(visible_size, hidden_size, None)\n",
    "        self.weights = nn.Parameter(torch.randn(visible_size, hidden_size))\n",
    "        self.visible_bias = nn.Parameter(torch.zeros(visible_size))\n",
    "        self.hidden_bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_prob = F.sigmoid(F.linear(x, self.weights.t(), self.hidden_bias))\n",
    "        hidden_state = torch.bernoulli(hidden_prob)\n",
    "        reconstructed_prob = F.sigmoid(F.linear(hidden_state, self.weights, self.visible_bias))\n",
    "        return hidden_state, reconstructed_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0741e106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM()\n",
      "Sampled Visible State: [[1. 0. 1. 0. 1.]]\n",
      "Hidden States: [[1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "visible_size = 5\n",
    "hidden_size = 3\n",
    "\n",
    "# Create an RBM model\n",
    "rbm_model = RBM(visible_size, hidden_size)\n",
    "\n",
    "# Define a sample visible state (binary values)\n",
    "sample_visible_state = torch.tensor([[1, 0, 1, 0, 1.]], dtype=torch.float)\n",
    "\n",
    "# Forward pass to get the hidden states\n",
    "hidden_states, _ = rbm_model(sample_visible_state)\n",
    "\n",
    "# Print the model architecture and hidden states\n",
    "print(rbm_model)\n",
    "print(\"Sampled Visible State:\", sample_visible_state.detach().numpy())\n",
    "print(\"Hidden States:\", hidden_states.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee6f13",
   "metadata": {},
   "source": [
    "# Deep Belief Network (DBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930ddac",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "    -Unsupervised learning & feature learning\n",
    "    -Model complex hierarchical representations of data\n",
    "Unique Features:\n",
    "    \n",
    "    -Multiple layer of stochastic, latent variables (usually binary)\n",
    "    -Stack of Restricted Boltzmann Machines (See super in the DBN class init for implementing via making DBN layers)\n",
    "    - Uses a layer-wise pre-training approach followed by fine-tuning using backprop\n",
    "    \n",
    "Common Uses:\n",
    "    \n",
    "    - Feature Learning\n",
    "    - Generative tasks (new samples from learned distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a12d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN(BaseNN):\n",
    "    def __init__(self, visible_size, hidden_sizes):\n",
    "        super(DBN, self).__init__(visible_size, None, None)\n",
    "        self.rbm_layers = nn.ModuleList([RBM(visible_size, hidden_size) for hidden_size in hidden_sizes])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through each RBM layer\n",
    "        for rbm_layer in self.rbm_layers:\n",
    "            x, _ = rbm_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cd7ea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN(\n",
      "  (rbm_layers): ModuleList(\n",
      "    (0-1): 2 x RBM()\n",
      "  )\n",
      ")\n",
      "Output: [[1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "visible_size = 5\n",
    "hidden_sizes = [5, 1]\n",
    "\n",
    "# Create a Deep Belief Network\n",
    "dbn_model = DBN(visible_size, hidden_sizes)\n",
    "\n",
    "# Define a sample input\n",
    "sample_input = torch.rand((1, visible_size))\n",
    "\n",
    "# Forward pass through the DBN\n",
    "output_dbn = dbn_model(sample_input)\n",
    "\n",
    "# Print the model architecture and output\n",
    "print(dbn_model)\n",
    "print(\"Output:\", output_dbn.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf783d",
   "metadata": {},
   "source": [
    "# Deep Convolutional Network (DCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878295fe",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    \n",
    "    -Processing structured grid data like images\n",
    "    -Excels @ capturing hierarchical spatial patterns\n",
    "\n",
    "Unique Features:\n",
    "    \n",
    "    -Uses convolutional layers w/ learnable filters that capture local patterns\n",
    "    -Typically includes pooling layers to reduce spatial dimensions & increase computational efficency\n",
    "    -Uses shared weights in convolutional layers for translation invariance\n",
    "    \n",
    "Common uses:\n",
    "    \n",
    "    -Image classification\n",
    "    -Feature learning in spatial data (hierarchical representations of spatial features)\n",
    "    -Transfer learning (pre-trained CNNs on large datasets often fine-tuned for specific tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "298e0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCNN(BaseNN):\n",
    "    def __init__(self, input_channels, num_classes, image_size):\n",
    "        hidden_size = 64\n",
    "\n",
    "        super(DeepCNN, self).__init__(input_size=image_size, hidden_size=hidden_size, output_size=num_classes)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * (image_size // 4) * (image_size // 4), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * (self.input_size // 4) * (self.input_size // 4))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df257385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Output Scores: [[-0.11650774  0.14896058 -0.14006138 -0.06969805 -0.08172397 -0.12495886\n",
      "  -0.13444492  0.15795682 -0.12663546  0.01591007]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_channels = 1  # Grayscale images\n",
    "num_classes = 10\n",
    "image_size = 28\n",
    "\n",
    "deep_cnn_model = DeepCNN(input_channels, num_classes, image_size)\n",
    "\n",
    "sample_image = torch.rand((1, input_channels, image_size, image_size))\n",
    "\n",
    "output_scores = deep_cnn_model(sample_image)\n",
    "\n",
    "print(deep_cnn_model)\n",
    "print(\"Output Scores:\", output_scores.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3319cc98",
   "metadata": {},
   "source": [
    "# Deconvolutional Network (DN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c51fa2d",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    \n",
    "    -Reconstruction & generation of struuctured data (especially images)\n",
    "    -specialize in capturing hierarchical spatial patterns\n",
    "\n",
    "Unique Features:\n",
    "    \n",
    "    -deconvolutional layers w/ learnable filters for reconstructing spatial patterns\n",
    "    -usually include unpooling layers to increase spatial dimensions while maintaining computational efficency\n",
    "    -shared weights in deconvolutional layers to introduce translation invariance during reconstruction\n",
    "    \n",
    "Common uses:\n",
    "    \n",
    "    -image reconstruction & generation\n",
    "    -feature learning in spatial data w/ focus on capturing hierarchical spatial patterns\n",
    "    -semantic segmentation in images\n",
    "    -inverse problems (ex.image restoration)\n",
    "    -transfer learning (pre-trained on larger dataset -> fine-tuned for specicific reconstruction task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3aa09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDeconvNet(BaseNN):\n",
    "    def __init__(self, input_channels, output_channels, output_size):\n",
    "        hidden_size = 64\n",
    "\n",
    "        super(DeepDeconvNet, self).__init__(input_size=None, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, 64 * (output_size // 4) * (output_size // 4))\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(in_channels=32, out_channels=output_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = x.view(-1, 64, (self.output_size // 4), (self.output_size // 4))\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.sigmoid(self.deconv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b62381",
   "metadata": {},
   "source": [
    "notes on parameters above: \n",
    "\n",
    "    channels: in context of images, 1=grayscale, 3=RGB\n",
    "    kernel_size: size of convolutional kernel-filter, size of local region considered for each convolutional operation\n",
    "    stride: step-size\n",
    "    padding: zero-padding addied to input of each side, helps maintain/adjust spatial dimensions\n",
    "    output_size: shape of output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec26ccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepDeconvNet(\n",
      "  (fc1): Linear(in_features=64, out_features=3136, bias=True)\n",
      "  (deconv1): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (deconv2): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      ")\n",
      "Output Image Shape: torch.Size([1, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_channels = 1  # Grayscale images\n",
    "output_channels = 3  # Number of channels in the output image (e.g., RGB)\n",
    "output_size = 28\n",
    "\n",
    "deep_deconv_model = DeepDeconvNet(input_channels, output_channels, output_size)\n",
    "\n",
    "sample_latent_vector = torch.rand((1, deep_deconv_model.hidden_size))\n",
    "\n",
    "output_image = deep_deconv_model(sample_latent_vector)\n",
    "\n",
    "print(deep_deconv_model)\n",
    "print(\"Output Image Shape:\", output_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03054440",
   "metadata": {},
   "source": [
    "# Deep Convolutional Inverse Graphics Network (DCIGN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faddcc1b",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "    -Inverting image rendering process to understand & reconstruct 3D structure from 2D images\n",
    "    -Specializes network for specific tasks involving 3D object manipulation & scene understanding \n",
    "Unique Features:\n",
    "\n",
    "    -Combines convolutional layers for feature extraction w/ inverse graphics layers for 3D reconstruction\n",
    "    -Capable of learning interpretable, manipulable representations of image elements\n",
    "    -Adaptable architecture for varying levels of detail and types of 3D reconstruction\n",
    "    \n",
    "Common uses:\n",
    "   \n",
    "    -3D object reconstruction from 2D images in computer vision and graphics\n",
    "    -Pose estimation for objects and characters in images\n",
    "    -comprehensive scene understanding for robotics & autonomous navigation\n",
    "    -applications in AR & VR for real-time image manipulation\n",
    "    -Image restoration & completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a00a65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCIGN(BaseNN):\n",
    "    def __init__(self, input_channels, input_size, output_size):\n",
    "        hidden_size = 64 \n",
    "\n",
    "        super(DCIGN, self).__init__(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Calculate the size of the flattened output after convolutional and pooling layers\n",
    "        self.flattened_size = 64 * (input_size // 4) * (input_size // 4)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flattened_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(-1, self.flattened_size)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34963764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor: tensor([[ 0.0539,  0.0224,  0.2151, -0.2104, -0.0529,  0.0575, -0.0496, -0.0005,\n",
      "         -0.0895, -0.1289]], grad_fn=<AddmmBackward0>)\n",
      "Output Shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "input_channels = 3  # for RGB images\n",
    "input_size = 32  # Example size, adjust as needed\n",
    "output_size = 10  # Example output size\n",
    "\n",
    "dcign = DCIGN(input_channels, input_size, output_size)\n",
    "\n",
    "sample_input = torch.randn(1, input_channels, input_size, input_size)\n",
    "\n",
    "output = dcign(sample_input)\n",
    "\n",
    "print(\"Output Tensor:\", output)\n",
    "print(\"Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd278c",
   "metadata": {},
   "source": [
    "# General Adversarial Network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbe30c",
   "metadata": {},
   "source": [
    "Objective:\n",
    "\n",
    "    -Generate images from random noise through adversarial training\n",
    "    - Improve generative models' performance by using 2 networks against each other (generator & discriminator) \n",
    "Unique Features:\n",
    "\n",
    "    -Adversarial training: Uses 2 NN's (generator/discriminator) that are trained simultaneously through adversarial processes. \n",
    "        -Generator learns to produce increasingly realistic data\n",
    "        -Discriminator learns to better distinguish between real & generated data\n",
    "    \n",
    "    -Feedback loop: Generator is updated based on the feedback from the discriminator, guiding it to product more realistic outputs\n",
    "    \n",
    "Common uses:\n",
    "   \n",
    "    -Image generation\n",
    "    -Style transfer\n",
    "    -Anomoly detection\n",
    "    -Super resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa33e6",
   "metadata": {},
   "source": [
    "## Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4611dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c8e3c",
   "metadata": {},
   "source": [
    "## Discriminator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1f612a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e8216",
   "metadata": {},
   "source": [
    "## GAN class\n",
    "Utilizes the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54a18ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(BaseNN):\n",
    "    def __init__(self, generator, discriminator):\n",
    "        super(GAN, self).__init__(input_size=None, hidden_size=None, output_size=None)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def forward(self, x):\n",
    "        generated_data = self.generator(x)\n",
    "        discriminator_output = self.discriminator(generated_data)\n",
    "        return generated_data, discriminator_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15f75f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAN(\n",
      "  (generator): Generator(\n",
      "    (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      "  (discriminator): Discriminator(\n",
      "    (fc1): Linear(in_features=10, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Generated Data: tensor([[0.4952, 0.4792, 0.4761, 0.3429, 0.4809, 0.4092, 0.4601, 0.4996, 0.5085,\n",
      "         0.5300]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator Output: tensor([[0.5585]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "\n",
    "generator = Generator(input_size, hidden_size, output_size)\n",
    "discriminator = Discriminator(output_size, hidden_size, 1)\n",
    "\n",
    "gan_model = GAN(generator, discriminator)\n",
    "\n",
    "sample_noise = torch.randn((1, input_size))\n",
    "\n",
    "generated_data, discriminator_output = gan_model(sample_noise)\n",
    "\n",
    "print(gan_model)\n",
    "print(\"Generated Data:\", generated_data)\n",
    "print(\"Discriminator Output:\", discriminator_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2bc21a",
   "metadata": {},
   "source": [
    "# Spiking Neural Network (SNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3386e",
   "metadata": {},
   "source": [
    "\n",
    "**Objective:**\n",
    "\n",
    "- Mimic the biological processes of the human brain more closely than traditional artificial neural networks by using neurons that fire in discrete spikes.\n",
    "- Process information through the timing of these spikes, enabling the network to efficiently handle spatiotemporal data and perform dynamic pattern recognition.\n",
    "\n",
    "**Unique Features:**\n",
    "\n",
    "- **Biologically Inspired:** Incorporates models of neurons that generate discrete spikes, a form of communication used in the biological nervous system.\n",
    "- **Temporal Dynamics:** Capable of capturing and processing temporal information inherent in the input data through the sequence and timing of spikes.\n",
    "- **Energy Efficiency:** Designed to be inherently more energy-efficient for certain computations, mirroring the energy efficiency seen in the human brain.\n",
    "- **Learning Through Time:** Utilizes learning mechanisms such as Spike-Timing-Dependent Plasticity (STDP), allowing the network to adapt based on the timing between spikes.\n",
    "\n",
    "**Common Uses:**\n",
    "\n",
    "- **Neurobiological Research:** Offers a platform for exploring theories of brain function and the principles underlying neural computation.\n",
    "- **Sensory Processing:** Applied in processing and interpreting data from sensory inputs, such as visual and auditory systems, in a manner similar to biological systems.\n",
    "- **Edge Computing:** Ideal for deployment in edge devices due to their low power consumption, where they can perform real-time data analysis.\n",
    "- **Pattern Recognition:** Utilized in tasks requiring the detection of patterns over time, such as speech recognition or gesture analysis.\n",
    "- **Robotic Control:** Empowers robots with the ability to process sensory inputs in real-time, leading to more adaptive and responsive behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6d2802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(BaseNN):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__(input_size, hidden_size, output_size)\n",
    "        # Define a simple linear layer to simulate neuron connections\n",
    "        self.linear = nn.Linear(input_size, hidden_size)\n",
    "        # Spike function could be a Heaviside step function or similar\n",
    "        self.spike_fn = lambda x: torch.heaviside(x - 0.5, torch.tensor([0.0]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.spike_fn(x)  # Simulate spiking behavior\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa46ad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Input: tensor([[ 0.0568, -0.0442,  0.9503, -0.6641, -0.7917,  0.8993,  0.7960,  1.4113,\n",
      "         -1.6402, -1.1859]])\n",
      "Spiked Output: tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         1., 0.]], grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 10  # Output size is not used in this simplified example but included for consistency with the class definition\n",
    "\n",
    "# Instantiate the SNN model\n",
    "snn_model = SNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Generate a sample input (batch size, input size)\n",
    "sample_input = torch.randn((1, input_size))\n",
    "\n",
    "# Forward pass through the SNN\n",
    "spiked_output = snn_model(sample_input)\n",
    "\n",
    "print(\"Sample Input:\", sample_input)\n",
    "print(\"Spiked Output:\", spiked_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ada33",
   "metadata": {},
   "source": [
    "Output explanation: The output tensor is of size *hidden size* with values either 0 or 1, representing whether each neuron in the hidden layer fired (1) or did not fire(0) based on the simplified spike function.\n",
    "\n",
    "This example demonstrates the instantation and baic usage, however it is a highly abstracted version for practical implementation and scope limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b21f0",
   "metadata": {},
   "source": [
    "# Liquid State Machine (LSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0646524",
   "metadata": {},
   "source": [
    "Objective:\n",
    "    \n",
    "    -Process time-varying inputs\n",
    "    -Utilize high-dimensional transient states (liquid states) induced by input stimuli for copmutation allowing the network to perform temporal pattern regonition and time-series prediction\n",
    "\n",
    "Unique Features:\n",
    "    \n",
    "    -Utilizes a network of spiking neurons to create a responsive state to input stimuli\n",
    "    -Specializes in handling sequences and temporal patterns\n",
    "    -Flexible readout layer interprets the reservoir's state for varied tasks\n",
    "\n",
    "Common Uses:\n",
    "    \n",
    "    -Neurobiological simulations and understanding brain functions\n",
    "    Speech and gesture recognition for interactive systems\n",
    "    -Time-series forcasting in finance & weather prediction\n",
    "    -Robotic sensory processing for adaptive control\n",
    "    -Biometric authentication through pattern analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c70767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSM(SNN):\n",
    "    def __init__(self, input_size, reservoir_size, output_size):\n",
    "        super(LSM, self).__init__(input_size, reservoir_size, output_size)\n",
    "        # In a true LSM, the reservoir would be more complex and involve dynamic connections.\n",
    "        # Here, we simulate it with a single RNN layer for simplicity.\n",
    "        self.reservoir = nn.RNN(input_size, reservoir_size, batch_first=True)\n",
    "        # The readout layer\n",
    "        self.readout = nn.Linear(reservoir_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Process input through the simplified 'reservoir'\n",
    "        reservoir_state, _ = self.reservoir(x)\n",
    "        \n",
    "        # Assuming the last state as the representation\n",
    "        reservoir_state = reservoir_state[:, -1, :]\n",
    "        output = self.readout(reservoir_state)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68dcf3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSM Model: LSM(\n",
      "  (linear): Linear(in_features=10, out_features=128, bias=True)\n",
      "  (reservoir): RNN(10, 128, batch_first=True)\n",
      "  (readout): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Output Shape: torch.Size([5, 1])\n",
      "Output: tensor([[ 0.0739],\n",
      "        [ 0.2955],\n",
      "        [-0.0266],\n",
      "        [-0.0481],\n",
      "        [ 0.0263]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 10\n",
    "reservoir_size = 128\n",
    "output_size = 1\n",
    "\n",
    "# Instantiate the LSM model\n",
    "lsm_model = LSM(input_size, reservoir_size, output_size)\n",
    "\n",
    "# Generate a sample input (batch size, sequence length, input size)\n",
    "# Let's create a batch of 5 sequences, each of length 7 (time steps) with 10 features\n",
    "sample_input = torch.randn((5, 7, input_size))\n",
    "\n",
    "# Forward pass through the LSM\n",
    "output = lsm_model(sample_input)\n",
    "\n",
    "print(\"LSM Model:\", lsm_model)\n",
    "print(\"Output Shape:\", output.shape)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4109c9e",
   "metadata": {},
   "source": [
    "### Expected Output and Understanding\n",
    "\n",
    "- **LSM Model:** This print statement will display the structure of the LSM model, including the RNN (reservoir) and the readout linear layer.\n",
    "- **Output Shape:** Since the readout layer's output size is 1, and you're processing a batch of 5 sequences, the output shape should be `[5, 1]`, indicating that for each sequence in the batch, you get a single output value.\n",
    "- **Output:** This will show the actual output values from the LSM. These values are generated by processing the synthetic sequential data through the LSM's reservoir and readout layer.\n",
    "\n",
    "This example is a straightforward demonstration meant to illustrate how you might set up and use an LSM model with PyTorch for sequence processing tasks. The synthetic data doesn't represent a specific real-world problem, but in practice, you could adapt this setup to work on tasks like time-series forecasting, sequence classification, or any problem where understanding temporal dynamics is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce46100",
   "metadata": {},
   "source": [
    "# Extreme Learning Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc53838",
   "metadata": {},
   "source": [
    "**High-Level Overview**\n",
    "\n",
    "Extreme Learning Machines (ELMs) represent an innovative class of single-hidden layer feedforward neural networks (SLFNs) that streamline the learning process by randomly assigning input weights and biases, focusing instead on analytically determining the output weights. This unique approach reduces training complexity and time, making ELMs particularly suitable for rapid prototyping and handling large or noisy datasets efficiently.\n",
    "\n",
    "**Data Type**\n",
    "\n",
    "ELMs are versatile, capable of processing:\n",
    "- Numerical\n",
    "- Time-series\n",
    "- Images\n",
    "- Continuous data\n",
    "\n",
    "This adaptability makes them applicable across a broad spectrum of data-intensive fields.\n",
    "\n",
    "**Task Objective**\n",
    "\n",
    "ELMs excel in:\n",
    "- Classification\n",
    "- Regression\n",
    "- Feature Learning\n",
    "\n",
    "Their fast learning speed and high efficiency position them as a powerful tool for both predictive modeling and data representation tasks.\n",
    "\n",
    "**Scalability**\n",
    "\n",
    "ELMs demonstrate remarkable scalability, efficiently managing large datasets and complex models with adjustable hidden nodes. This attribute is pivotal for applications in big data, where the volume and dimensionality of data can significantly impact computational performance.\n",
    "\n",
    "**Robustness to Noise**\n",
    "\n",
    "One of the standout features of ELMs is their robustness to noise, making them exceptionally reliable in real-world scenarios where data quality may vary. This robustness ensures that ELMs maintain high performance even when data is imperfect or incomplete.\n",
    "\n",
    "**Implementation Variants**\n",
    "\n",
    "Several variants of ELMs have been developed to cater to specific needs, including:\n",
    "- **Kernel ELM (KELM):** Offers enhanced capabilities for non-linear problem solving.\n",
    "- **Online Sequential ELM (OSELM):** Ideal for dynamic environments where data is available in sequences or streams.\n",
    "\n",
    "**Practical Application Guidance**\n",
    "\n",
    "**When to Use ELMs:**\n",
    "- Rapid model development is required.\n",
    "- Dealing with large or noisy datasets.\n",
    "- The task involves linear or non-linear problems where quick training is beneficial.\n",
    "\n",
    "**Considerations:**\n",
    "- While ELMs offer significant advantages in terms of speed and simplicity, they may not be the best fit for tasks requiring deep interpretability of model decisions. \n",
    "- For highly unstructured data, such as raw text or images that necessitate deep learning techniques, exploring other neural network models might yield better results.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Extreme Learning Machines offer a unique combination of speed, efficiency, and versatility, making them a valuable addition to the neural network toolkit. By understanding their capabilities, implementation variants, and practical applications, researchers and practitioners can effectively leverage ELMs to address a wide range of challenges in data analysis and predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacf9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
